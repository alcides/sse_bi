On-Line Transaction Processing (OLTP) systems constitute the kernel of the information systems used today to support the daily operations of most organizations. Although these systems comprise the best examples of complex business-critical systems, no practical way has been proposed so far to characterize the impact of faults in such systems or to compare alternative solutions concerning dependability features. This paper presents a practical example of benchmarking key dependability features of four different transactional systems using a first proposal of dependability benchmark for OLTP application environments. This dependability benchmark is an extension to the TPC-C standard performance benchmark, and specifies the measures and all the steps required to evaluate both the performance and dependability features of OLTP systems. Two different versions of the Oracle transactional engine running over two different operating systems were evaluated and compared. The results show that dependability benchmarking can be successfully applied to OLTP application environments.
This paper proposes a set of operators for software fault emulation through low-level code mutations. The definition of these operators was based on the analysis of an extensive collection of real software faults. Using the Orthogonal Defect Classification as a starting point, faults were classified in a detailed manner according to the high-level constructs where the faults reside and their effects in the program. We observed that a large percentage of faults fall in well-defined classes and can be characterized in a very precise way, allowing accurate emulation through a small set of mutation operators. The resulting operators closely emulate a broad range of common programmer mistakes. Furthermore, as the mutation is performed directly at the executable code, software faults can be injected in targets for which source is not available.
This paper proposes a practical way to evaluate the behavior of commercial-off-the-shelf (COTS) operating systems in the presence of faulty device drivers. The proposed method is based on the emulation of software faults in target device drivers and the observation of the behavior of the system and of a workload regarding a comprehensive set of failure modes analyzed according to different dimensions. The emulation of software faults itself is done through the injection at machine-code level of selected mutations that represent the code produced when typical programming errors are made in the high-level language code. An important aspect of the proposed methodology is the use of simple and established practices to evaluate operating systems failure modes, thus allowing its use as a dependability benchmarking technique. The generalization of the methodology to any software system built of discrete and identifiable components is also discussed.
This paper proposes a new technique to emulate software faults by educated mutations introduced at the machine-code level and presents an experimental study on the accuracy of the injected faults. The proposed method consists of finding key programming structures at the machine code-level where high-level software faults can be emulated. The main advantage of emulating software faults at the machine-code level is that software faults can be injected even when the source code of the target application is not available, which is very important for the evaluation of COTS components or for the validation of software fault tolerance techniques in COTS based systems. The technique was evaluated using several real programs and different types of faults and, additionally, it includes our study on the key aspects that may impact on the technique accuracy. The portability of the technique is also addressed. The results show that classes of faults such as assignment, checking, interface, and simple algorithm faults can be directly emulated using this technique.
Two important questions on experimental dependa-bility evaluation remain largely unanswered: 1) how to analyze the usually large amount of raw data produced in dependability evaluation experiments and 2) how to compare results from different experiments or results from similar experiments across different systems. These problems are also common to other dependability evaluation techniques such as the ones based on simulation, or even to the analysis of field data on computer faults. We propose the use of data warehousing technologies to store raw results from different experiments/setups in a common multidimensional structure where raw data can be analyzed and shared world wide by means of web-enabled OLAP (On-Line Analytical Processing) tools. This paper describes how to use the proposed approach in a concrete example of dependability evaluation experiment.
This paper evaluates the impact of transient errors in the operating system of a COTS-based system (CETIA board with two PowerPC 750 processors running LynxOS) and quantifies their effects at both the OS and at the application level. The study has been conducted using a Software-Implemented Fault Injection tool (Xception) and both realistic programs and synthetic workloads (to focus on specific OS features) have been used. The results provide a comprehensive picture of the impact of faults on LynxOS key features (process scheduling and the most frequent system calls), data integrity, error propagation, application termination, and correctness of application results.
The characterization of Database Management Sys-tems (DBMS) recovery mechanisms and the comparison of the recovery features of different DBMS require a practical approach to benchmark the effectiveness of recovery in the presence of faults.  Existing performance benchmarks for transactional and database areas include two major components: a workload and a set of performance measures. The definition of a benchmark to characterize DBMS recovery needs a new component ' the faultload. A major cause of failures in large DBMS is operator faults, which make them an excellent starting point for the definition of a generic faultload. This paper proposes the steps for the definition of generic faultloads based on operator faults for DBMS recovery benchmarking. A classification for operator faults in DBMS is proposed and a comparative analysis among three commercially DBMS is presented. The paper ends with a practical example of the use of operator faults to benchmark different configurations of the recovery mechanisms of the Oracle 8i DBMS.
A major cause of failures in large database manage-ment systems (DBMS) is operator faults. Although most of the complex DBMS have comprehensive recovery mecha¬nisms, the effectiveness of these mechanisms is difficult to characterize. On the other hand, the tuning of a large database is very complex and database administrators tend to concentrate on performance tuning and disregard the recovery mechanisms. Above all, database adminis¬trators seldom have feedback on how good a given con¬figuration is concerning recovery. This paper proposes an experimental approach to characterize both the perfor-mance and the recoverability in DBMS. Our approach is presented through a concrete example of benchmarking the performance and recovery of an Oracle DBMS run¬ning the standard TPC-C benchmark, extended to include two new elements: a faultload based on operator faults and measures related to recoverability. A classification of operator faults in DBMS is proposed. The paper ends with the discussion of the results and the proposal of guidelines to help database administrators in finding the balance between performance and recovery tuning.



This paper presents and evaluates a simple but very effective method to implement large data ware-houses on an arbitrary number of computers, achieving very high query execution performance and scalability. The data is distributed and processed in a potentially large number of autonomous computers using our technique called data warehouse striping (DWS). The major problem of DWS technique is that it would require a very expen-sive cluster of computers with fault tolerant capabilities to prevent a fault in a single computer to stop the whole system. In this paper, we propose a radically different approach to deal with the problem of the unavailability of one or more computers in the cluster, allowing the use of DWS with a very large number of inexpensive computers. The proposed approach is based on approximate query answering techniques that make it possible to deliver an approximate answer to the user even when one or more computers in the cluster are not available. The evaluation presented in the paper shows both analytically and experimentally that the approximate results obtained this way have a very small error that can be negligible in most of the cases.

Data warehouse design is clearly dominated by the business perspective. Quite often, data warehouse administrators are lead to data models with little room for performance improvement. However, the increasing demands for interactive response time from the users make query performance one of the central problems of data warehousing today. In this paper we defend that data warehouse design must take into account both the business and the performance perspective from the beginning, and we propose the extension to typical design methodologies to include performance concerns in the early design steps. Specific analysis to predicted data warehouse usage profile and meta-data analysis are proposed as new inputs for improving the transition from logical to physical schema. The proposed approach is illustrated and discussed using the TPC-H performance benchmark and it is shown that significant performance improvement can be achieved without jeopardizing the business view required for data warehouse models.
There are several auxiliary pre-computed access structures that allow faster answers by reading less base data. Examples are materialized views, join indexes, B-tree and bitmap indexes. This paper proposes dimension-join, a new type of index especially suited for data warehouses. The dimension-join borrows ideas from several concepts. It is both a bitmap index and a multi-table join and when being used one of the tables is not read to improve performance. It is a multi-table join because it holds information belonging to two tables, which is similar to the join index proposed by Valduriez. However, instead of being composed by the tables primary keys, the dimension-join index is also a bitmap index over the fact table using values from a dimension column. The dimension-join index is very useful when selecting facts depending of dimension tables belonging to snowflakes. The dimension-join represents a direct connection between the fact table and a table in the snowflake that can avoid several joins and produce enormous performance improvements. This paper also evaluates experimentally the dimension-join indexes using the TPC-H benchmark and shows that this new index structure can dramatically improve the performance for some queries.


This paper presents and evaluates a methodology for the emulation of software faults in COTS components using software implemented fault injection (SWIFI) technology. ESFFI (Emulation of Software Faults by Fault Injection) leverages matured fault injection techniques, which have been used so far for the emulation of hardware faults, and adds new features that make possible the insertion of errors mimicking those caused by real software faults. The major advantage of ESFFI over other techniques that also emulate software faults (mutations, for instance) is making fault locations ubiquitous; every software module can be targeted no matter if it is a device driver running in operating kernel mode or a 3rd party component whose source code is not available. Experimental results have shown that for specific fault classes, e.g. assignment and checking, the accuracy obtained by this technique is quite good




This paper presents an experimental study on the emulation of software faults by fault injection. In a first experiment, a set of real software faults has been compared with faults injected by a SWIFI tool (Xception) to evaluate the accuracy of the injected faults. Results revealed the limitations of Xception (and other SWIFI tools) in the emulation of different classes of software faults (about 44% of the software faults cannot be emulated). The use of field data about real faults was discussed and software metrics were suggested as an alternative to guide the injection process when field data is not available. In a second experiment, a set of rules for the injection of errors meant to emulate classes of software faults was evaluated. The fault triggers used seem to be the cause for the observed strong impact of the faults in the target system and in the program results. The results also show the influence in the fault emulation of aspects such as code size, complexity of data structures, and recursive versus sequential execution.
This paper presents and discusses observed failure modes of a common-off-the-shelf (COTS) Database Management System (DBMS) under the presence of transient operational faults induced by SWIFI. The standard Transaction Processing Performance Council (TPC) TPC-C benchmark and associated environment is used here together with fault-injection technology, building a framework that discloses both dependability and performance figures. Over 1600 faults were injected in the database server of a Client/Server computing environment built upon Oracle 8.1.5 database engine and Windows NT running on COTS machines with Intel Pentium processors. A macroscopic view on the impact of faults revealed that: 1) A large majority of the faults caused no observable abnormal impact in the database server; In 96% of hardware faults and 80% of software faults the database server behaved normally. 2) Software faults are more prone to let the database server hanging or cause abnormal terminations. 3) Up to 51% of software faults lead to observable failures in the client processes.
The ascendance of networked information in our economy and daily lives has increased the awareness of the importance of dependability features. OLTP (On-Line Transaction Processing) systems constitute the kernel of the information systems used today to support the daily operations of most of the business. Although these systems comprise the best examples of complex business-critical systems, no practical way has been proposed so far to characterize the impact of faults in such systems or to compare alternative solutions concerning dependability features. This paper proposes a dependability benchmark for OLTP systems. This dependability benchmark uses the workload of the TPC-C performance benchmark and specifies the measures and all the steps required to evaluate both the performance and key dependability features of OLTP systems, with emphasis on availability. This dependability benchmark is presented through a concrete example of benchmarking the performance and dependability of several different transactional systems configurations. The effort required to run the dependability benchmark is also discussed in detail.


The DBench (Dependability Benchmarking) project aims at formulating a conceptual framework for benchmarking the dependability of COTS and COTS-based systems. As a starting point of the project, this report surveys and comments on the state-of-the-art techniques in selected areas that we have currently identified to be pertinent for the activity of conducting the benchmarking process.
The areas covered encompass defining the context of benchmarking and its conjunction with issues of dependability. These include aspects of dependability assessment (such as analytical modelling, fault injection and field measurement), performance benchmarking and robustness benchmarking. For each area, several techniques and problems relevant to dependability benchmarking are identified, including areas requiring more research. This report compiles our current opinions on the topics and establishes the background for further DBench activities.
The survey identifies that representativeness is a key feature, since it is important both for fault injection experiments as well as for performance benchmarks. 
Another insight derived from the survey is the importance of considering dependability in association with functionality and performance attributes of the systems ' a stand-alone interpretation of dependability is probably not an objective property of any system. Furthermore, past experience has shown using a combined experimental and modelling approach to been successful in assessing computer system dependability. Accordingly, applying this to dependability benchmarking is also of interest, and warrants further research.
Finally, the problem of presenting relevant results to the users of a benchmark is an important facet. Performance benchmarks have typically employed a policy of presenting a few easy to understand metrics to the benchmark user. This has been a successful approach as benchmark users are not required to be experts on the subject. However, the basic issue here is ascertain the technical value of a benchmark with r
The goal of dependability benchmarking is to provide generic ways of characterising the behaviour of components and computer systems in the presence of faults, allowing for the quantification of dependability measures. Beyond existing evaluation techniques, dependability benchmarking must provide a uniform, repeatable and cost-effective way of performing this evaluation either as stand alone assessment or more often for comparative evaluation across systems. This document presents the preliminary framework proposed by the DBench project to investigate, define, and validate dependability benchmarks for computer systems, with particular emphasis on COTS-based systems and COTS components. The multiple dimensions of the problem are discussed and the framework is presented through a set of dependability benchmarking scenarios, ranging from pure experimental approaches to benchmarking scenarios where modelling and experimentation are tightly coupled. The main problems and research goals behind each benchmarking scenario are discussed and two concrete examples of preliminary dependability benchmarks already under investigation are presented.
This paper presents the results of a continuing research work on the practical characterization of operating systems behavior when one of its components has software faults. The methodology used is based on the emulation of software faults in device drivers and the observation of the behavior of the overall system regarding a comprehensive set of failure modes analyzed according to different dimensions related to different user perspectives. The emulation of the software faults is done through the injection at machine-code level of specific mutations that reproduce the code generated by compilers when typical programming errors exist in the high level language code. Two important aspects of this methodology are the independence of source code availability and the use of simple and established practices to evaluate operating systems failure modes, thus allowing its use as a dependability benchmarking technique. The generalization of the methodology to any software system built of discrete and identifiable components is also discussed.

In this paper we study the behavior of Algorithm Based Fault Tolerance (ABFT) techniques under faults injected according to a quite general fault model. Besides the problem of roundoff error in floating point arithmetic, we identify two further weakpoints, namely lack of protection of data during input and output, and incorrect execution of the correctness checks. We propose the Robust ABFT technique to handle those weakpoints. We then generalize it to programs that use assertions, where similar problems arise, leading to the technique of Robust Assertions, whose effectiveness is shown by fault injection experiments on a realistic control application. With this technique a system follows a new failure model, that we call Fail-Bounded, where with high probability all results produced are either correct or, if wrong, they are within a certain bound of the correct value, whose exact value depends on the output assertions used. We claim that this failure model is very useful to describe the behavior of many low redundancy systems.
The DWS (Data Warehouse Striping) technique is a round-robin data partitioning approach especially designed for distributed data warehouse envi-ronments. In DWS the fact tables are distributed by an arbitrary number of computers and the queries are executed in parallel by all the computers, guaran-tying a nearly optimal speed up and scale up. This technique is combined with an approximate query answering (AQA) strategy to deal with fails in one or more nodes. This paper presents a middle layer software and administration tools allowing the transparent implementation of DWS-AQA in commercial da-tabases and OLAP (On-line Analytical Processing) tools.
This chapter addresses Xception - a software implemented fault injection tool. Among its key features are the usage of the advanced debugging and performance monitoring resources available in modern processors to emulate realistic faults by software, and to monitor the activation of the faults and their impact on the target system behaviour in detail. Xception has been used extensively on the field and is one the very few fault injection tools commercially available and supported.
A major cause of failures in large database management systems (DBMS) is operator/administrator faults. Although most of the complex DBMS available today have comprehensive recovery mechanisms, the effectiveness of these mechanisms is difficult to characterize. On the other hand, the tuning of a large database is very complex and database administrators tend to concentrate on performance tuning and disregard the recovery mechanisms. Above all, database administrators seldom have feedback on how good a given configuration is concerning recovery. This paper proposes an experimental approach to characterize both the performance and the recoverability of DBMS. Our approach is presented through a concrete example of benchmarking the performance and recovery of an Oracle DBMS running the standard TPC-C benchmark, extended to include two new elements: a faultload based on operator faults and measures related to recoverability. A classification of operator/administrator faults in DBMS is proposed.  A set of tools have been design and built to reproduce operator faults in an Oracle 8i DBMS, using exactly the same means used in the field by the real database administrator. This experimental approach is generic (i.e., can be applied to any DBMS) and is fully automatic. The paper ends with the discussion of the results and the proposal of guidelines to help database administrators in finding the balance between performance and recovery tuning.
As soluções de geração de código são amplamente usadas para criar aplicações de bases de dados que possuem, por norma, bastante código repetitivo. No entanto, todas as aplicações possuem especificidades subjacentes à área de negócio a que se destinam. A iGen é uma plataforma de geração de código com suporte de ciclo de desenvolvimento completo, permitindo criar aplicações com elevada qualidade de uma forma simples, favorecendo a redução de tempo de desenvolvimento e de custos. Esta plataforma é genérica, sendo independente da tecnologia e da área de negócio. Outras vantagens da iGen são a escalabilidade da solução, modelação por entidades, ser orientada para o uso de ferramentas visuais e potenciar a transferência de conhecimento. Este artigo descreve a plataforma iGen, contextualizando a sua utilização nas diferentes fases do processo de desenvolvimento de software.
The increased reliance of our society on computers requires that their dependability be measured. Benchmarks with that objective must include fault injectors that are very simple to use. They should be downloadable from the web together with the other components of the benchmark, be able to target all system components, and should not require any complex setup or installation procedure. Since the existing injectors did not fulfill this requirement, we have developed DBench-FI, a fault injector for dependability benchmarking that is very simple to install and use. DBench-FI belongs to the software implemented fault injection (SWIFI) family, since no special hardware can be used for this purpose. The current version targets the Linux OS on Intel processors, and uses a flexible runtime kernel upgrading algorithm to allow access to the target process memory space, that can be in either user or system space, enabling in this way the injection of faults. Furthermore, it does not require the availability of the source code of any system component or user process. It is not based on any debug mode either, being able to inject faults even in tasks that were already running when it is installed, irrespective of their complexity, like the Oracle Database Management System that is used in the examples in the paper. This set of features is, to the best of our knowledge, unique. It currently only injects memory faults, but there are plans to include other fault models in the future. In order to demonstrate the capability of the DBench-FI tool, the results of complex fault injection campaigns are presented.
This paper discusses the problem of the emulation of software faults by fault injection. The first part of the paper investigates the possibilities of accurate emulation of real software faults by a SWIFI tool (Xception). Results revealed the limitations of Xception (and other SWIFI tools) in the emulation of several classes of software faults. Based on this first conclusion, the paper presents G-SWFIT which is, to the best of our knowledge, the first technique specifically proposed for the injection of software faults. The feasibility and accuracy of G SWFIT have been experimentally evaluated, and the results show that this technique provides a very good accuracy. The technique is also quite portable, as all the details related to the emulation of software faults in different en-vironments are encapsulated in fault emulation operator libraries.


This paper discusses the problems of pin-level fault injection for dependability
validation and presents the architecture of a pin-level fault injector called RIFLE. This system can be adapted to a wide range of target systems and the faults are mainly injected in the processor pins. The injection of the faults is deterministic and can be reproduced if needed. Faults of different nature can be injected and the fault injector is able to detect whether the injected fault has produced an error or not without the requirement of feedback circuits. RIFLE can also detect specific circumstances in which the injected faults do not affect the target system. Sets of faults with specific impact on the target system can be generated. The paper also presents fault injection results showing the coverage and latency achieved with a set of simple behavior based error detection mechanisms. It is shown that up to 72,5% of the errors can be detected with fairly simple mechanisms. Furthermore, for over 90% of the faults the target system has behaved according to the fail-silent model, which suggests that a traditional computer equipped
with simple error detection mechanisms is relatively close to a fail-silent computer.


The assessment of the dependability properties of a system (dependability benchmarking) is a critical step when choosing among similar components/products. This paper presents a proposal for the benchmarking of the dependability properties of web-servers. Our benchmark is composed of the three key components: measures, workload, and faultload. We use the SPECWeb99 benchmark as starting point, adopting the workload and performance measures from this performance benchmark, and we added the faultload and new measures related to dependability. We illustrate the use of the proposed benchmark through a case-study involving two widely used web servers (Apache and Abyss) running on top of three different operating systems. The faultloads used encompass software faults, hardware faults and network faults. We show that by using the proposed dependability benchmark it is possible to observe clear differences regarding dependability properties of the web-servers.

The most critical component of a dependability benchmark is the faultload, as it should represent a repeatable, portable, representative, and generally accepted set of faults. These properties are essential to achieve the desired standardization level required by a dependability benchmark but, unfortunately, are very hard to achieve. This is particularly true for software faults, which surely accounts for the fact that this important class of faults has never been used in known dependability benchmark proposals. This paper proposes a new methodology for the definition of faultloads based on software faults for dependability benchmarking. Our methodology builds on previous published results based on field data and uses a fault injection technique based on executable code mutation to emulate programming errors. Faultload properties such as repeatability, portability and scalability are also analyzed and validated through experimentation using a case study of dependability benchmarking of web servers. We concluded that software fault based faultloads generated using our methodology are appropriate and useful for dependability benchmarking. As our methodology is not tied to any specific software vendor or platform, it can be used to generate faultloads for the evaluation of any software product such as OLTP systems. Another important characteristic of the proposed methodology is the independence of source code availability. This allows us to generate faultloads appropriate for dependability evaluation of generic COTS software modules or systems.




Computer systems dependability has to be easy to measure to become a commodity. This is not yet the case, as there are still significant uncertainties at several levels of the evaluation process. In this chapter we start by describing the available experimental techniques that help in that process, like fault injection, robustness testing and field measurement. The capabilities and limitations of each of these techniques is presented, and the main dimensions required for them to participate in systematic dependability benchmarking are discussed, like how to build representative evaluation setups, choosing workloads and faultloads that are representative, portable and repeatable, and choosing the right measures to obtain meaningful results. After this, still only characterization data on fault tolerance methods will be available; to obtain global dependability measures like availability and reliability a modelling phase is required, where adequate fault rates are considered. We discuss also the development phase of a system where each of the previous techniques is applied, and end by presenting our opinion on the most important research directions for experimental dependability benchmarking.
The DWS (Data Warehouse Striping) technique allows the distribution of large data warehouses through a cluster of computers. The data partitioning approach partition the facts tables through all nodes and replicates the dimension tables. The replication of the dimension tables creates a limitation to the applicability of the DWS technique to data warehouses with big dimensions. This paper proposes a strategy to handle large dimensions in a distributed DWS system and evaluates the proposed strategy experimentally. With the proposed strategy the performance speed up and scale up obtained in the DWS technique are not affected by the presence of big dimensions. Furthermore, it extends the scope of the technique to queries that browse big dimensions that can also benefit of the performance increase of the DWS technique.
Dependability benchmarking has been subject of intense research in recent years, having already led to the proposal of dependability benchmarks for database systems. However, the methodology to define dependability benchmarks is not clear yet. Furthermore, and more important, the validation of the proposed benchmarks has been disregarded so far. This question is par-ticularly acute, as the ultimate goal of a dependability benchmark is to allow the comparison of different systems/components, which means that without proper validation of the benchmark one may draw wrong conclusions. This pa-per proposes a set of general guidelines for the definition and validation of de-pendability benchmarks for database centric On-Line Transaction Processing (OLTP) systems. The close relationship between the definition of a depend-ability benchmark and the validation aspects is also addressed.



Developing database applications with timeliness requirements is a difficult problem. During the execu-tion of transactions, database applications with timeli-ness requirements have to deal with the possible oc-currence of timing failures, when the operations specified in the transaction do not complete within the expected deadlines. In spite of the importance of time-liness requirements in database applications, database management systems (DBMS) do not assure any tem-poral property, not even the detection of the cases when the transaction takes longer than the ex-pected/desired time. Our goal is to investigate ways to add timeliness properties to the typical ACID (Atomic-ity, Consistency, Isolation, and Durability) properties supported by most DBMS.
Xception is an automated and comprehensive fault injection and robustness testing environment that enables accurate and flexible V&V (verification & validation) and evaluation of mission and business critical computer systems and computer components, with particular emphasis to software components. In this paper we focus on the new robustness testing features of Xception and illustrate them with a concrete example of robustness testing of the Real Time Executive for Multiprocessor Systems (RTEMS) performed under a European Space Agency (ESA) contract. To the best of our knowledge, this is the first time that robustness testing results for this real time operating system are presented. The testing revealed a significant number of critical flaws in RTEMS 4.5.0 and shows the effectiveness of Xception toolset.
This paper presents and evaluates an approach that allows the compression of data in Relational Database Management Systems (RDBMS) using existing text compression algorithms. Although the technique proposed is completely general, we believe it is particularly advantageous for the compression of medium size and large dimension tables in data warehouses. In fact, dimensions usually have a high number of text attributes and a reduction in the size of middle or large dimension have a big impact in the execution time of queries that join that dimension with the fact tables. In general, the high complexity and long execution time of most data warehouse queries make the compression of dimension text attributes (and possible text attributes that may exist in the fact table, such as false facts) an effective approach to speed up query response time. The proposed approach has been evaluated using the well-known TPC-H benchmark and the results show that speed improvements greater than 40% can be achieved for most of the queries

An important step in the development of dependable systems is the validation of their fault tolerance properties. Fault injection has been widely used for this purpose, however with the rapid increase in processor complexity, traditional techniques are also increasingly more difficult to apply. This paper presents a new software implemented fault injection and monitoring environment, called Xception, which is targeted for the modern and complex processors. Xception uses the advanced debugging and performance monitoring features existing in most of the modern processors to inject quite realistic faults by software, and to monitor the activation of the faults and their impact on the target system behavior in detail. Faults are injected with minimum interference with the target application. The target application is not modified, no software traps are inserted, and it is not necessary to execute the target application in special trace mode (the application is executed at full speed). Xception provides a comprehensive set of fault triggers, including spatial and temporal fault triggers, and triggers related to the manipulation of data in memory. Faults injected by Xception can affect any process running on the target system (including the kernel), and it is possible to inject faults in applications for which the source code is not available. Experimental results are presented to demonstrate the accuracy and potential of Xception in the evaluation of the dependability properties of the complex computer systems available nowadays.
In the research reported in this paper, transient faults were injected in the nodes and in the communication subsystem (by using software fault injection) of a commercial parallel machine running several real applications. The results showed that a significant percentage of faults caused the system to produce wrong results while the application seemed to terminate normally, thus demonstrating that fault tolerance techniques are required in parallel systems, not only to assure that long-running applications can terminate but also (and more important) that the results produced are correct. Of the techniques tested to reduce the percentage of undetected wrong results only ABFT proved to be effective. For other simple error detection methods to be effective, they have to be designed in, and not added as an after thought. Faults injected in the communication subsystem proved the effectiveness of end-to-end CRCs on the data movements between processors.

This paper presents Xception, a software fault injection and monitoring environment. Xception uses the advanced debugging and performance monitoring features existing in most of the modern processors to inject more realistic faults by software, and to monitor the activation of the faults and their impact on the target system behaviour in detail. Faults are injected with minimum interference with the target application. The target application is not modified,
no software traps are inserted, and it is not necessary to execute it in special trace mode (the application is executed at full speed). Xception provides a comprehensive set of fault triggers, including spatial and temporal fault triggers, and triggers related to the manipulation of data in memory. Faults injected by Xception can affect any process running on the target system including the operating system. Sets of faults can be defined by the user according to several criteria, including the emulation of faults in specific target processor functional units. Presently, Xception has been implemented on a parallel machine build around the PowerPC 601 processor running the PARIX operating system. Experiment results are presented showing the impact of faults on several parallel applications running on a commercial parallel system. It is shown that up to 73% of the faults, depending on the processor functional unit affected, can cause the application to produce wrong results. The results show that the impact of faults heavily depends on the application and the specific processor functional unit affected by the fault.
Traditionally, fail-silent computers are implemented by
using massive redundancy (hardware or software). In this
research we investigate if it is possible to obtain a high
degree of fail-silent behavior from a computer without
hardware or software replication by using only simple
behavior based error detection techniques. It is assumed
that if the errors caused by a fault are detected in time it
will be possible to stop the erroneous computer behavior,
thus preventing the violation of the fail-silent model. The
evaluation technique used in this research is physical fault
injection at the pin level. Results obtained by the injection
of about 20000 different faults in two different target
systems have shown that 1) in a system without error
detection up to 46% of the faults caused the violation of
the fail-silent model; 2) in a computer with behavior
based error detection the percentage of faults that caused
the violation of the fail-silent mode was reduced to values
from 2.3% to 0.4%; 3) the results are very dependent on
the target system, on the program under execution during
the fault injection and on the type of faults.
This paper presents a methodology for the automated detection of buffer overflow vulnerabilities in executable software. Buffer overflow exploitation has been used by hackers to breach security or simply to crash computer systems. The mere presence inside the software code of a vulnerability that allows for buffer overflow exploitations presents a serious risk.  So far, all methodologies devised to mitigate this problem assume source code availability or prior knowledge on vulnerable functions. Our methodology removes this dependency and allows the analysis of executable code without any knowledge about its internal structure. This independence is fundamental for relevant scenarios such as COTS selection during system integration (for which source code is usually not available), and the definition of attackloads for dependability benchmarking.
We propose the grand challenge problem of dependability benchmarking and prediction for real-time mission-critical systems (RTMCSs).  Evaluating dependability would quantify the degree of reliance that could justifiably be placed on a critical system, even in the face of partial failures or exceptional conditions.  A comprehensive result would require an inter-disciplinary approach embracing the entire product lifecycle.  While there are significant technical hurdles to both assessing the dependability of individual elements and combining resultant measures, a viable approach must be found to ensure that the computing systems our society is coming to depend upon will be reliable, available, safe, and secure.  The participation of several communities, including the Real Time Computing community, is vital to successfully address this challenge.
Data warehouses are used to store large amounts of data. This
data is often used for On-Line Analytical Processing (OLAP)
where short response times are essential for on-line decision
support. One of the most important requirements of a data
warehouse server is the query performance. The principal aspect
from the user perspective is how quickly the server processes a
given query: 'the data warehouse must be fastâ?.
The main focus of our research is finding adequate solutions to
improve query response time of typical OLAP queries and
improve scalability using a distributed computation environment
that takes advantage of characteristics specific to the OLAP
context. Our proposal provides very good performance and
scalability even on huge data warehouses.


















The injection of faults has been widely used to evaluate fault tolerance mechanisms and to assess the impact of faults in computer systems. However, the injection of software faults is not as well understood as other classes of faults (e.g., hardware faults). In this paper we analyze how software faults can be injected (emulated) in source-code independent manner. We specifically address important emulation requirements such as fault representativeness and emulation accuracy. We start with the analysis of an extensive collection of real software faults. We observed that a large percentage of faults falls into well-defined classes and can be characterized in a very precise way, allowing accurate emulation of software faults through a small set of emulation operators. A new software fault injection technique (G-SWFIT) based on emulation operators derived from the field study is proposed. This technique consists of finding key programming structures at the machine code-level where high-level software faults can be emulated. The fault-emulation accuracy of this technique is shown. This work also includes a study on the key aspects that may impact the technique accuracy. The portability of the technique is also discussed and it is shown that a high degree of portability can be achieved.
Protective wrappers are used to make components behave in a robust way. They are particularly useful when the robustness of the reused component is not guaranteed. How to be sure that the wrapped component meets the system requirements? How to assure that the wrapper really protects the system against component failures? This paper presents the use of system-level fault injection to answer these questions. We also present a case study to illustrate the proposed methodology as well as the experimental results obtained.
Rigorous verification and validation of space software systems has always been mandatory, as these systems once deployed cannot be maintained, in most cases. Additionally, due to the intrinsic criticality of this type of systems, the failure of a given component might mean the loss of the mission. Nowadays, one can observe that off-the-shelf (OTS) components (or commercial off-the-shelf - COTS) are being used in onboard space systems (e.g. operating systems, run time libraries, etc.) in order to reduce project costs. The use of OTS components in real time safety critical applications may increase the failure probability, since these components are not designed and developed for environment with strict timing and/or safety requirements. The assessment approach proposed in this paper is based on the joint use of robustness testing (interface fault injection) and a particular fault injection technique named G-SWFIT that injects realistic software faults. The proposed toolset is meant both for 1) revealing OTS components vulnerabilities, which is suitable to be used early in the system life cycle, and 2) assess the impact of component failures in the entire system, within the 1st steps of validation. An example of using the proposed approach in a satellite data handling software prototype is also presented.
This paper evaluates the use of software complexity metrics to define representative fault distributions for software fault injection experiments. A field data study on more than 350 bug reports available from open software initiatives was used to compare the fault distributions generated by our approach with the real fault distributions observed in the field. Results show that the way we distribute software faults for fault injection is consistent with field observations when the size and complexity of the software modules are not very high. For very large modules the fault density observed are lower than the estimated by our approach. Possible explanations and improvements are discussed.
The injection of interface faults through API parameter corruption is a technique commonly used in experimental dependability evaluation. Although the interface faults injected by this approach can be considered as a possible consequence of actual software faults in real applications, the question of whether the typical exceptional inputs and invalid parameters used in these techniques do represent the consequences of software bugs is largely an open issue. This question may not be an issue in the context of robustness testing aimed at the identification of weaknesses in software components. However, the use of interface faults by API parameter corruption as a general approach for dependability evaluation in component-based systems requires an in depth study of interface faults and a close observation of the way internal component faults propagate to the component interfaces. In this work we present the results of experimental evaluation of realistic component-based applications developed in Java and C using the injection of interface faults by API parameter corruption and the injection of software faults inside the components by modification of the target code. The faults injected inside software components emulate typical programming errors and are based on an extensive field data study previously published. The results show the consequences of internal component faults in several operational scenarios and provide empirical evidences that interface faults and software component faults cause different impact in the system.
This paper presents the improvements that were implemented in the
current version of Jaca, a fault injection tools that is used to validate Java
applications. Due to these improvements Jaca reports based on .CVS file
format allow statistical analysis and require less effort to handle the
experiments results. Jaca monitoring abilities provide detailed information to
follow exceptional behavior during the execution of fault injection campaign.
These improvements also provide more robustness, minimize the user
interaction in the fault injection campaign and improve the performance of an
injection execution.
With the goal of reducing time to market and project costs, the current trend of real time business and mission critical systems is evolving from the development of custom made applications to the use of commercial off the shelf (COTS) products. Obviously, the same confidence and quality of the custom made software components is expected from the commercial applications. In most cases, such products (COTS) are not designed with stringent timing and/or safety requirements as priorities. Thus, to decrease the gap between the use of custom made components and COTS components, this paper presents a methodology for evaluating COTS products in the scope of dependable, real time systems, through the application of fault injection techniques at key points of the software engineering process. By combining the use of robustness testing (fault injection at interface level) with software fault injection (using educated fault injection operators), a COTS component can be assessed in the context of the system it will belong to, with special emphasis given to timing and safety constraints that are usually imposed by the target real time dependable environment. In the course of this work, three case studies have been performed to assess the methodology using realistic scenarios that used common COTS products.
Web-services are supported by a complex software infrastructure that must provide a robust service to the client applications. This paper presents a practical approach for the evaluation of the robustness of web-services infrastructures. A set of robustness tests (i.e., invalid web-services call parameters) is applied during web-services execution in order to reveal possible robustness problems in the web-services code and in the application server infrastructure. The approach is illustrated using two different implementations of the web-services specified by the TPC-App performance benchmark running on top of the JBoss application server. The proposed approach is generic and can be used to evaluate the robustness of web-services implementation (relevant for programmers) and application server infrastructures (relevant for administrators and system integrators).
This chapter presents a dependability benchmark for web-servers. Our benchmark is composed of the three key components: measures, workload, and faultload. We use the SPEC-Web99 benchmark as starting point, adopting the workload and performance measures from this performance benchmark, and we added the faultload and new measures related to dependability. We illustrate the use of the proposed benchmark through a case-study involving two widely used web servers running on top of three different operating systems. The faultloads used in our case-study encompass software faults, hardware faults and network faults. We show that by using the proposed dependability benchmark it is possible to observe clear differences regarding dependability properties of the web-servers.
On-Line Transaction Processing (OLTP) systems are the core of the information systems used to support the daily operations of organizations. These systems, whose correct behavior is of extreme importance, use a database management system &#40;DBMS&#41; that assures transactional properties and provides data recovery features. Although these systems represent the best examples of business-critical systems, no practical way has been pro-posed so far to characterize and compare alternative solutions concerning dependability features. In fact, transac-tional systems industry holds a well established infrastructure for performance evaluation, where the benchmarks developed by the Transactional Performance Processing Council (TPC) represent the most important bench-marking initiative of all computer industry. However, as performance evaluation has been the main focus, de-pendability benchmarking of transactional systems has been largely disregarded over the years. This chapter proposes a dependability benchmark for OLTP application environments that specifies the measures and all components required to evaluate both the performance and key dependability features of OLTP systems, with particular emphasis on availability. The benchmark is presented through a concrete example of benchmarking the performance and dependability of several different transactional systems configurations. The results obtained show clearly that it is possible to apply dependability benchmarking in transactional systems in a very successful way.
One important question in component-based software development is how to estimate the risk of using COTS components, as the components may have hidden faults and typically the source code is not available for analysis. This question is particularly relevant in scenarios where it is necessary to choose the most reliable COTS when several alternative components of equivalent functionality are available. The estimated risk introduced in the system by each component can help the system integrator in such cases. This paper proposes a practical approach to assess the risk of using a given software component (COTS or non COTS). Although we focus on comparing components, the methodology can be useful to assess the risk in individual modules. The proposed approach uses the injection of realistic residual software faults to assess the impact of possible component failures and uses software complexity metrics to estimate the probability of residual defects in software components. The proposed approach is demonstrated and evaluated in a comparison scenario using two real and competing off-the-shelf components (the RTEMS and the RTLinux real time operating system) in a realistic application of a satellite data handling application used by the European Space Agency.









The DWS (Data Warehouse Striping) technique is a round-robin data partitioning approach especially designed for distributed data warehousing environments. In DWS the fact tables are distributed by an arbitrary number of low-cost computers and the queries are executed in parallel by all the com-puters, guarantying a nearly optimal speed up and scale up. However, the use of a large number of inexpensive nodes increases the risk of having node failures that impair the computation of queries. This paper proposes an approach that provides Data Warehouse Striping with the capability of answering to queries even in the presence of node failures. This approach is based on the selective replication of data over the cluster nodes, which guarantees full availability when one or more nodes fail. The proposal was evaluated using the newly TPC-DS benchmark and the results show that the approach is quite effective.


The use of Java Message Service (JMS) for enterprise applications communication and integration is increasing very quickly. However, although JMS is frequently used in business-critical environments, applications are typically developed with the assumption that the middleware being used is robust, which is not always the case. Robustness failures in such environments are particularly dangerous, as they may originate vulnerabilities that can be maliciously exploited with severe consequences for the systems subject of attack. This paper proposes an approach for the evaluation of the robustness of JMS middleware. Our approach is presented through a concrete example of evaluating the robustness of three well-known JMS solutions (JBoss MQ 3.2.8.SP1, JBoss MQ 4.2.1.GA, and Active MQ 4.1.1), in which several robustness and critical security related problems have been disclosed (including specification conformance disparities).

Current business critical environments increasingly rely on SOA standards to execute business operations. These operations are frequently based on web service compositions that use several web services over the internet and have to fulfill specific timing constraints. In these environments, an operation that does not con-clude in due time may have a high cost as it can easily turn into service abandonment with financial and pres-tige losses to the service provider. In fact, at certain points, carrying on with the execution of an operation may be useless as a timely response will be impossible to obtain. This paper proposes a time-aware pro-gramming model for web services that provides trans-parent timing failure detection. The paper illustrates the proposed model using a set of services specified by the TPC-App performance benchmark.





Web services are increasingly being used in business critical environments, enabling uniform access to services provided by distinct parties. In these environments, an operation that does not execute on due time may be completely useless, which may result in service abandonment, and reputation or monetary losses. However, existing web services environments do not provide mechanisms to detect or predict timing violations. This paper proposes a web services programming model that transparently allows temporal failure detection and uses historical data for temporal failure prediction. This enables providers to easily deploy time-aware web services and consumers to express their timeliness requirements. Timing failures detection and prediction can be used by client applications to select alternative services in runtime and by application servers to optimize the resources allocated to each service
Developing robust web services is a difficult task. Field studies show that a large number of web services are deployed with robustness problems (i.e., presenting unexpected behaviors in the presence of invalid inputs). Several techniques for the identification of robustness problems have been proposed in the past. This paper proposes a mechanism that automatically fixes the problems detected. The approach consists of using robustness testing to detect robustness issues and then mitigate those issues by applying inputs verification based on well-defined parameter domains, including domain dependencies between different parameters. This integrated and fully automatable methodology has been used to improve three different implementations of the TPC-App web services. Results show that this tool can be easily used by developers to improve the robustness of web services implementations.
Web services represent a powerful interface for back-end database systems and are increasingly being used in business critical applications. However, field studies show that a large number of web services are deployed with security flaws (e.g., having SQL Injection vulnerabilities). Although several techniques for the identification of security vulnerabilities have been proposed, developing non-vulnerable web services is still a difficult task. In fact, security-related concerns are hard to apply as they involve adding complexity to already complex code. This paper proposes an approach to secure web services against SQL and XPath Injection attacks, by transparently detecting and aborting service invocations that try to take advantage of potential vulnerabilities. Our mechanism was applied to secure several web services specified by the TPC-App benchmark, showing to be 100% effective in stopping attacks, non-intrusive and very easy to use.
This paper proposes a new automatic approach for the detection of SQL Injection and XPath Injection vulnerabilities, two of the most common and most critical types of vulnerabilities in Web services. Although there are tools that allow testing Web applications against security vulnerabilities, previous research shows that the effectiveness of those tools in Web services environments is very poor. In our approach a representative workload is used to exercise the Web service and a large set of SQL/XPath injection attacks are applied to disclose vulnerabilities. Vulnerabilities are detected by comparing the structure of the SQL/XPath commands issued in the presence of attacks to the ones previously learned when running the workload in the absence of attacks. Experimental evaluation shows that our approach performs much better than known tools (including commercial ones), achieving extremely high detection coverage while maintaining the false positives rate very low.


<b><a href="http://ieeexplore.ieee.org/stamp/stamp.jsp?tp=&arnumber=5283945">Download from IEEE Xplore</a> </b>

Although Web services are becoming business-critical components, they are often deployed with critical software bugs that can be maliciously explored. Web vulnerability scanners allow detecting security vulnerabilities in Web services by stressing the service from the point of view of an attacker. However, research and practice show that different scanners have different performance on vulnerabilities detection. In this paper we present an experimental evaluation of security vulnerabilities in 300 publicly available Web services. Four well known vulnerability scanners have been used to identify security flaws in Web services implementations. A large number of vulnerabilities has been observed, which confirms that many services are deployed without proper security testing. Additionally, the differences in the vulnerabilities detected and the high number of false-positives (35% and 40% in two cases) and low coverage (less than 20% for two of the scanners) observed highlight the limitations of Web vulnerability scanners on detecting security vulnerabilities in Web services.


<b><a href="http://ieeexplore.ieee.org/stamp/stamp.jsp?tp=&arnumber=5270294">Download from IEEE Xplore</a> </b>
Database Management Systems (DBMS) have a long tradition in high security and several mechanisms needed to protect data have been proposed/consolidated in the database arena. However, the effectiveness of those mechanisms is very dependent on the actual configuration chosen by the database administrator. Tuning a large database is quite complex and achieving high security is a very difficult task that requires a lot of expertise and continuous and proactive work. In this paper we present an assessment tool aimed at evaluating the security of DBMS configurations. The proposed tool is simple and effective, and can be used by administrators with very little security knowledge. We evaluate the tool by performing the assessment of four different real database installations based on four well-known and widely used DBMS engines.
The use of Service Oriented Architecture (SOA) in enterprise applications development is increasing very quickly. In a SOA environment providers supply a set of services that must be robust. Although SOA is being used in business-critical environments, there is no practical means to measure or compare the robustness of services. Robustness failures in such environments are dangerous, as they can be maliciously exploited with severe consequences for the attacked systems. This chapter addresses the problem of robustness validation in SOA environments. The approach proposed is based on a set of robustness tests that is used to discover both programming and design errors. Two concrete examples are presented: one focusing on web services and the other targeting Java Message Service (JMS) middleware. The proposed approach is useful for both providers (to validate the robustness of deployed services) and consumers (to select the services that best fit architectural requirements).
Business critical applications are increasingly being deployed as web services that access database systems, and must provide secure operations to its clients. Although the open web environment emphasizes the need for security, several studies show that web services are still being deployed with command injection vulnerabilities. This paper proposes a learning-based approach to secure web services against SQL and XPath Injection attacks. Our approach is able to transparently learn valid request patterns (learning phase) and then detect and abort potentially harmful requests (protection phase). When it is not possible to have a complete learning phase, a set of heuristics can be used to accept/discard doubtful cases. Our mechanism was applied to secure TPC-App services and open source services. It showed to be extremely effective in stopping all tested attacks, while introducing a negligible performance impact.
Predicting failures at runtime is one of the most promising techniques to increase the availability of computer systems. However, failure prediction algorithms are still far from providing satisfactory results. In particular, the identification of the variables that show symptoms of incoming failures is a difficult problem. In this paper we propose an approach for identifying the most adequate variables for failure prediction. Realistic software faults are injected to accelerate the occurrence of system failures and thus generate a large amount of failure related data that is used to select, among hundreds of system variables, a small set that exhibits a clear correlation with failures. The proposed approach was experimentally evaluated using two configurations based on Windows XP. Results show that the proposed approach is quite effective and easy to use and that the injection of software faults is a powerful tool for improving the state of the art on failure prediction.


The AMBER Raw Data Repository is a repository of field data and raw results from resilience assessment experiments. Its goal is to grant both the research and IT industry communities with an infrastructure to gather, analyze and share field data resulting from resilience assessments of systems and services, stimulating a better coordination of high quality research in the area, and contributing to the promotion of a standardization of resilience measurement, which will in turn have a positive impact in the industry. While experimental and field data repositories are recognizably fundamental for supporting the advance of research and the dissemination of knowledge, the research community still seems somewhat reluctant in embracing such enterprises. This paper presents our experience in building and deploying the AMBER Raw Data Repository, and intends to share insights gained in the process, as well as raising some discussion topics on the implementation and future of global experimental data repositories.




Self-adaptive systems are widely recognized as the future of computer systems. Due to their dynamic and evolving nature, the characterization of self-adaptation and resilience attributes is of upmost importance. The problem is that nowadays there is no practical way to characterize self-adaptation capabilities or to compare alternative solutions concerning resilience. In this paper we discuss the problem of resilience benchmarking of self-adaptive systems. We start by identifying a set of key challenges and then propose a research roadmap to tackle those challenges.
The extraction of temporal information from text documents is becoming increasingly important in many applications such as natural language processing, information retrieval, question answering, etc.. Indeed, the temporal dimension plays a key role on most of these systems, promoting better performance. Our goal is the definition of a temporal document representation, incorporating the time dimension into information retrieval model to improve the quality of the results. Our approach is based on temporal segmentation of documents. Temporal-aware retrieval models may explore a richer temporal document representation, enabled by segmentation. To achieve this, first we must identify temporal expressions and capture, when possible, their normalized time values. Starting from our prior work on temporal expressions recognition, we present in this paper, a resolution tool that achieves promising results in a Portuguese collection. Furthermore, a temporal characterization of the used collection shows enough and suitable information for a meaningful temporal document segmentation.
The annotation or extraction of temporal information from text documents is becoming increasingly important in many natural language processing
applications such as text summarization, information retrieval, question
answering, etc.. This paper presents an original method for easy recognition
of temporal expressions in text documents. The method creates semantically classified temporal patterns, using word co-occurrences obtained from training corpora and a pre-defined seed keywords set, derived from the used language temporal references. A participation on a Portuguese named entity evaluation contest showed promising effectiveness and efficiency results. This approach can be adapted to recognize other type of expressions or languages, within other contexts, by defining the suitable word sets and training corpora.
As técnicas de recolha de informação assumiram um papel de grande relevo nos últimos anos, em virtude da importância assumida pelos motores de busca na Internet. No entanto, a utilização de informação temporal para melhorar os resultados das pesquisas tem sido pouco explorada, apesar de existir um grande potencial para conseguir
esse melhoramento. De facto, a noção de tempo é essencial para muitas das pesquisas efectuadas num sistema de recolha de informação, como por exemplo, na área da saúde onde será pertinente reconstruir o historial clínico dos pacientes com a capacidade de encontrar eventos e apresentá-los num espaço temporal, permitindo, desta forma, dar maior exactidão ao relatório (Alonso et al., 2007). 
No entanto, nem sempre o tempo surge de forma explícita nos documentos, mas as referências temporais podem ajudar a identificar a relevância dos documentos encontrados. O interesse no processamento de informação temporal tem crescido nos últimos anos e tem-se intensificado nas mais diversas áreas de investigação.
O objectivo do sistema que desenvolvemos é o de identificar informação temporal existente em documentos, para posteriormente ser utilizada, como papel importante na ordenação da lista de resultados obtida pelas pesquisas efectuadas em sistemas de recolha de informação.
Como existe ainda pouco trabalho desenvolvido no processamento de informação temporal da língua portuguesa, decidimos criar um sistema de raiz que seguisse um algoritmo simples e rápido. O processo de anotação/extracção de informação num sistema de recolha de informação terá de ser rápido para que não comprometa todo o sistema.
Pretende-se que o sistema PorTexTO, designado por PORtuguese Temporal EXpressions Tool, seja um sistema simples e com baixo tempo de processamento. Para que o desempenho não seja comprometido, o sistema poderá não encontrar todas as expressões temporais existentes nos documentos que processar, mas deverá encontrar as que ocorram mais vezes nos documentos em português e que são definidas através de estudo estatístico.
In this work, we introduce a software testbed for temporal processing of
Portuguese texts, composed by several building blocks: identification, classification and resolution of temporal expressions and temporal text segmentation. Starting from a simple document, we can reach a set of temporally annotated segments, which enables the establishment of relationships between words and time. This temporally enriched information is then placed into an Information Retrieval system. This work represents a step forward for Portuguese language processing, with notorious lack of tools. Its main novelty is temporal segmentation of texts. Even with target application in temporal aware Information Retrieval, the described software tools can be used in other application scenarios.
Software reuse is the practice of using existing artifacts (code, architecture, requirements, etc.) in new projects. The ad-vantages of using previously developed software in new projects are easily understood. However, reusing artifacts is usually done in an ad-hoc and incipient way, requiring an im-portant effort of adaptation, so developers frequently prefer to develop components from scratch. In this paper we present a strategy that is being adopted by Critical Software, a medium-sized company, to promote software reuse. This strategy starts by assuming that the success of software reuse is dependent on the ability of measuring its advantages. We have thus proposed the use of the Goal-Question-Metric (GQM) technique, extend-ed with Data Warehousing data model design concepts to ex-tract a set of reuse-specific metrics for measuring the gains of reuse. We show that it is very easy to measure the productivity improvement due to code reuse, by simply measuring or esti-mating the efforts of developing a component for reuse, inte-grating it a new artifact, and developing this artifact, built with reusing the component.


Robustness is an attribute of resilience that measures the behaviour of the system under non-standard conditions. Robustness is defined as the degree to which a system operates correctly in the presence of exceptional inputs or stressful environmental conditions. As triggering robustness faults could in the worst case scenario even crash the system, detecting this type of faults is of utmost importance. This chapter presents the state of the art on robustness testing by summarizing the evolution of basic robustness testing techniques, giving an overview of the specific methods and tools developed for major application domains, and introducing penetration testing, a specialization of robustness testing, which searches for security vulnerabilities. Finally, the use of testing results in resilience modelling and analysis is discussed.

Zoltán Micskei (Budapest University of Technology and Economics, Hungria), István Majzik (Budapest University of Technology and Economics, Hungria), Henrique Madeira, Marco Vieira, Nuno Antunes, Alberto Avritzer (Siemens Corporate Research, EUA)








This chapter is devoted to field studies and the aspects related to this kind of measurements. The importance of measurements collected from the operational scenarios is discussed, and two case studies are presented. Field measurements are closely tied to data repositories, and this chapter presents an overview of some field data repositories available to the public.


The injection of software faults in software components to assess the impact of these faults on other components or on the system as a whole, allowing the evaluation of fault tolerance, is relatively new compared to decades of research on hardware fault injection. This paper presents an extensive experimental study (more than 3.8 million individual experiments in three real systems) to evaluate the representativeness of faults injected by a state-of-the-art approach (G-SWFIT). Results show that a significant share (up to 72 percent) of injected faults cannot be considered representative of residual software faults as they are consistently detected by regression tests, and that the representativeness of injected faults is affected by the fault location within the system, resulting in different distributions of representative/nonrepresentative faults across files and functions. Therefore, we propose a new approach to refine the faultload by removing faults that are not representative of residual software faults. This filtering is essential to assure meaningful results and to reduce the cost (in terms of number of faults) of software fault injection campaigns in complex software. The proposed approach is based on classification algorithms, is fully automatic, and can be used for improving fault representativeness of existing software fault injection approaches.
Fault injection has been successfully used in the past to support the generation of realistic failure data for offline training of failure prediction algorithms. However, runtime computer systems evolution requires the online generation of training data. The problem is that using fault injection in a production environment is unacceptable. Virtualization is a cheap sand boxing solution that may be used to run multiple copies of a system, over which fault injection can be safely applied. Nevertheless, there is no guarantee that the data generated in the virtualized environment can be used for training the algorithms that will run in the original system. In this work we study the similarity of failure data obtained in the two scenarios, considering different virtualized environments. Results show that the data share key characteristics, suggesting virtualization as a viable solution to be further researched.
Most web applications have critical bugs (faults) affecting their security, which makes them vulnerable to attacks by hackers and organized crime. To prevent these security problems from occurring it is of utmost importance to understand the typical software faults. This paper contributes to this body of knowledge by presenting a field study on two of the most widely spread and critical web application vulnerabilities: SQL Injection and XSS. It analyzes the source code of security patches of widely used web applications written in weak and strong typed languages. Results show that only a small subset of software fault types, affecting a restricted collection of statements, is related to security. In order to understand how these vulnerabilities are really exploited by hackers, this paper also presents an analysis of the source code of the scripts used to attack them. The outcomes of this study can be used to train software developers and code inspectors in the detection of such faults, and are also the foundation for the research of realistic vulnerability and attack injectors that can be used to assess security mechanisms, like intrusion detection systems, vulnerability scanners, and static code analyzers.
In this paper we propose a methodology and a prototype tool to evaluate web application security mechanisms. The methodology is based on the idea that injecting realistic vulnerabilities in a web application and attacking them automatically can be used to support the assessment of existing security mechanisms and tools in custom setup scenarios. To provide true to life results, the proposed vulnerability and attack injection methodology relies on the study of a large number of vulnerabilities in real web applications. In addition to the generic methodology, the paper describes the implementation of the Vulnerability & Attack Injector Tool (VAIT) that allows the automation of the entire process. We used this tool to run a set of experiments that demonstrate the feasibility and the effectiveness of the proposed methodology. The experiments include the evaluation of coverage and false positives of an Intrusion Detection System for SQL Injection attacks and the assessment of the effectiveness of two top commercial web application vulnerability scanners. Results show that the injection of vulnerabilities and attacks is indeed an effective way to evaluate security mechanisms and to point out not only their weaknesses but also ways for their improvement.
Third-party software certification should attest that the software product satisfies the required confidence level according to certification standards such as ISO/IEC 9126, ISO/IEC 14598 or ISO/IEC 25051. In many application areas, especially in mission-critical applications, certification is essential or even mandatory. However, the certification of software products using common off-the-shelf (COTS) components is difficult to attain, as detailed information about COTS is seldom available. Nevertheless, software products are increasingly being based on COTS components, which mean that traditional certification processes should be enhanced to take COTS into account in an effective way. This paper proposes a mean to help in the certification of component-based systems through an experimental risk assessment methodology based on fault injection and statistical analysis. Using the proposed methodology the certification authority or the system integrator can compare among components available the one that best fit for the system that is assembling a component that provides a specific functionality. Based on the results it is also possible to decide whether a software product may be considered certified or not in what concerns the risk of using a COTS into the system. The proposed approach is demonstrated and evaluated using a space application running on top of two alternative COTS real-time operating systems: RTEMS and RTLinux.
The injection of software faults in software components to assess the impact of these faults on other components or on the system as a whole, allowing the evaluation of fault tolerance, is relatively new compared to decades of research on hardware fault injection. This paper presents an extensive experimental study (more than 3.8 million individual experiments in three real systems) to evaluate the representativeness of faults injected by a state-of-the-art approach (G-SWFIT). Results show that a significant share (up to 72 percent) of injected faults cannot be considered representative of residual software faults as they are consistently detected by regression tests, and that the representativeness of injected faults is affected by the fault location within the system, resulting in different distributions of representative/nonrepresentative faults across files and functions. Therefore, we propose a new approach to refine the faultload by removing faults that are not representative of residual software faults. This filtering is essential to assure meaningful results and to reduce the cost (in terms of number of faults) of software fault injection campaigns in complex software. The proposed approach is based on classification algorithms, is fully automatic, and can be used for improving fault representativeness of existing software fault injection approaches.
Despite of the existence of several techniques for emulating software faults, there are still open issues regarding representativeness of the faults being injected. An important aspect, not considered by existing techniques, is the non-trivial activation condition (trigger) of real faults, which causes them to elude testing and remain hidden until operation.






Thefaultloadisoneofthemostcriticalelementsofexperimentaldependabilityevaluation.Itshouldembodyarepeatable,portable,representativeandgenerallyacceptedfaultset.Concerningsoftwarefaults,thedefinitionofthatkindoffaultloadsisparticularlydifficult,asitrequiresamuchmorecomplexemulationmethodthanthetraditionalstuck-atorbit-flipusedforhardwarefaults.Althoughfaultloadsbasedonsoftwarefaultshavealreadybeenproposed,thechoiceofadequatefaultinjectiontargets(i.e.,actualsoftwarecomponentswherethefaultsareinjected)isstillanopenandcrucialissue.Furthermore,knowingthatthenumberofpossiblesoftwarefaultsthatcanbeinjectedinagivensystemispotentiallyverylarge,theproblemofdefiningafaultloadmadeofasmallnumberofrepresentativefaultsisofutmostimportance.Thispaperpresentsacomprehensivefaultinjectionstudyandproposesastrategytoguidethefaultinjectiontargetselectiontoreducethenumberoffaultsrequiredforthefaultloadandexemplifiestheproposedapproachwitharealweb-serverdependabilitybenchmarkandalarge-scaleintegervectorsortapplication.
With the rise of software complexity, software-related accidents represent a significant threat for computer based systems. Software Fault Injection is a method to anticipate worst-case scenarios caused by faulty software, through the deliberate injection of software faults. This survey provides a comprehensive overview of the state-of-the-art on Software Fault Injection, to support researchers and practitioners in the selection of the approach that fits best their dependability assessment goals, and it discusses how these approaches have evolved to achieve fault representativeness, efficiency, and usability. The survey includes a description
of relevant applications of Software Fault Injection in the context of fault-tolerant systems. (the paper has 57 pages)
Data warehouses are of crucial importance to decision-making in competitive organizations. The fact that they store enormous quantities of data is a challenge in what concerns performance and scalability, as users request instant answers. None of the traditional performance strategies is sufficiently good to make complex aggregation queries take only minutes or seconds. The summary warehouse (SW)  achieves such a speedup by storing only general-purpose sampling summaries well-fit for aggregated exploration analysis.  

The major limitation of SWs results from the tradeoff between accuracy and speed: smaller, faster summaries cannot answer less-aggregated queries. 

In this paper we present the Bag-of-Summaries approach (BofS) designed to deal effectively with this problem: BofS maintains a set of summaries with varied sizes and chooses the right one to answer a query with the desired accuracy and best possible speedup, based on query granularity considerations. We also present experimental results that show the advantage of BofS.
Data warehouses are of crucial strategic importance to decision-making in many competitive organizations. The fact that they store enormous quantities is a challenge in what concerns performance and scalability, as Gigabytes or Terabytes of data are stuffed in those systems, while users request instant answers to their queries. It is possible to return very fast approximate answers to user queries in exploration phases using pre-computed summaries. For instance, a query that would take two hours can be completed in one minute or less using a summary. Sampling has been proposed as a strategy to produce general-purpose summaries well-fit for all types of exploration analysis. However, their usage is constrained by the fact that there must be a representative number of samples in grouping intervals to yield acceptable accuracy. In this paper we propose and evaluate a technique that deals with the representation issue by using time interval-biased stratified samples (TISS). The technique is able to deliver fast accurate analysis to the user by taking advantage of the importance of the time dimension in most user analysis. It is designed as a transparent middle layer, which analyzes and rewrites the query to use a summary instead of the base data warehouse. The technique is presented and evaluated experimentally in a typical TPC-H setup. The estimations and error bounds returned using the technique are compared to those of traditional sampling summaries, to show that it achieves significant improvement in accuracy.
In large data warehousing environments, it is often advantageous to provide fast, approximate answers to complex aggregate queries based on samples of the full data. However, uniformly extracted samples often do not guarantee a number of samples in grouping interval estimations to yield acceptable accuracy. This is crucial in most practical less-aggregated analyses, which are mostly based on recent data (e.g. forecasting, performance analysis). 

We propose the use of time-interval stratified samples (TISS), a simple time-biased sampling strategy that produces summaries biased towards recency. This bias minimizes the representational issue and improves the accuracy in important less-aggregated analysis without significantly deteriorating aggregated analysis on older data. 

TISS obtains a much better accuracy than either uniform or the recently proposed congressional samples (CS) for queries analyzing recent data, while maintaining full ad-hoc usability. Furthermore, we show that TISS can be coupled with CS to combine very accurate TISS-like estimations with CS minimal representation guarantees on defined query patterns (TISS-CS). The use of CS in this context is important to provide an additional minimal representation guarantee on the less-well represented older data.


Data warehouses (DW) that store enormous quantities of data put a major challenge in what concerns performance and scalability, as users request instant answers to their queries. Traditional solutions rely on very expensive architectures and structures for speedup and scale-up. The Summary warehouse (SW) is an inexpensive solution that has the potential to deliver very fast approximate answers to aggregate queries using only general-purpose sampling summaries.  
Although summaries are expected to be extremely fast, some analysis requires larger summaries to estimate individual group results, compromising the speedup advantage. This is the accuracy/speedup (A/S) tradeoff. 
In this paper we propose the 'Distributed Set-of-Summaries for Quality of Serviceâ? (DSQoS) that solves the A/S issue by optimizing the accuracy and response time for each query pattern in order to guarantee a desired Quality of Service (QoS). This QoS is defined in terms of response time and accuracy bounds. The strategy determines the required summary size to guarantee the accuracy targets and then dynamically select a set of summaries, distributed in various nodes, which can ensure the QoS constraints (time and accuracy). The strategy presents enormous possibilities since each node can contain summaries with different sizes, depending on the node characteristics, and can dynamically be added and removed from the system.
We discuss the design of the approach and the strategies used to process queries. In the experimental section we show how the approach is able to deliver almost instant and accurate answers without employing expensive architectures, which would be impossible using other strategies.

This paper discusses efficient hash-partitioning using workload access
patterns to place and process relations in a cluster or distributed query-intensive
database environment. In such an environment, there is usually more than one
partitioning alternative for each relation. We discuss a method and algorithm to
determine the hash partitioning attributes and placement. Among the alternatives, our
algorithm chooses a placement that reduces repartitioning overheads using expected or
historical query workloads. The paper includes a simulation study showing how our
strategy outperforms ad-hoc placement and previously proposed distributed database
strategies.
In this paper we propose and evaluate an algorithm
for efficient processing of complex queries in a
partitioned data warehouse. Partitioning allows cheap
computer nodes to be able to host and process efficiently
large data warehouses. In order for the system to be truly
useful, significant speedup should be achieved for all
query patterns. We describe the system, identify complex
join queries and propose algorithms to deal efficiently
with those queries. We also take a set of performance-constrained
computer nodes and the query set from the
decision support performance evaluation benchmark
TPC-H as a study case and evaluation basis for our
analysis.

Parallelism can be used for major performance improvement in large Data warehouses (DW) with performance and scalability challenges. A simple low-cost shared-nothing architecture with horizontally fully-partitioned facts can be used to speedup response time of the data warehouse significantly. However, extra overheads related to processing large replicated relations and repartitioning requirements between nodes can significantly degrade speedup performance for many query patterns if special care is not taken during placement to minimize such overheads. In this paper we show these problems experimentally with the help of the performance evaluation benchmark TPC-H and identify simple modifications that can minimize such undesirable extra overheads. We analyze experimentally a simple and easy-to-apply partitioning and placement decision that achieves good performance improvement results.





In this paper we concentrate on guaranteeing efficient availability and promoting manageability in a node-partitioned data warehouse (NPDW). The objective is that the system be always-on and always efficient even when entire parts of it are taken offline for maintenance and management functions such as loading with new data or other DBA functionality. Replication has already been studied for parallel databases in general. We investigate how alternative replication strategies can be applied to the NPDW context and analyze advantages and drawbacks against metrics.
In a broad sense, overlays can be setup as spaces -'communitiesâ?- for user, data or resource access, publishing and sharing, forming domains within a larger universe. Ad-hoc creation and linking of spaces can replace single linear address spaces or hierarchically structured ones. Our focus is on how can such completely independent and autonomous spaces be created and linked dynami-cally and information routed between them: we propose dynamic space man-agement and inter-space linking and routing alternatives. We compare alterna-tive gateway strategies and gateway hotspot avoidance. We also study the effi-ciency of the proposed schemes analytically.
Data warehouses are repositories of large amounts of historical data and are used primarily for decision support purposes. On the other hand, grids consist on the aggregation of distributed computational resources and presentation of these as a single service with a common interface. The deployment of distributed data warehouses on a grid architecture with QoS control strategies could lead to high levels of flexibility, scalability, reliability and efficiency. However, due to grids characteristics, it could also lead to great challenges. In this paper we investigate an efficient architecture to deploy large data warehouses in grids with high availability and good load balancing. We propose architecture and present experimental results. 








Database servers typically offer a best-effort model of service to submitted commands, that is, they try to process every command as fast as possible. Hence, they are not prepared to provide differentiation for quality of service. In this paper we consider the distributed grid-DWPA architecture context, which fragments and replicates data into several sites to provide an efficient grid data warehouse solution. Instead of offering best-effort service to every query, we propose the use of a performance predict model that is used in conjunction with QoS oriented scheduling to enable the establishment of service level agreements (SLA).
Quality of Service (QoS) related objectives are used in many current applications, like grid-based systems and e-commerce applications. Network managers, application servers and grid resource managers are examples of software that is often QoS-enabled. On the other hand, database servers usually provide a best-effort model, answering each query as fast as possible. Thus, they are not prepared to provide QoS differentiation for incoming queries. In this work, we present an external control system for QoS-oriented scheduling of database requests. Our external system prioritizes queries and automatically adjusts the number of queries that are concurrently executed (degree of multi-programming) in order to achieve user-specified Service Level Objectives (SLO). There are works on external schedulers, admission control systems and on the specification of the degree of multi-programming, but most focus on best-effort models and do not consider Service Level Objectives. Our experimental results show that our approach is effective and that using a specialized scheduler for achieving SLO is very important.

Many global organizations are generating large volumes of highly distributed data that must be shared for querying by the organization's participants, which can be put together by a grid infrastructure. Scheduling query execution in these environment is an important issue. Centralized and hierarchical scheduling architectures are between the most commonly used scheduling architectures in distributed databases and grid computing, respectively. Each architectural model has its own advantages and disadvantages. In this paper, we evaluate the usage of both models in querying highly distributed data. We consider several scheduling policies and experimentally compare scheduling models and policies in terms of throughput and SLO-fulfillment. The obtained results indicate that layered on-demand scheduling may lead to good results in both throughput and SLO-oriented environments.


This paper outlines the conceptualization, design and implementation of a platform for remote real-time control laboratory using the Internet. The platform consists of two fundamental modules the Server application and the Client application. The Internet provides a gateway to transfer data between these two components. JAVA and MATLAB® have been selected as the development languages, and the interface between them implemented with the Jmatlink library.
In the last few years, the Grid technology has emerged as
an important tool for many scientific and commercial global organiza-
tions. In grid-based systems, intelligent job scheduling is used to achieve
Service Level Objectives (SLOs) and to provide some kind of Quality
of Service (QoS) differentiation between users or applications. In data
grids, the grid infra-structure is used to provide transparent access to
geographically distributed data, which may be replicated in order to
increase availability and performance. In this work, we deal with QoS-
oriented query scheduling in data grids. Although there exist several
works on job scheduling in Grids, QoS-oriented query scheduling in grid-
based databases is still an open issue. For instance, how can we provide
guarantees against response-time expectations? Our proposal uses a rep-
utation system to answer this problem satisfactorily. We also present
experimental results that prove the benefits of proposed strategies.
In the last few years, highly distributed, heterogeneous and dynamic environments have become usual contexts for scientific and business domains. In this work, we consider query scheduling over a grid-enabled distributed database, where the data may be partially or totally replicated into the component sites. Although there have been some previous proposals for query scheduling in distributed databases, they did not consider site reputation, which is important in autonomous and heterogeneous distributed systems. We propose a reputation-based election-inspired query scheduling strategy. Sites are autonomous concerning candidacy for answering queries, in which case they must report an expected response time commitment to those queries. A reputation system is used for ranking sites on their response time estimations. Commitment information and subsequent outcome allows the reputation-based election-inspired approach to improve the overall mean response time of the system. We compare it experimentally with other distributed schedulers to show that the use of reputation and elections improves performance in heterogeneous autonomous environments.
In today’s internet-connected data driven world, the demand on high performance data management systems is progressively growing. The data warehouse (DW) concept has evolved from a centralized local repository into a broader concept that encompasses a community service with unique storage and processing capabilities. This increase in popularity has lead to the appearance of new DW architectures and optimizations. In this chapter we propose two key inter-related enabler technologies for this vision: a parallel query optimizer which is able to optimize queries in any parallel DW independently of the underlying database management system &#40;DBMS&#41;, and a scheduling approach for Grid DWs, which decides in which Grid site a query should be executed.We experimentally prove that the approaches allow the community Data Warehouse to work efficiently. 
Globally accessible data warehouses are useful in many commercial and scientific organizations. For instance, research centers can be put together through a grid infrastructure in order to form a large virtual organization with a huge virtual data warehouse, which should be transparently and efficiently queried by grid participants. As it is frequent in the grid environment, in the Grid-based Data Warehouse one can both have resource constraints and establish Service Level Objectives (SLOs), providing some Quality of Service (QoS) differentiation for each group of users, participant organizations or requested operations. In this work, we discuss query scheduling and data placement in the grid-based data warehouse, proposing the use of QoS-aware strategies. There are some works on parallel and distributed data warehouses, but most do not concern the grid environment and those which do so, use best-effort oriented strategies. Our experimental results show the importance and effectiveness of proposed strategies.
Many global organizations are generating huge volumes of data, which are stored in highly distributed databases. These databases can be put together through a Grid infrastructure in order to form a large virtual data warehouse, which is physically distributed but can be transparently queried by Grid participants. But in Grids the system environment is heterogeneous and resource availability may vary over time, which may lead to performance degradation and data unavailability. In this chapter, the authors present Grid-NPDW which uses specialized data placement and job scheduling strategies in order to construct a Grid-based data warehouse with high performance and availability.
The computational grid offers services for efficiently scheduling jobs on the grid, but for grid-enabled applications where data handling is a most relevant part, the data grid kicks in. It typically builds on the concept of files, sites and file transfers between sites. These use a data transfer service, plus a replica manager to keep track of where replicas are located. The authors consider a multi-site, grid-aware data warehouse, which is a large distributed repository sharing a schema and data concerning scientific or business domains. Differently from typical grid scenarios, the data warehouse is not simply a set of files and accesses to individual files. It is a single distributed schema and both localized and distributed computations must be managed over that schema. Given this difference, it is important to study approaches for placement and computation over the grid data warehouse and this is our contribution in this book chapter.
Real deployments of wireless sensor networks (WSN) are rare, and virtually all have considerable limitations when the application in critical scenarios is concerned. On one side, research in WSNs tends to favour complex and non-realistic mechanisms and protocols and, on the other side, the responsible for the critical scenarios, such as the industry, still prefer well-known but expensive analog solutions. However, the aim of the GINSENG Project is to achieve the same reliability of WSNs that the conventional analog systems provide, by controlling the network performance. In this paper we present the GINSENG architecture and the platform that have been implemented in a real scenario, considered one of the most critical in the world: an Oil Refinery.
This paper proposes an approach to improve the level of Quality of Experience (QoE) that distributed database systems provide. 

Quality of Experience is a measure of users' satisfaction when using a certain service or application. Therefore, the main objective of this paper is to provide mechanisms to increase users' satisfaction when accessing distributed database systems.

In traditional database systems, users cannot specify execution-related constraints. Then, the database system cannot evaluate if user expectations are satisfied and neither the system can take corrective actions when necessary.  

In this work, we present the QoE-oriented distributed database system &#40;QoE-DDB&#41;. It allow users to specify Data Access Requirements (DARs) and aims to please users by satisfying the DARs they define. We define a set of types of Data Access Requirements and propose some SQL extensions that enable users to specify execution-related requirements. Proposed types of DARs include execution deadline and priority, execution start and finish times, data availability and freshness degrees, and disconnected execution mode. 

In our QoE-DDB, each user's command is transformed into one or more tasks that are executed by data services. Community modules and local data services negotiate Service Level Objectives (SLOs) for each task, which improves the system's dependability. We propose both QoE-oriented scheduling and dynamic data placement strategies. Proposed architecture and scheduling strategies enable the system to be used in a wide range of distributed environments, from tightly-coupled homogeneous environments (e.g. composed by off-the-shelf computers connected by a LAN) to highly heterogeneous and geographically distributed systems, where data services have some degree of autonomy. 

Traditional performance indicators (e.g. throughput and response time) are not adequate to measure the QoE a system provides. We also propose some specialized Key Performance Indicators (KPIs) to estimate the QoE level a database system provides. 

Finally, we present experimental results obtained through the use benchmark data and queries together with a prototype that implements proposed strategies. In our experiments, we consider realistic scenarios and compare proposed scheduling strategies with their best-effort oriented counterparts. Obtained results prove the importance of our QoE-oriented approach.
Applications for embedded systems are being spread to areas in which easy configuration by non-programmers will enhance acceptance, and also contexts where reconfiguration is needed during the application lifespan. In this paper we propose an architecture designed to provide a single homogeneous configuration interface that allows configuration and operation of any computation-capable node or group of nodes and hides node heterogeneity, as opposed to hand-coding every component of the system for every minor or major configuration change and/or having multiple configuration systems.
The main contribution of this paper is how to design a middleware architecture with a single uniform component to work with such heterogeneous underlying parts as a embedded network. This advances the current state-of-the-art in middleware, by providing a single component that abstracts the underlying differences in both devices such as PCs and embedded devices and in communications such as tcp and proprietary stacks to create a global processor. We have successfully applied an embodiment of this concept as middleware component in our lab testbed within EU project. The evaluation shows that our middleware delivers as a truly useful hardware-abstracting configurable processor for application adaptation.

Some applications of Wireless sensor networks (WSNs), especially in industrial sense and react scenarios, require fairly fast sampling rates. Considering that a few sensors may share a common sink, sharing part of their path on the way to the sink may result in undesirable message losses and delays that cannot be solved without modifying data communication rates. Our research focuses on planning a WSN to avoid excess traffic during sensing and acting to guarantee the minimal delay for critical scenarios. In this paper we propose an integrated approach to plan, test and reconfigure a network. Initially, our approach gives guidance for a base-plan for the network. With this first-cut plan we test the performance of the network and if necessary reconfigure it. The results of tests given are followed by traffic-level adjustments of the system by several possible techniques: adjustment of number of nodes, network partitions, reduction of the sampling rate or in-network processing with strategies such as aggregation techniques or in-node closed control loops. We evaluate experimentally the proposed approach with two different mechanisms of communication, and different levels of traffic, showing that our planning and reconfiguration allows users to make the best choices for the application context.
Sensor and Actuator Networks (SAN) are distributed systems deployed to sense, monitor and act on the environment. They may include any of the following: sensors, wiring, embedded system components, wireless communication and backbone servers. Given current technological advances, costs and flexibility advantages of wireless sensor networks technology, wireless SAN (WSAN) deployments are being considered in such environments. WSAN nodes are small embedded devices that provide sensing, acting and (limited) in-network processing capabilities. A significant research challenge is how to provide an interface that allows configuration and operation of any computation-capable node or group of nodes in an industrial setting, as opposed to hand-coding every component of the system for every minor or major configuration change and/or having multiple configuration systems. In this paper we propose a configuration approach over industrial WSAN networks. Our approach was successfully tested in an industrial refinery setting, allowing deployers to configure and re-configure the system to meet specific needs.
The GINSENG project is about the development
of a performance-controlled wireless sensor network that can
be used in time-critical applications and hostile environments
such as industrial plant automation and control. GINSENG aims
at integrating wireless sensor networks with existing enterprise
resource management solutions using a middleware while a
cornerstone is ongoing evaluation in a challenging industrial
environment in an oil re?nery in Portugal. In this paper we
?rst present our testbed as it is currently installed. We go on
by introducing our solution to access, debug and ?ash the sensor
nodes remotely from an operations room in the plant or from any
location with internet access. We further present our methodology
when it comes to experiments in the re?nery and also show some
exemplary results from the re?nery testbed.
Although software adaptation has been identified as an effective approach for addressing context-aware applications, the existing work on WSNs fails to support context-awareness and mostly focuses on developing techniques to reprogram the whole sensor node rather than reconfiguring a particular portion of the sensor application software. Therefore, enabling adaptability in the higher layers of the network architecture such as the middleware and application layers, besides its consideration in the lower layers, becomes of high importance. In this paper we propose an approach to hide heterogeneity and offer a single common configuration and processing component for all nodes of that heterogeneous system. This advances the current state-of-the-art, by providing a lego-like model whereby a single simple but powerful component is deployed in any node regardless of its underlying differences, and the system is able to remotely configure and process data in any node in a most flexible way, since every node has the same uniform API, processing and access functionalities.
Wireless sensor networks (WSNs) are deployed to sense, monitor and act on the environment. Deployments in scenarios such as industrial sense and react environments require a set of functionality, and for both ease and reconfiguration capabilities it is important to offer an appropriate framework. We propose a framework for interaction with real-world devices by abstracting proprietary protocols, allowing the interaction between client applications and heterogeneous sensor network platforms and protocols. The framework allows the (re)configuration of alarms, actions or closed-loop techniques, offering flexibility and the possibility to modify for providing performance control guarantees. It allows users to configure and apply various operations, including complex closed-loop techniques that monitor and act over any actuator in the WSN, independently of the underlying WSN infrastructure. The framework is being deployed in a real scenario in the context of European FP7 GINSENG project (wireless networks with performance control guarantees).
Industrial plants such as oil refineries typically use wired sensor systems to monitor and control the production processes. As the deployment and maintenance of such cabled systems are expensive, it is desirable to replace or augment these systems using wireless technology.  Wireless sensor networks (WSNs) are deployed to sense, monitor and act on the environment. Especially in industrial sense and react scenarios that require fairly fast sampling rates, minimum losses and minimum latency on messages. Considering that a few sensors may share a common sink, sharing part of their path on the way to the sink may result in undesirable message losses and delays. Our research focuses on planning a WSN to avoid excess traffic during sensing and acting to guarantee the minimal delay for critical scenarios, such as industrial applications and closed-loop control applications. In this paper we report on our experience with both a simple plan for adapting a WSN for high-rate sampling objectives and with experimental tests. Simple theoretical results let us know whether a network plan will be able to handle data processing as required. With this first-cut plan we can then test the performance of the network and if necessary reconfigure it. Basic data quality issues are also taken into account. Different characteristics can be chosen depending on application needs, resulting in reconfiguration of network parameters (number of nodes, network partitions or reduction of the sampling rate or application of filters to the data). We report on the experimental results in a testbed and industrial environment.
Distributed Sensor and actuator networks (SANs) are deployed to sense, monitor and act on the environment, Wireless Sensor Networks offer some flexibility and lower installation cost advantages in those settings and are integrated within a distributed heterogeneous network. The underlying infrastructure may include wired or wireless devices, PCs and backbone servers arranged in an heterogeneous distributed system. Direct interfacing of sensor and actuator nodes to the industrial networks allow monitoring and control systems to be easily integrated and commanded from external, possibly web-based platforms. These architectures raise important research issues in terms of performance control requirements. 
In this paper we propose a framework for interacting with real-world devices by abstracting proprietary protocols, allowing the interaction between client applications and heterogeneous sensor network platforms and protocols. The architecture was designed to allow the (re)configuration of alarms, actions or closed-loop techniques in any part of the system &#40;sensors or PCs&#41;, offering flexibility and the possibility to modify for providing performance control guarantees. It allows users to configure and apply various operations, including complex closed-loop techniques that monitor and act over any actuator in the network, independently of the underlying node infrastructure. Supported by the proposed architecture we develop a framework that is being deployed in an industrial testbed.  The performance of the prototype implementation will be discussed with emphasis on timing because the readings are used to compute closed-loop control.

This paper presents a platform for heterogeneous industrial wireless sensor and actuator networks (WSANs). We propose an abstraction layer middleware to support context-aware services. The proposed architecture was designed to provide a single homogeneous configuration interface that allows configuration and operation of any computation-capable node or group of nodes and hides node heterogeneity, as opposed to hand-coding every component of the system for every minor or major configuration change and/or having multiple configuration systems. 
The framework allows the (re)configuration of alarms, actions or closed-loop techniques in any part of the system &#40;e.g. remote sensors, gateways or control stations&#41;, offering flexibility and the possibility to modify for providing performance control guarantees. It allows users to configure and apply various operations to monitor and act over any actuator in the WSN, independently of the underlying WSAN infrastructure.
Wireless sensor networks (WSNs) are deployed to sense, monitor and act on the environment. Some applications of these networks, especially in industrial sense and react scenarios, require high performance and guaranties. Our work is focused on building a system that allows configuring/reconfiguring alarms, actions or closed-loop techniques in the context of GINSENG project – wireless sensor networks with performance control guarantees. We propose an approach for interaction with real-world devices through a web services interface, allowing users to configure and apply various operations, including complex closed-loop techniques that monitor and act over any actuator in the WSN. To allow the interaction between a client application and the motes we implemented an API to access services of the motes.
We are witnessing a large increase of Wireless Sensor Network (WSN) deployments, used to sense, monitor and act on the environment, because cable-free solutions are easier to deploy. However, in industrial applications, WSNs still aren’t seen as a viable option because of the required fast sampling rates and the reduced time delay, particularly in a multi-hop deployment where traffic congestion may occur. Message losses are unacceptable, particularly in what concerns critical messages containing actuation instructions or other urgent data, which may have time-constraint requirements that cannot be guaranteed. Our research focuses on integrating traffic-aware data processing strategies and network traffic prioritization to overcome congestion states, in order to guarantee that urgent alarms and commands are enacted in time. Traffic is divided into urgent messages (alarms and actuation commands), that have time-delivery requirements; and the remaining periodic sensed data. In this paper we propose an integrated approach NetDP, which applies adaptable data processing strategies (DP) and traffic reduction (DP-Manager) policies to ensure that application requirements are satisfied with minimal message losses, while simultaneously guaranteeing timely delivery of alarms. We demonstrate that NETDP solution with different data processing strategies and levels of system stress can efficiently guarantee the timely delivery of alarms and actuation messages.
Wireless sensor networks (WSNs) are deployed to sense, monitor 
and act on the environment. Some applications of these networks, especially in industrial sense and react scenarios, require high performance and guaranties. Our work is focused on building a system that allows configuring/reconfiguring alarms, actions or closed-loop techniques in the context of GINSENG project – wireless sensor networks with performance control guarantees. We propose an approach for interaction with real-world devices through a web services interface, allowing users to configure and apply various operations, including complex closed-loop techniques that monitor and act over any actuator in the WSN. To allow the interaction between a client application and the motes we implemented an API to access services of the motes.
Routing in sensor networks is very challenging, due to several characteristics that distinguish them from contemporary communication and wireless ad-hoc networks. Many new goal and data-oriented algorithms have been proposed for the problem of routing data in sensor networks. Most routing protocols can be classified as data-centric, hierarchical, location-based or QoS-aware. Data-centric protocols are query-based and depend on the naming of desired data. Hierarchical protocols aim at clustering the nodes so that cluster heads can do some aggregation and reduction of data in order to save energy. Location-based protocols utilize the position information to relay the data to the desired regions. The QoS-aware are based on general network-flow modeling for meeting some QoS requirements. In this chapter, we will explore the goal and data-oriented routing mechanisms for sensor networks developed in recent years. Our aim is to help better understanding current routing protocols for wireless sensor networks and point out open issues that should be subject to further research.
The star schema model has been widely used as the facto DW storage organization on relational database management systems (RDBMS). The physical division in normalized fact tables (with metrics) and denormalized dimension tables allows a trade-off between performance and storage space while, at the same time offering a simple business understanding of the overall model as a set of metrics (facts) and attributes for business analysis (dimensions). However, the underlying premises of such trade-off between performance and storage have changed. Nowadays, storage capacity increased significantly at affordable prices (below 50$/terabyte) with improved transfer rates, and faster random access times particularly with modern SSD disks. In this paper we evaluate if the underlying premises of the star schema model storage organization still upholds. We propose an alternative storage organization (called ONE) that physically stores the whole star schema into a single relation, providing a predictable and scalable alternative to the star schema model. We use the TPC-H benchmark to evaluate ONE and the star schema model, assessing both the required storage size and query execution time.
CEP and Databases share some characteristics but traditionally are treated as two separate worlds, one oriented towards real-time event processing and the later oriented towards long-term data management. However many real-time data and business intelligence analysis do need to confront streaming data with long-term stored one. For instance, how do current power consumption and power grid distribution compare with last year’s for the same day? 
StreamNetFlux is a novel system that recently emerged to the market designed to integrate CEP and database functionalities. This blending allows the processing of queries that need such capabilities with top efficiency and scalability features.
Current data base management systems (DBMS) compete aggressively for performance. In order to accomplish that, they are adopting new storage schemas, developing better compression algorithms, using faster hardware, optimizing parallel and distributed data processing. Current row-wise systems do not exploit massive ordering redundancy, and current column-wise approaches exploit only partially. An important current research issue concerns replacing optimization and processing complexity by less complex but ultra fast solutions. We propose the varDB approach to optimize performance over data warehouses. The solution minimizes complex operators, by applying a simple scheme and organizing all structures and processing to that end: massive ordering with efficient sorting and log2N searching. Considering data warehouses, with periodic loads and frequent analysis operations, such an approach provides very fast query processing. In our work we show how it is possible to use this massive data ordering/sorting in order to optimize queries for high speed, even without the use of data compression (therefore also avoiding compression/decompression overheads). We dedicate our attention to sort columns of data and correlating them with other replicated and unsorted columns. For querying, we focus on binary-search and the use of mainly offsets. Our tests of loading data, sorting vs. creating indexes and executing very selective operations like data filtering and joining show, using a simple disk based prototype, that we are able to obtain much better performance comparing with optimized row-wise engines, and also improvements when comparing with column-wise optimized engines. Comparing to those we were able to attain at least similar performance for many queries and much better performance for queries with complex joins. 
Quality of service is a key issue in current and future computer systems. Applications run on systems and typically access a backend DBMS which doesn't provide performance-related QoS guarantees, and whose resources are constrained. Throughput increases with the number of concurrent transactions until it reaches a saturation point (optimal EC) where more concurrent transactions lead to a drop in throughput. In order to sustain quality of service, it is essential to use admission control mechanisms that manage congestion and do not admit transactions that can't be executed within relevant quality-of service constraints, thus avoiding the degradation of performance for running transactions. In this paper we use a QoS Brokering architecture to enhance DBMS with QoS capabilities that allow the system to deliver QoS guarantees while sustaining large throughputs with reduced miss ratios. Experimental results are obtained using the TPC-C transactional benchmark.
CEP and Databases have traditionally been viewed as two separate entities, the first one oriented towards real-time event processing and the second one oriented towards long-term management of data. But in fact many real-time data and business intelligence analysis do need to confront streaming data with long-term stored one. For instance, how do today's sales compare with last year's sales for the same day? We demonstrate StreamNetFlux, a new system being commercialized that integrates CEP and databases in a way that is smooth and transparent to the user. Our demonstration will show that the new system is able to process queries that need such capabilities in an integrated fashion and with top efficiency and scalability features. The demonstration will use a Telco case study and will show that not only the system is able to provide integrated processing, as it is also extremely efficient and scalable.
As redes de sensores sem fios são uma tecnologia recente com um âmbito de aplicações muito vasto. No entanto, a sua aplicabilidade tem sido limitada a cenários pouco exigentes ao nível do desempenho, sendo a fiabilidade assegurada por uma elevada redundância. Com a intenção de expandir a aplicação destas redes a ambientes industriais e hospitalares, onde a qualidade de serviço é uma necessidade crítica, surgiram novos desafios científicos que importa estudar, sendo alguns objecto de investigação no âmbito do projecto GINSENG.

Most of my publications
Advances in communications and embedded systems have led to the proliferation of wireless sensor and actuator networks (WSANs) in a wide variety of application domains. One important key of many such WSAN applications is the needed to meet non-functional requirements (e.g., lifetime, reliability, time guarantees) as well as functional ones (e.g. monitoring, actuation). Some application domains even require that sensor nodes be deployed in harsh environments (e.g., refineries), where they can fail due to communication interference, power problems or other issues. Unfortunately, the node failures can be catastrophic for critical or safety related systems. State machines can offer a promising approach to separate the two concerns – functional and non-functional – bringing forth reliability exception conditions handling, by means of fault handling states. We develop an approach that allows users to define and program typical applications using their platform language, but also adds state machine logic to design, view and handle explicitly other concerns such as reliability. The experimental section shows a working deployment of this concept in an industrial refinery setting.
When wireless sensor networks are introduced in industrial settings, they will be part of a much larger heterogeneous sensor and actuation network that includes those sub-networks together with Ethernet cabled Programmable Logic Controllers (PLCs) and control stations. Performance issues arise in such systems. Particularly, actuation latencies are considered crucial in those scenarios. In this paper, we present an approach for planning and evaluating a heterogeneous supervisory control and data acquisition (SCADA) system with wireless sensor networks. We propose closed-loop schemas and mechanisms for measuring and verifying the latency bounds in the whole system. As a case study, we deploy and evaluate our approach on an oil refinery, where several heterogeneous networks are used to monitor and control a water treatment process. Results demonstrated the effectiveness of our method in planning, debugging and providing feedback about the network (in)ability to provide timely actuation guarantees.
Wireless SAN (WSAN) may include wired and/or wireless devices, PCs and control stations arranged in a heterogeneous distributed system. Instead of assuming that embedded device nodes (e.g. MicaZ or TelosB motes), gateways (e.g. PC running Linux) and control stations are disparate entities with their own programming and processing model, it should be viewed as a single heterogeneous distributed system, offering more uniformity, simplicity and flexibility. Enabling adaptivity in the higher layers of the network architecture such as the middleware and application layers, beside its consideration in the lower layers, becomes of high importance. In this paper we propose an approach to hide heterogeneity and offer a single common configuration and processing component for all nodes of that heterogeneous system. In particular, this proposal aims at providing an abstraction to facilitate development of adaptive Wireless Sensor and Actuator Network (WSAN) applications. The main contribution of this paper is how to design a middleware architecture with a single uniform component to work with such heterogeneous underlying parts as a WSAN. This advances the current state-of-the-art in middleware for WSANs, by providing a single component that abstracts the underlying differences in both devices such as PCs and motes and in communications such as TCP and proprietary stacks to create a global processor.

















Deployment of embedded systems in industrial environments requires pre-configuration for operation, and in some contexts easy re-configuration capabilities are also desirable. It is therefore useful to define a mechanism for embedded devices that will operate in sensor and actuator networks to be remotely (re)configured and to have flexible computation capabilities. We propose such a configuration, reconfiguration and processing mechanism in the form of a software architecture. A node component should be deployed in any embedded device and implements API, configuration, processing and communication. The resulting system provides remote configuration and processing of data in any node in a most flexible way, since every node has the same uniform API, processing and access functionalities. The experimental section shows a working deployment of this concept in an industrial refinery setting, as part of the EU FP7 project Ginseng
NoSQL data stores are widely used to store and retrieve possibly large amounts of data, typically in a key-value format. There are many NoSQL types with different performances, and thus it is important to compare them in terms of performance and verify how the performance is related to the database type. In this paper, we evaluate five most popular NoSQL databases: Cassandra, HBase, MongoDB, OrientDB and Redis. We compare those databases in terms of query performance, based on reads and updates, taking into consideration the typical workloads, as represented by the Yahoo! Cloud Serving Benchmark. This comparison allows users to choose the most appropriate database according to the specific mechanisms and application needs.
Relational databases are a technology used universally that enables storage, management and retrieval of varied data schemas. However, execution of requests can become a lengthy and inefficient process for some large databases. Moreover, storing large amounts of data requires servers with larger capacities and scalability capabilities. Relational databases have limitations to deal with scalability for large volumes of data. On the other hand, non-relational database technologies, also known as NoSQL, were developed to better meet the needs of key-value storage of large amounts of records. But there is a large amount of NoSQL candidates, and most have not been compared thoroughly yet. The purpose of this paper is to compare different NoSQL databases, to evaluate their performance according to the typical use for storing and retrieving data. We tested 10 NoSQL databases with Yahoo! Cloud Serving Benchmark using a mix of operations to better understand the capability of non-relational databases for handling different requests, and to understand how performance is affected by each database type and their internal mechanisms.
NoSQL data stores appeared to fill a gap in the database market: that of highly scalable data storage that can be used for simple storage and retrieval of key-indexed data while allowing easy data distribution over a possibly large number of servers. Cassandra has been pinpointed as one of the most efficient and scalable among currently existing NoSQL engines. Scalability of these engines means that, by adding nodes, we could have more served requests with the same performance and more nodes could result in reduced execution time of requests. However, we will see that adding nodes not always results in performance increase and we investigate how the workload, database size and the level of concurrency are related to the achieved scaling level. We will overview Cassandra data store engine, and then we evaluate experimentally how it behaves concerning scaling and request time speedup. We use the YCSB – Yahoo! Cloud Serving Benchmark for these experiments.
Benchmarking is a common practice for the evaluation of database computer systems. By executing certain benchmarks, manufacturers and researchers are able to highlight the characteristics of a certain system and are also able to rank the system against the rest. On the other hand, at the moment, BigData is a hot topic. It concerns dealing efficiently with information that is challenging to handle, due to volume, velocity or variety. As more and more platforms are proposed to deal with BigData, it becomes important to have benchmarks that can be used to evaluate performance characteristics of such platforms. At the same time, Decision Support applications are related to BigData, as they need to efficiently deal with huge datasets. In this paper we describe benchmarks representing Decision Support Systems (TPC-H, SSB, TPC-DS), and benchmarks for the Big Data class (YCSB, BigBench, and BigFrame), in order to help users to choose the most appropriate one for their needs. We also characterize the relationship between Big Data benchmarks and Decision Support benchmarks.
NoSQL databases were developed as highly scalable databases that allow easy data distribution over a number of servers. With the increased interest of researchers and companies in non-relational technology, NoSQL databases became widely used and a common belief emerged defending that those engines scale well. This means that the use of more nodes would result in reduced execution time of requests and the system would scale adequately, by adding nodes proportionally to data size and load. However, sometimes, adding nodes may not result in improvement of request-serving time. Therefore, it is useful to investigate how different factors, such as workload, data size and number of simultaneous sessions influence scaling capabilities. We will review the architecture of Cassandra, which is known for being one of the most efficient NoSQL engines, and analyze its scalability, using the Yahoo Cloud Serving Benchmark. The results will allow a better understanding of scalability and scalability limitations in that type of environment.
In computing, a benchmark is the result of running a set of computer programs or a computer program, in order to assess relative performance by running a series of standard tests. By doing this, researchers highlight the characteristics of certain systems and are able to rank the system against the rest. On the other hand, BigData is a hot topic. It not only deals with large amounts of data sets and the procedures and tools used to analyze and manipulate them, but also to a computational turn in research and thought. At the same time, Decision Support applications are related to Big Data as they need to deal with large datasets. In this paper we describe two of the most popular benchmarks, one representing Decision Support Systems (TPC-H), and the other represents the Big Data class (YCSB - Yahoo Cloud Serving Benchmark).
Scalability is essential for web applications with massive numbers of users. The high growth rates observed in web based systems having hundreds of thousands of users accessing it continuously led to major response time problems for users who are trying to receive information at the same time. As more and more users access a web site, one needs to know how the performance varies. The main challenge Facebook engineer's face, despite receiving billions of users on a daily basis, is keeping the website online and functional. The current architecture of Facebook is very large and consists of many technologies and thousands of servers. In this paper Facebook's architecture and how it handles scalability issues is going to be described, helping to have a better understanding of how Facebook actually works.

In this paper we address the challenge of creating an electronic ticketing system for transportation systems that can partially or completely run on the cloud. This challenge is defined within the scope of an industrial project. The resulting system should be able to reach a large spectrum of customers and should provide two key advantages: lower operational costs, especially for small clients without IT departments, and faster execution of queries for monthly or other sorts of analysis, using the elasticity of cloud-based resources.

To fulfill the goals of the project, we propose very standard technologies and procedures: a three-tiered architecture; a separation of the online and analysis databases; and an Enterprise Service Bus to get the input from very diverse hardware and software stacks. In this paper we discuss several options regarding the location of these facilities on the cloud and we also evaluate the costs involved.

While this work already defines many features of the system, it must be considered as preliminary, as some open details remain for future work.

Cassandra is an open source distributed data store system designed for managing and storing huge amounts of data. It can serve as both a read-intensive database for large-scale business intelligence systems and a real-time operational data store for online transactional applications. In this article, we describe three of the most relevant benchmarks that were developed to assess both NoSQL and big data capabilities and complex analytic functionality (YCSB, BigBench, and TPC-DS). We also review the Cassandra database explaining its characteristics and operational principles. The main focus of this paper is to explain the three benchmarks and Cassandra in order to present the results we obtained from this database.
The big data era brought a set of new database features, such as parallel execution of requests and data distribution among different nodes as well as new types of databases. NoSQL technology emerged to aid people and companies to easily scale systems with simple and transparent data distribution. It became possible to cope with higher demand in less time while performing different types of operations and storing large amounts of data. In this paper, we evaluate Cassandra's scalability and execution time of CRUD operations and, posteriorly, compare one relational and one non-relational system by evaluating their performance during execution of decision support queries. For that purpose, we used two standard benchmarks, Yahoo! Cloud Serving Benchmark, to evaluate execution time of requests and speedup of Cassandra, and Star-Schema Benchmark, to run queries over MySQL cluster, as relational database, and Hadoop with Hive as non-SQL counterpart. We conclude about the capabilities and limitations of those systems.
Decision support systems are increasingly used in the management of industrial enterprises, the state sector and the scientific community. With the increase of size of transactional and log data sources, as well as the increase in size and speed needs for data warehousing systems that support decision-making, the data becomes big and queries used in these systems are very complex and time-consuming. This leads to the need for large-scale processing in acceptable time intervals. In this paper, we assess whether both SQL and NoSQL scalability platforms are able to process efficiently big data warehouses. We evaluate the database engine MySQL cluster, which enables clustering and scalability of in-memory databases in a shared nothing platform, and we compare it to traditional database engine Microsoft SQL Server 2012 (non-parallel) and to Hive. In the experimental evaluation, we use one appropriate benchmark to evaluating decision support systems, the Star Schema Benchmark (SSB). The results obtained allow us to estimate the improvement obtained with the increase of resources allocated to the MySQL cluster or Hive, and to obtain a better understanding of scalability limitations and characteristics when these engines are used for decision support systems.


Data mining is the process of discovering patterns in large datasets. With the exponential growth of available information, new machine learning, statistics and other analytics techniques have to be developed to solve the processing needs required to do such analysis fast enough to be used successfully. In this study, techniques like cluster analysis are used over generated data in order to do customer segmentation, and the system performance is evaluated by measuring the processing time. The data used in the current paper is generated using the Star Schema Benchmark (SSB). Our main goal is to find a scalable solution to run data mining over a decision support benchmark. Four different systems will be tested: single node MySQL, MySQL cluster, Apache Mahout and R. By running MySQL cluster and Mahout, each system distributed by four nodes, the paper compares the performance of k-means run in parallel. MySQL and R will allow for comparison of this kind of execution against methods running on a single machine, both on relational and non-relational systems.

As falhas de operador são actualmente consideradas uma das principais causas de indisponibilidade em sistemas de gestão de bases de dados (SGBD). Apesar da maior parte dos SGBD existentes possuírem mecanismos de recuperação bastante completos, a eficiência desses mecanismos é difícil de caracterizar. Por outro lado, a optimização de grandes SGBD é uma tarefa complexa e os administradores tendem em concentrar-se no desempenho, esquecendo muitas vezes os mecanismos de recuperação. A elevada complexidade destes mecanismos e o facto de raramente existir algum retorno sobre a eficiência de uma determinada configuração (em termos de recuperação), facilmente explicam este cenário.
Neste trabalho é proposta uma abordagem experimental para a avaliação do desempenho e da recuperação em SGBD. Esta abordagem baseia-se na extensão de benchmarks para avaliação do desempenho, através da introdução de dois novos elementos: 1) medidas relacionadas com a recuperação; e 2) uma faultload (conjunto de falhas ou condições que levem à activação dos mecanismos de recuperação) baseada em falhas de operador. 
Para além de constituir a primeira proposta de extensão de uma benchmark para avaliação do desempenho, de modo a permitir a caracterização da recuperação, este trabalho propõe ainda uma classificação e um método de emulação para falhas de operador em sistemas de bases de dados. Este método consiste na execução de comandos de administração errados, usando exactamente as mesmas ferramentas utilizadas pelos administradores de bases de dados no seu dia-a-dia (o que garante uma fiel reprodução deste tipo de falhas). As principais falhas de operador possíveis em três SGBD existentes no mercado, são identificadas e classificadas.
A abordagem experimental proposta é demonstrada através de um exemplo concreto de avaliação do desempenho e recuperação de diferentes configurações de um SGBD Oracle 8i, através da extensão da benchmark TPC-C. Esta abordagem experimental é genérica (i.e., pode ser aplicada em qualquer SGBD) e totalmente automática. A tese termina com a análise e discussão dos resultados obtidos, sendo propostos alguns guias que podem ser usados para alcançar o melhor compromisso entre o desempenho e a recuperação.


Many businesses are now moving towards the use of composite web services that are based on a collection of web services working together to achieve an objective. Although they are becoming business-critical elements, current development support tools do not provide a practical way to include fault tolerance characteristics in web services compositions. This paper proposes a mechanism that allows programmers to easily develop fault tolerant compositions using diverse web services. The mechanism allows programmers to specify alternative web services for each operation and offers a set of artifacts that simplify the coding process, by automatically dealing with all the aspects related to the redundant web services invocation and responses voting. The mechanism is also able to perform a continuous evaluation of the services based on their behavior during operation. The approach is illustrated using compositions based on web services publicly available in the Internet and on the web services specified by the standard TPC-App performance benchmark.
Business intelligence is becoming a key component in the
information infrastructure of most organizations. In fact, the increasing
focus on information as a valuable resource for competitiveness is leading
most organizations to build business intelligence environments that support
the decision making activities. These infrastructures are typically
based on very powerful commercial tools. The problem is that, due to the
high cost of this software many companies cannot afford the implementation
of such environments. Open source software is changing the way
companies build their information infrastructures and business intelligence
applications are no exception. The problem is that most available
tools are too specific and cannot be applied in a general manner. In
this paper, we present a BI tool named SAID, which is based on the
integration of several open source software modules.



Many businesses are now moving towards the use of composite web services. These consist of a collection of web services working together to achieve an objective. Although they are becoming business-critical elements, current development tools do not provide a practical way to include fault tolerance characteristics in web services compositions. This paper proposes a mechanism that allows programmers to easily develop fault tolerant compositions using diverse services. The mechanism allows programmers to specify alternative web services for each operation and offers a set of artifacts that simplify the coding process, by automatically dealing with all aspects related to the redundant web services invocation and responses voting. The mechanism is also able to perform a continuous evaluation of the services based on their behavior during operation. The approach is illustrated using compositions based on services publicly available in the Internet and on the services specified by the standard TPC-App performance benchmark.
Testing web services for robustness is a difficult task. In fact, existing development support tools do not provide any practical mean to assess web services robustness in the presence of erroneous inputs. Previous works pro-posed that web services robustness testing should be based on a set of robustness tests (i.e., invalid web ser-vices call parameters) that are applied in order to discov-er both programming and design errors. Web services can be classified based on the failure modes observed. In this paper we present and discuss the architecture and use of an on-line tool that provides an easy interface for web services robustness testing. This tool is publicly available and can be used by both web services providers (to assess the robustness of their web services code) and consumers (to select the services that best fit their re-quirements). The tool is demonstrated by testing several web services available in the Internet.



Seizure prediction in epileptic patients will allow a deep improvement in their quality of life. In the paper a new method using energy relative measures in wavelet coefficients is proposed and tested in several patients. The results show the potential of the technique, but also its limitations, stressing the needs for further work in a larger number of patients, using multimodal information and an advanced database with a large  features set to be used in seizure prediction An advanced computational framework is under development, using multisensorial information to build a large set of features to be used in a classification system supporting seizure prediction. This system is composed of two main parts the algorithms base and the database, briefly described
A computational framework to support seizure predictions in epileptic patients is presented. It is based on mining and knowledge discovery in Elec-troencephalogram (EEG) signal. A set of features is extracted and classification techniques are then used to eventually derive an alarm signal predicting a coming seizure. The epileptic patient may then take steps in order to prevent accidents and social exposure.
Purpose: 
To develop computational intelligence algorithms for seizure prediction to embed in a transportable device to support refractory epileptic patients.

Methods: 
Firstly a set of features is extracted from the EEG, measuring energy, time-frequency and nonlinear dynamic contents. These features are then used for classification of the brain state into four classes: inter-ictal, pre-ictal, ictal, pos-ictal. Two approaches from computational intelligence are applied: (i) artificial neural networks in the original 14 features space (several architectures are compared:  feedforward, with and without memory, radial basis function, Elman), (ii) multidimensional scaling to reduce the 14th dimensional space to 3-dimensional space where classification may be done in an easier way.
A methodology is proposed and developed for epileptic seizures prediction through multifeatures extracted from EEG, and submitted to space reduction. Concepts from energy, frequency-time, and nonlinear dynamics are used to obtain the set of 14 features. Multidimensional scaling is used for space reduction from high dimensional to three dimensional space, in the VISRED platform. Results show the potentiality of this methodology. A computational system with an algorithms base and a data base, under development, is briefly sketched, in order to allow to face the high variability of biological systems
Research and practice show that a large number of web services are deployed with robustness problems (i.e., presenting unexpected behaviors in the presence of invalid inputs). Test-driven development, particularly suitable for web service environments, is a software development technique based on predefined test cases that are used during development to validate the desired software improvements or new functionalities. However, building representative test cases is quite difficult and developers often miss the test cases for robustness validation. This paper demonstrates how robustness testing can be integrated in the test driven development process to improve web services robustness. To demonstrate the proposed approach we have invited experienced developers to implement three different versions of the web services specified by the standard TPC-App performance benchmark, two following the standard test-driven development process and the other using the approach proposed in this paper. Results suggest that test-driven development coupled with robustness testing is a practical way to prevent the deployment of services with robustness problems.
Web services represent a powerful interface for back-end systems that must provide a robust interface to client applications, even in the presence of invalid inputs. However, developing robust services is a diffi- cult task. In this paper we demonstrate wsrbench, an online tool that facilitates web services robustness test- ing. Additionally, we present two scenarios to motivate robustness testing and to demonstrate the power of robustness testing in web services environments.
Web services are often deployed with critical software bugs that can be maliciously exploited. Web vulnerability scanners are regarded as an easy way to test Web applications against security vulnerabilities. However, previous research shows that the effectiveness of these tools in Web services environments is very poor. In fact, the high number of false-positives and the low coverage observed in practice highlight the strong limitations of these tools. The goal of this paper is to demonstrate that it is possible to develop a vulnerability scanner for Web services that performs much better than the commercial ones currently available. Thus, we propose an approach to detect SQL injection vulnerabilities, one of the most common and most critical types of vulnerabilities in web environments. Experimental evaluation shows that our approach performs much better than well-known commercial tools, achieving very high detection coverage while maintaining the false positives rate quite low.


<b><a href="http://ieeexplore.ieee.org/stamp/stamp.jsp?tp=&arnumber=5234322">Download from IEEE Xplore</a> </b>




Web services are becoming business-critical components that must provide a non-vulnerable interface to the client applications. However, previous research and practice show that many web services are deployed with critical vulnerabilities. SQL injection vulnerabilities are particularly relevant, as Web services frequently access a relational database using SQL commands. Penetration testing and static code analysis are two well-know techniques often used for the detection of security vulnerabilities. In this work we compare how effective these two techniques are on the detection of SQL injection vulnerabilities in Web services code. To understand the strengths and limitations of these techniques, we used several commercial and open source tools to detect vulnerabilities in a set of vulnerable services. Results suggest that, in general, static code analyzers are able to detect more SQL injection vulnerabilities than penetration testing tools. Another key observation is that tools implementing the same detection approach frequently detect different vulnerabilities. Finally, many tools provide a low coverage and a high false positives rate, making them a bad option for programmers.


<b><a href="http://ieeexplore.ieee.org/stamp/stamp.jsp?tp=&arnumber=5369093">Download from IEEE Xplore</a> </b>
Many organizations have to manage an increasingly large number of software projects. In many cases, these projects are outsourced to different companies or developed across several departments. This creates a problem because it is increasingly difficult for a project manager, responsible for several projects, to have an accurate view of all activities underway, being able to act proactively before problems occur. In this paper we present an approach and corresponding implementation for effectively tracking and managing multiple projects across an organization, offering an integrated view of the state of projects being implemented. This approach allows to effectively monitoring risks, progress, budget, deliveries, and other critical aspects, allowing responding in real-time. The system was implemented for the European Space Agency, being now successfully used in a number of projects. This paper also presents an accurate view on how software development and tracking takes place in the scope of missioncritical systems.
-
Testing web services for robustness is an effective way of disclosing software bugs. However, when executing robustness tests, a very large amount of service responses has to be manually classified to distinguish regular responses from responses that indicate robustness problems. Besides requiring a large amount of time and effort, this complex classification process can easily lead to errors resulting from the human intervention in such a laborious task. Text classification algorithms have been applied successfully in many contexts (e.g., spam identification, text categorization, etc) and are considered a powerful tool for the successful automation of several classification-based tasks. In this paper we present a study on the applicability of five widely used text classification algorithms in the context of web services robustness testing. In practice, we assess the effectiveness of Support Vector Ma-chines, Naïve Bayes, Large Linear Classification, K-nearest neighbor (Ibk), and Hyperpipes in classifying web services responses. Results indicate that these algorithms can be effec-tively used to automate the identification of robustness issues while reducing human intervention. However, in all mechanisms there are cases of misclassified responses, which means that there is space for improvement.
Vulnerability detection tools are frequently considered the silver-bullet for detecting vulnerabilities in web services. However, research shows that the effectiveness of most of those tools is very low and that using the wrong tool may lead to the deployment of services with undetected vulnerabilities. In this paper we propose a benchmarking approach to assess and compare the effectiveness of vulnerability detection tools in web services environments. This approach was used to define a concrete benchmark for SQL Injection vulnerability detection tools. This benchmark is demonstrated by a real example of benchmarking several widely used tools, including four penetration-testers, three static code analyzers, and one anomaly detector. Results show that the benchmark accurately portrays the effectiveness of vulnerability detection tools and suggest that the proposed approach can be applied in the field.


Received the <b>Best Paper Award </b> at the <a href="http://conferences.computer.org/icws/2010/">IEEE 8th International Conference on Web Services (ICWS 2010)</a>


<b><a href="http://ieeexplore.ieee.org/stamp/stamp.jsp?tp=&arnumber=5552783">Download from IEEE Xplore</a> </b>






Transactional systems are the core of the information systems of most organizations. Although there is general acknowledgement that failures in these systems often entail significant impact both on the proceeds and reputation of companies, the benchmarks developed and managed by the Transaction Processing Performance Council (TPC) still maintain their focus on reporting bare performance. Each TPC benchmark has to pass a list of dependability-related tests (to verify ACID properties), but not all benchmarks require measuring their performances. While TPC-E measures the recovery time of some system failures, TPC-H and TPC-C only require functional correctness of such recovery. Consequently, systems used in TPC benchmarks are tuned mostly for performance. In this paper we argue that nowadays systems should be tuned for a more comprehensive suite of dependability tests, and that a dependability metric should be part of TPC benchmark publications. The paper discusses WHY and HOW this can be achieved. Two approaches are introduced and discussed: augmenting each TPC benchmark in a customized way, by extending each specification individually; and pursuing a more unified approach, defining a generic specification that could be adjoined to any TPC benchmark.
Web services are widely used as software components that must provide a robust interface to client applications. Robustness testing is an approach particularly suitable for detecting robustness issues in web services. In fact, several research works have been conducted in the past leading to the proposal of different robustness testing techniques for such environments. However, although of utmost importance, most techniques do not consider the needs of complex systems and services as the ones being developed by the software industry for business and safety critical scenarios. In this industrial practice paper we introduce an enhanced web services robustness testing approach that is being used at SESM Scarl. The proposed technique was built based on actual needs and has been applied in the context of an Air Traffic Control (ATC) application. Results show that companies can successfully use robustness testing during the development of highly complex service based applications.
Testing Web Services (WS) for robustness is a lengthy and arduous process. After testing a set of services, there is typically a very large quantity and variety of test results to be analyzed, which poses a challenge to the developer that has to manually process all results and identify the outputs that indicate the presence of bugs in the code. Previous research indicates that well-known automatic classification algorithms can be used to automate this step. However, the applicability of such algorithms is also limited, as they are frequently unable to deal with the large diversity of outputs present in typical WS scenarios, thus producing incorrect results. In this paper we propose an approach that allows the automatic classification of the results of WS robustness tests. The technique integrates rule-based classification (including domain rules) and conventional machine-learning algorithms trained using generic data. The proposed approach was used to classify a large set of results of tests performed over publicly available WS and also over in-house implementations of several TPC benchmarks. Results show the effectiveness of the technique, indicating that it can be integrated in the robustness testing procedure, enabling, in this way, the delivery of a full end-to-end automatic approach for WSs robustness testing.
Web services are often deployed with critical software bugs that may be maliciously exploited. Developers often trust on penetration testing tools to detect those vulnerabilities but the effectiveness of such technique is limited by the lack of information on the internal state of the tested services. This paper proposes a new approach for the detection of injection vulnerabilities in web services. The approach uses attack signa-tures and interface monitoring to increase the visibility of the penetration testing process, yet without needing to access web service?s internals (as these are frequently not available). To demonstrate the feasibility of the approach we implemented a prototype tool to detect SQL Injection vulnerabilities in SOAP. An experimental evaluation comparing this prototype with three commercial penetration testers was conducted. Results show that our prototype is able to achieve much higher detec-tion coverage than those testers while avoiding false positives, indicating that the proposed approach can be used in real de-velopment scenarios.
Web services are increasingly being used in business critical environments as a mean to provide a service or integrate distinct software services. Research indicates that, in many cases, services are deployed with robustness issues (i.e., displaying unexpected behaviors when in presence of invalid input conditions). Recently, Test-Driven Development (TDD) emerged as software development technique based on test cases that are defined before development, as a way to validate functionalities. However, programmers typically disregard the verification of limit conditions, such as the ones targeted by robustness testing. Moreover, in TDD, tests are created before developing the functionality, conflicting with the typical robustness testing approach. This chapter discusses the integration of robustness testing in TDD for improving the robustness of web services during development. We have requested three programmers to create a set of services based on open-source code and to implement different versions of the services specified by TPC-App, using both TDD and the approach presented in this chapter. Results indicate that TDD with robustness testing is an effective way to create more robust services.
Although no single tool or technique can guard against the host of possible attacks, a defense-in-depth approach, with overlapping
protections, can help secure Web applications.

<b><a href="http://ieeexplore.ieee.org/stamp/stamp.jsp?tp=&arnumber=5999632">Download from IEEE Xplore</a> </b>
Effort estimation in software development projects is far from being an easy task. In fact, despite the several effort estimation techniques available in the literature and the need for companies to perform such task in a daily basis, most small and medium-sized companies still suffer from the problem of incorrect estimations that often result in losing the contract bid or in failure during project execution. In this paper we present and discuss the implementation of a software effort estimation process in a medium-sized company, currently recognized as CMMI Level 5. The paper contextualizes the problem and the company, introduces the estimation techniques used, and presents some preliminary results, showing that software effort estimation can be successfully applied in medium-sized companies at low cost, allowing the reduction of project uncertainty and increasing the probability of success during bidding and execution. 
Data Warehouses (DWs) are the enterprise’s most valuable assets in what concerns critical business information, making them an appealing target for malicious inside and outside attackers. Given the volume of data and the nature of DW queries, most of the existing data security solutions for databases are inefficient, consuming too many resources and introducing too much overhead in query response time, or resulting in too many false positive alarms (i.e., incorrect detection of attacks) to be checked. In this paper, we present a survey on currently available data security techniques, focusing on specific issues and requirements concerning their use in data warehousing environments. We also point out challenges and opportunities for future research work in this field.
Technological evolution has redefined many business models. Many decision makers are now required to act near real-time, instead of periodically, given the latest transactional information. Decision-making occurs much more frequently and considers the latest business data. Since data warehouses (DWs) are the core of business intelligence, decision support systems need to deal with 24/7 real-time requirements. Thus, the ability to deal with continuous data loading and decision support availability simultaneously is critical, for producing continuous actionable knowledge. The main challenge in this context is to efficiently manage the DW’s refreshment, when data sources change, to recapture consistency and accuracy with those sources, while maintaining OLAP availability and database performance. This paper proposes a simple, fast and efficient solution based on database replication and temporary tables to change a traditional enterprise DW into a real-time DW, enabling continuous data loading and OLAP availability on a 24/7 schedule. Experimental evaluations using a real-world DW and the TPC-H decision support benchmark show its advantages and analyze its impact in OLAP performance.
Data Warehouses (DWs) are the enterprise’s most valuable asset in what concerns critical business information, making them an appealing target for attackers. Packaged database encryption solutions are considered the best solution to protect sensitive data. However, given the volume of data typically processed by DW queries, the existing encryption solutions heavily increase storage space and introduce very large overheads in query response time, due to decryption costs. In many cases, this performance degradation makes encryption unfeasible for use in DWs. In this paper we propose a transparent data masking solution for numerical values in DWs based on the mathematical modulus operator, which can be used without changing user application and DBMS source code. Our solution provides strong data security while introducing small overheads in both storage space and database performance. Several experimental evaluations using the TPC-H decision support benchmark and a real-world DW are included. The results show the overall efficiency of our proposal, demonstrating that it is a valid alternative to existing standard encryption routines for enforcing data confidentiality in DWs.
Data Warehouses (DWs) store the golden nuggets of the business, which makes them an appealing target. To ensure data privacy, encryption solutions have been used and proven efficient in their security purpose. However, they introduce massive storage space and performance overheads, making them unfeasible for DWs. We propose a data masking technique for protecting sensitive business data in DWs that balances security strength with database performance, using a formula based on the mathematical modular operator. Our solution manages apparent randomness and distribution of the masked values, while introducing small storage space and query execution time overheads. It also enables a false data injection method for misleading attackers and increasing the overall security strength. It can be easily implemented in any DataBase Management System &#40;DBMS&#41; and transparently used, without changes to application source code. Experimental evaluations using a real-world DW and TPC-H decision support benchmark implemented in leading commercial DBMS Oracle 11g and Microsoft SQL Server 2008 demonstrate its overall effectiveness. Results show substantial savings of its implementation costs when compared with state of the art data privacy solutions provided by those DBMS and that it outperforms those solutions in both data querying and insertion of new data.
Nowadays, most enterprises require near real-time Data Warehouses (DWs) that are able to deal with continuous updates while providing 24/7 availability. Distributed data using round-robin algorithms on clusters of shared-nothing machines is commonly used for improving performance. In this paper, we propose a solution for distributed DW databases that ensures its continuous availability and deals with frequent data loading requirements, introducing small performance overhead. We use a data striping and replication architecture to distribute portions of each fact table among pairs of slave nodes. Each slave node is an exact replica of its partner in the pair. This allows balancing query execution and replacing any defective node, ensuring the system’s continuous availability. The size of each portion in a given node depends on its individual features, namely performance benchmark measures and dedicated database RAM. The estimated cost for executing each query workload in each slave node is also used for balancing and optimizing query performance. We include experiments using the TPC-H decision support benchmark to evaluate the scalability of our solution and show that it outperforms standard round-robin distributed DW setups.



Developers often rely on penetration testing tools to detect vulnerabilities in web services, although frequently without really knowing their effectiveness. In fact, the lack of information on the internal state of the tested services and the complexity and variability of the responses analyzed, limits the effectiveness of such technique, highlighting the importance of evaluating and improving existing tools. The goal of this paper is to investigate if attack signatures and interface monitoring can be an effective mean to assess and improve the perfor-mance of penetration testing tools in web services environ-ments. In practice, attacks performed by such tools are signed and the interfaces between the target application and external resources are monitored (e.g., between services and a database server), allowing gathering additional information on existing vulnerabilities. A prototype was implemented focusing on SQL injection vulnerabilities. The experimental evaluation results clearly show that the proposed approach can be used in real scenarios.
Data Warehouses (DWs) are the core of enterprise sensitive data, which makes protecting confidentiality in DWs a critical task. Published research and best practice guides state that encryption is the best way to achieve this and maintain high performance. However, although encryption algorithms strongly fulfill their security purpose, we demonstrate that they introduce massive storage space and response time overheads, which mostly result in unacceptable security-performance tradeoffs, compromising their feasibility in DW environments. In this paper, we enumerate state-of-the-art data masking and encryption solutions and discuss the issues involving their use from a data warehousing perspective. Experimental evaluations using the TPC-H decision support benchmark and a real-world sales DW support our remarks, implemented in Oracle 11g and Microsoft SQL Server 2008. We conclude that the development of alternate solutions specifically tailored for DWs that are able to balance security with performance still remains a challenge and an open research issue.
Data Warehouses (DWs) are used for producing business knowledge and aiding decision support. Since they store the secrets of the business, securing their data is critical. To accomplish this, several Database Intrusion Detection Systems (DIDS) have been proposed. However, when using DIDS in DWs, most solutions either produce too many false positives (i.e. false alarms) that must be verified or too many false negatives (i.e. true intrusions that pass undetected). Moreover, many approaches detect intrusions a posteriori which, given the sensitivity of DW data, may result in irreparable cost. To the best of our knowledge, no DIDS specifically tailored for DWs has been proposed. This pa-per examines intrusion detection from a data warehousing perspective and the reasons why traditional database security methods are not sufficient to avoid in-trusions. We define the specific requirements for a DW DIDS and propose a conceptual approach for a real-time DIDS for DWs at the SQL command level that works transparently as an extension of the DataBase Management System &#40;DBMS&#41; between the user applications and the database server itself. A prelim-inary experimental evaluation using the TPC-H decision support benchmark is included to demonstrate the DIDS’ efficiency.
Decision support for 24/7 enterprises requires 24/7 available Data Warehouses (DWs). In this context, web-based connections to DWs are used by business management applications demanding continuous availability. Given that DWs store highly sensitive business data, a web-based connection provides a door for outside attackers and thus, creates a main security issue. Database Intrusion Detection Systems (DIDS) deal with intrusions in databases. However, given the distinct features of DW environments most DIDS either generate too many false alarms or too low intrusion detection rates. This paper proposes a real-time DIDS explicitly tailored for web-access DWs, functioning at the SQL command level as an extension of the DataBase Management System, using an SQL-like rule set and predefined checkups on well-defined DW features, which enable wide security coverage. We also propose a risk exposure method for ranking alerts which is much more effective than alert correlation techniques.























Benchmarks have been traditionally tailored to static, unchangeable systems, functioning in well-known and controlled environments. Thus, established benchmarks (and benchmarking approaches) are becoming progressively less representative of real world scenarios, as change is gaining emphasis as a fundamental player in computing systems runtime conditions. As today’s systems are becoming, at the very least, reactive to changes (either endogenous or exogenous) at some level, if not even proactive in reaching their goals more efficiently and effectively, we believe that benchmarks must also evolve, becoming applicable to systems that react to change, adapt, evolve, and have the capability to improve their own performance. In this position paper, we argue that representative changeloads are now as vital as representative workloads, and that changeload-based benchmarks will become a key part in the development and evaluation of SASO systems. We present and discuss some applications of benchmarks in this area, proposing some directions for research.




Web Applications and Services are often deployed with critical software bugs that may be maliciously exploited. The adoption of Service Oriented Architectures (SOAs) in a wide range of organizations, including business-critical systems, opens the door to new security challenges.  The problem is that developers are frequently not specialized on security and the common time-to-market constraints limit an in depth test for vulnerabilities. Additionally, research and practice shows that the effectiveness of existing vulnerability detection tools is very poor. This highlights the need for tools capable of efficiently detecting vulnerabilities in SOAs. This chapter discusses these problems and proposes new techniques and tools to improve services security by detecting vulnerabilities in a SOA in an automated manner.

Service Oriented Architectures are nowadays used in a wide range of organizations to support critical daily operations. Although the underlying services should behave in a secure manner, they are often deployed with bugs that can be maliciously exploited. The characteristics of service-based environments open the door to security challenges that must be handled properly, including services under the control of multiple providers and dynamism of interactions and compositions. This paper presents an extensible tool able to widely test such infrastructures for vulnerabilities. The tool is based in an iterative process that uses interface monitoring to automatically monitor and discover the existing services, resources and interactions, and applies different testing approaches depending on the level of access to each existing services. Two case studies has been developed do demonstrate the tool, and results show that the tool can effectively be used in different service-based scenarios, under different access conditions to the target services.

Protecting Data Warehouses (DWs) is critical, because they store the secrets of the business. Although published research and best practice guides state encryption is the best way to assure the confidentiality of sensitive data and maintain high performance, this adds overheads that jeopardize their feasibility in DWs. In this paper, we propose a Specific Encryption Solution tailored for DWs (SES-DW), using a numerical cipher with variable mixes of eXclusive Or (XOR) and modulo (division remainder) operators. Data storage overhead is avoided by preserving each encrypted column’s datatype, while transparent SQL rewriting is used to avoid I/O and network bandwidth bottlenecks by discarding data roundtrips for encryption and decryption purposes. The experimental evaluation using the TPC-H benchmark and a real-world sales DW with Oracle 11g and Microsoft SQL Server 2008 shows that SES-DW achieves better response time in both inserting and querying, than standard and state-of-the-art encryption algorithms such as AES, 3DES, OPES and Salsa20, while providing considerable security strength.


Online Failure Prediction is a cutting-edge technique for improving the dependability of software systems. It makes extensive use of machine learning techniques applied to variables monitored from the system at regular intervals of time (e.g. mutexes/s, paged bytes/s, etc.). The goal of this work is to assess the impact of considering the time dimension in failure prediction, through the use of sliding windows. The state-of-the-art SVM (Support Vector Machine) classifier is used to support the study, predicting failure events occurring in a Windows XP machine. An extensive comparative analysis is carried out, in particular using a software fault injection technique to speed up the failure data generation process.
Resilience benchmarking is currently the focus of many research initiatives. Assessing and comparing computer systems under changing environments is becoming crucial due to the dynamic characteristics of modern computing environ-ments. Although several metrics have been proposed over the years, there is no universally accepted resilience metric, which hampers the definition of representative, useful, and accepted benchmarks. In this paper we propose a resilience metric, the Specific Corrected Resilience (SCoRe), which portrays the ability of the system to keep operating with a desired level of quality, in spite of the imposed stress, opening the way for benchmarking the resilience of computer systems.
SPEC Research Group - IDS Benchmarking Working Group, Standard Performance Evaluation Corporation (SPEC)
http://research.spec.org/working-groups/ids-benchmarking-working-group.html

Modern intrusion detection systems (IDSes) for virtualized environments are deployed in the virtualization layer with components inside the virtual machine monitor (VMM) and the trusted host virtual machine (VM). Such IDSes can monitor at the same time the network and host activities of all guest VMs running on top of a VMM being isolated from malicious users of these VMs. We refer to IDSes for virtualized environments as VMM-based IDSes. In this work, we analyze state-of-the-art intrusion detection techniques applied in virtualized environments and architectures of VMM-based IDSes. Further, we identify challenges that apply specifically to benchmarking VMM-based IDSes focusing on workloads and metrics. For example, we discuss the challenge of defining representative baseline benign workload profiles as well as the challenge of defining malicious workloads containing attacks targeted at the VMM. We also discuss the impact of on-demand resource provisioning features of virtualized environments (e.g., CPU and memory hot plugging, memory ballooning) on IDS benchmarking measures such as capacity and attack detection accuracy. Finally, we outline future research directions in the area of benchmarking VMM-based IDSes and of intrusion detection in virtualized environments in general.


Find the report <a href="http://research.spec.org/fileadmin/user_upload/documents/wg_ids/endorsed_publications/SPEC-RG-2013-002-BenchmarkingVMMBIDSes.pdf">here</a>.
EWDC is a privileged forum for discussion of current and emerging trends on dependability area, fostering a close and fruitful contact between researchers from both academia and industry.  After the first series, started in 1989 and held on a yearly basis until 2000, EWDC was revived in 2009, running since then every two years, alternatively with the European Dependable Computing Conference (EDCC). In 2009 EWDC was held in Toulouse, and in 2011 in Pisa.
EWDC 2013 focus on dependability and security of software and services. This is a challenging theme, especially when considering that information systems are more and more based on complex, heterogeneous, dynamic software and services, which are characterized by demanding quality attributes. Interoperability in the presence of dependability and security guarantees, as well as techniques and tools to assess the impact of accidental and malicious threats, are among the crucial aspects to be addressed.

This chapter presents a survey on the most relevant software development practices that are used nowadays to build software products for the web, with security built in. It starts by presenting three of the most relevant Secure Software Development Lifecycles, which are complete solutions that can be adopted by development companies: the CLASP, the Microsoft Secure Development Lifecycle, and the Software Security Touchpoints. However it is not always feasible to change ongoing projects or replace the methodology in place. So, this chapter also discusses other relevant initiatives that can be integrated into existing development practices, which can be used to build and maintain safer software products: the OpenSAMM, the BSIMM, the SAFECode, and the Securosis. The main features of these security development proposals are also compared according to their highlights and the goals of the target software product.

The widespread use of SOAs and their specific char-acteristics raise new challenges for V&V practices. This paper presents some of these challenges and introduces Runtime V&V as a possible future solution.
Fault injection is a well-known technology that enables assessing dependability attributes of computer systems. Many works on fault injection have been developed in the past, and fault injection has been used in different application domains. This fast abstract briefly revises previous applications of fault injection, especially for embedded systems, and puts forward ideas on its future use, both in terms of application areas and business markets.
The main problem faced by system administrators nowadays is the protection of data against unauthorized access or corruption due to malicious actions. In fact, due to the impressive growth of the Internet, software security has become one vital concern in any information infrastructure. Unfortunately, software security is still commonly misunderstood. This chapter presents key concepts on security, also providing the basis for understanding existing challenges on developing and deploying secure software systems.
Organizational Information Systems (IS) collect, store, and manage personal and business data. Due to regulation laws and to protect the privacy of users, clients, and business partners, these data must be kept private. This paper proposes a model and a mechanism that allows defining access control policies based on the user profile, the time period, the mode and the location from where data can be accessed. The proposed policy model is simple enough to be used by a business manager, yet it has the flexibility to define complex restrictions. At runtime, a protection layer monitors data accesses and enforces existing pol-icies. A prototype tool was implemented to run an experimental evaluation, which showed that the tool is able to enforce access control with minimal per-formance impact, while assuring scalability both in terms of the number of us-ers and the number of policies.





http://research.spec.org/working-groups/ids-benchmarking-working-group.html


Web Services are a set of technologies designed to support the invocation of remote services by client applications, with the key goal of providing interoperable application-to-application interaction while supporting vendor and platform independence. The goal of this work is to study the real level of interoperability provided by these technologies through a massive experimental campaign involving a wide set of very popular frameworks for web services, implemented using seven different programming languages. We have tested the inter-operation of eleven client-side framework subsystems with three of the most widely used server-side implementations, each one hosting thousands of different services. The results highlight numerous situations where the goal of interoperability between different frameworks is not met due to problems both on the client and the server side. Moreover, we have identified issues also affecting interactions between the client and server subsystems of the same framework.
In a typical web services environment, a web service framework supports the client and server interaction by, among other tasks, announcing the services interfaces and translating application-level service calls to SOAP messages. Although designed to support inter-operation, research and practice suggest that existing client-side and server-side frameworks, many times, cannot fully inter-operate. The problem is that, as web services are increasingly being deployed to support business-critical environments, interoperability issues may prevent or impact business transactions, potentially resulting in huge financial and reputation losses. In this paper we present an experimental evaluation of the interoperability of 1024 publicly available web services, against a set of diverse and well-known client- side web service frameworks. We have detected at least one severe interoperability issue in over 54% of the services tested and quite different inter-operation capabilities regarding the client-side frameworks. Results clearly show that, although providers frequently claim interoperability capabilities, urgent improvements are required.
Web services are supported by a set of protocols that have been designed with the main goal of providing interoperable communication to applications. In typical business-critical services environments the occurrence of interoperability issues can have disastrous consequences, including direct financial costs, reputation, and client fidelity losses. Despite this, experience suggests that interoperability is still quite difficult to achieve, since the heterogeneity of frameworks for providing web services is quite large. In addition, current tools have limited testing capabilities and, in many cases do not specialize in this problem. In this paper we present ITWS, an extensible Interoperability Testing tool for Web Services that is able to assess the interoperability of a web service, supported by any given framework. We have used ITWS to test the interoperability of a set of home-implemented TPC-App web services and a set of thousands of web services created in .NET C# against 11 client-side web service frameworks, including frameworks for mainstream programming languages. Numerous issues have been disclosed, showing the benefits of using ITWS and the importance of testing services for interoperability.
SPEC Research Group - IDS Benchmarking Working Group, Standard Performance Evaluation Corporation (SPEC)
http://research.spec.org/working-groups/ids-benchmarking-working-group.html

Modern virtualized service infrastructures expose attack vectors that enable attacks of high severity, such as attacks targeting hypervisors. A malicious user of a guest VM (virtual machine) may execute an attack against the underlying hypervisor via hypercalls, which are software traps from a kernel of a fully or partially paravirtualized guest VM to the hypervisor. The exploitation of a vulnerability of a hypercall handler may have severe consequences such as altering hypervisor’s memory, which may result in the execution of malicious code with hypervisor privilege. Despite the importance of vulnerabilities of hypercall handlers, there is not much publicly available information on them. This significantly hinders advances towards securing hypercall interfaces. In this work, we provide in-depth technical information on publicly disclosed vulnerabilities of hypercall handlers. Our vulnerability analysis is based on reverse engineering the released patches fixing the considered vulnerabilities. For each analyzed vulnerability, we provide background information essential for understanding the vulnerability, and information on the vulnerable hypercall handler and the error causing the vulnerability. We also show how the vulnerability can be triggered and discuss the state of the targeted hypervisor after the vulnerability has been triggered. 


Find the report <a href="http://research.spec.org/fileadmin/user_upload/documents/wg_ids/endorsed_publications/SPEC-RG-2014-001_HypercallVulnerabilities.pdf">here</a>.
Failure Prediction allows improving the dependability of computer systems, but its use is still uncommon due to scarcity of failure-related data that can be used for training, assessing and comparing alternative failure predictors. As failures are rare events and the characteristics of failure data varies from system to system, in this paper we propose the use of realistic software fault injection to facilitate the generation of failure data on a particular system installation. In practice, we propose a comprehensive experimental approach that allows generating failure data in short time and we study the applicability and limitations of such process in assessing and comparing alternative failure prediction algorithms. A case study is presented comparing four algorithms for predicting failures in a system based on a Windows OS. Results show that using fault injection allows to dramatically speed up the generation of failure data and that the proposed procedure can be used in practice.
Failure Prediction allows improving the dependability of computer systems, but its use is still uncommon due to scarcity of failure-related data that can be used for training, assessing and comparing alternative failure predictors. As failures are rare events and the characteristics of failure data varies from system to system, in this paper we propose the use of realistic software fault injection to facilitate the generation of failure data on a particular system installation. In practice, we propose a comprehensive experimental approach that allows generating failure data in short time and we study the applicability and limitations of such process in assessing and comparing alternative failure prediction algorithms. A case study is presented comparing four algorithms for predicting failures in a system based on a Windows OS. Results show that using fault injection allows to dramatically speed up the generation of failure data and that the proposed procedure can be used in practice.

Web services are often deployed with critical software security faults that open them to malicious attack. Penetration testing using commercially available automated tools can help avoid such faults, but new analysis of several popular testing tools reveals significant failings in their performance.


Issue No.02 - Feb. (2014 vol.47)
pp: 30-36
Databases often support enterprise business and store its secrets. This means that securing them from data damage and information leakage is critical. In order to deal with intrusions against database systems, Database Intrusion Detection Systems (DIDS) are frequently used. This paper presents a survey on the main database intrusion detection techniques currently available and discusses the issues concerning their application at the database server layer. The identified weak spots show that most DIDS inadequately deal with many characteristics of specific database systems, such as ad hoc workloads and alert management issues in data warehousing environments, for example. Based on this analysis, research challenges are presented, and requirements and guidelines for the design of new or improved DIDS are proposed. The main finding is that the development and benchmarking of specifically tailored DIDS for the context in which they operate is a relevant issue and remains a challenge. We trust this work provides a strong incentive to open the discussion between both the security and database research communities.
Data Warehouses (DWs) are used for producing business knowledge and aiding decision support. Since they store the secrets of the business, securing their data is critical. To accomplish this, several Database Intrusion Detection Systems (DIDS) have been proposed. However, when using DIDS in DWs, most solutions produce either too many false-positives (i.e., false alarms) that must be verified or too many false-negatives (i.e., true intrusions that pass undetected). Moreover, many approaches detect intrusions a posteriori which, given the sensitivity of DW data, may result in irreparable cost. To the best of our knowledge, no DIDS specifically tailored for DWs has been proposed. This paper examines intrusion detection from a data warehousing perspective and the reasons why traditional database security methods are not sufficient to avoid intrusions. We define the specific requirements for a DW DIDS and propose a conceptual approach for a real-time DIDS for DWs at the SQL command level that works transparently as an extension of the Database Management System &#40;DBMS&#41; between the user applications and the database server itself. A preliminary experimental evaluation using the TPC-H decision support benchmark is included to demonstrate the DIDS’ efficiency.

Hypervisors are becoming increasingly ubiquitous with the growing proliferation of virtualized data centers. As a result, attackers are exploring vectors to attack hypervisors, against which an attack may be executed via several attack vectors such as device drivers, virtual machine exit events, or hypercalls, which enable  intrusions in hypervisors through their hypercall interfaces. Despite the importance, there is very limited publicly available information on vulnerabilities of hypercall handlers and attacks triggering them, which significantly hinders advances towards monitoring and securing these interfaces. In this experience report paper, we characterize the hypercall attack surface based on analyzing a set of vulnerabilities of hypercall handlers. We systematize and discuss the errors that caused the considered vulnerabilities, and activities for executing attacks triggering them. We also demonstrate attacks triggering the considered vulnerabilities and analyze their effects. Finally, we suggest an action plan for improving the security of hypercall interfaces.
Architecture-based self-adaptation is considered as a promising approach to drive down the development and operation costs of complex software systems operating in ever changing environments. However, there is still a lack of evidence supporting the arguments for the beneficial impact of architecture-based self-adaptation on resilience with respect to other customary approaches, such as embedded code-based adaptation. In this paper, we report on an empirical study about the impact on resilience of incorporating architecture-based self-adaptation in an industrial middleware used to collect data in highly populated networks of devices. To this end, we compare the results of resilience evaluation between the original version of the middleware, in which adaptation mechanisms are embedded at the code-level, and a modified version of that middleware in which the adaptation mechanisms are implemented using Rainbow, a framework for architecture-based self-adaptation. Our results show improved levels of resilience in architecture-based compared to embedded code-based self-adaptation.







Web Services are a technology designed to support the invocation of remote elements by client applications, with the goal of providing interoperable application-to-application interaction while assuring vendor and platform independence. In business-critical environments, the occurrence of in-teroperability issues can have disastrous consequences, including direct financial costs, reputation, and client fidelity losses. Despite this, experience shows that services interoperability is still quite difficult to achieve. The goal of this paper is to propose a practical testing process to understand the real level of interoperability provided by web services platforms. An extensible tool, that implements the proposed approach, has been used to run a large campaign during which we have tested the interoperability of a large number of web services, comprising both home-implemented and publicly available services, de-ployed on top of several web service platforms and against 11 client-side web service platforms. Nu-merous issues have been disclosed, showing the effectiveness of the proposed approach and the im-portance of having an automatic tool for testing web services for interoperability.
Web Services are designed with the key goal of providing interoperable application-to-application interaction, regardless of the platforms involved. Although experience shows that interoperability is difficult to achieve, developers still have limited tools to assess the interoperability of their services and, to the best of our knowledge, none able to support end-to-end interoperability certification. In this paper, we lay the foundations of an interoperability certification process for web services, which allows attesting the interoperability level of a given web service and also identifying possible interoperability issues. In practice, the process can be used by developers or providers to certify a given web service for interoperability, ensuring successful interaction with client-side platforms. We show the effectiveness of the process by conducting a large experimental evaluation to certify five different implementations of the services specified by the TPC-App benchmark, and about 2500 synthetic generated services.

Web services frequently provide business–critical functionality over the Internet, being widely exposed and thus representing an attractive target for security attacks. In particular, Denial of Service (DoS) attacks may inflict severe damage to web service providers, including financial and reputation losses. This way, it is vital that the software supporting services deployment (i.e., the web service framework) is able to provide a secure environment, so that the services can be delivered even when facing attacks. In this paper, we present an experimental approach that allows understanding how well a given web service framework is prepared to handle DoS attacks. The approach is based on a set of phases that include the execution of a large number of well-known DoS attacks against a target framework and the classification of the observed behavior. Results show that four out of the six frameworks tested are vulnerable to at least one type of DoS attack, and indicate that even very popular platforms require urgent security improvements
Online Failure Prediction allows improving system dependability by foreseeing incoming failures at runtime, enabling mitigation actions to be taken in advance, though prediction systems’ learning and assessing is hard due to the scarcity of failure data. Realistic software fault injection has been identified as a valid solution for addressing the scarcity of failure data, as injecting software faults (the most occurring on computer systems) increases the probability of a system to fail, hence allowing the collection of failure-related data in short time. Moreover, realistic injection permits the emulation of software faults likely to exist in the target system after its deployment. However, besides the representativeness of the software faults injected is recognized as a necessary condition for generating valid failure data, studies on the representativeness of generated failure-related data has still not been addressed. In this work we present a preliminary study towards the assessment the representativeness of failure-related data by using G-SWFIT realistic software fault injection technique. We here address the definition of concepts and metrics for the representativeness estimation and assessment.
For over forty years, relational databases have been the leading model for data storage, retrieval and management. However, due to increasing needs for scalability and performance, alternative systems have started being developed, namely NoSQL technology. With increased interest in NoSQL technology, as well as more use case scenarios, over the last few years these databases have been more frequently evaluated and compared. It is necessary to find if all the possibilities and characteristics of non-relational technology have been disclosed. While most papers perform mostly performance evaluation using standard benchmarks, it is nevertheless important to notice that real world scenarios, with real enterprise data, do not function solely based on performance. In this paper, we have gathered a concise and up-to-date comparison of NoSQL engines, their most beneficial use case scenarios from the software engineer viewpoint, their advantages and drawbacks by surveying the currently available literature.
The continuous information growth in current organizations has created a need for adaptation and innovation in the field of data storage. Alternative technologies such as NoSQL have been heralded as the solution to the ever- growing data requirements of the corporate world, but these claims have not been backed by many real world studies. Cur- rent benchmarks evaluate database performance by executing specific queries over mostly synthetic data. These artificial scenarios, then, prevent us from easily drawing conclusions for the real world and appropriately characterize the performance of databases in a real system. To counter this, we used a real world enterprise system with real corporate data to evaluate the performance characteristics of popular NoSQL databases and compare them to SQL counterparts. In particular, we present one of the first write-heavy evaluations using enterprise soft- ware and big data. We tested Cassandra, MongoDB, Couchbase Server and MS SQL Server and compared their performance while handling demanding and large write requests from a real company with an electrical measurement enterprise system.
For over forty years, relational databases have been the leading model for data storage, retrieval and management. However, due to increasing needs for scalability and performance, alternative systems have emerged, namely NoSQL technology. The rising interest in NoSQL technology, as well as the growth in the number of use case scenarios, over the last few years resulted in an increasing number of evaluations and comparisons among competing NoSQL technologies. While most research work mostly focuses on performance evaluation using standard benchmarks, it is important to notice that the architecture of real world systems is not only driven by performance requirements, but has to comprehensively include many other quality attribute requirements. Software quality attributes form the basis from which software engineers and architects develop software and make design decisions. Yet, there has been no quality attribute focused survey or classification of NoSQL databases where databases are compared with regards to their suitability for quality attributes common on the design of enterprise systems. To fill this gap, and aid software engineers and architects, in this article, we survey and create a concise and up-to-date comparison of NoSQL engines, identifying their most beneficial use case scenarios from the software engineer point of view and the quality attributes that each of them is most suited to.
Although Service Oriented Architectures (SOAs) are being increasingly used in business-critical scenarios, the applicability of Verification and Validation (V&V) is still very limited. The problem is that V&V activities have to be implemented at runtime to fit the characteristics of SOA. Recent proposals of runtime V&V techniques specific to SOA domain are far from being complete and a key issue lies in understanding how the “failures propagate” in a dynamic system and how to continuously verify its evolving elements. This paper introduces
an approach to deal with the propagation of failures in a SOA environment. The proposed technique is based on three key steps: estimating the failure rate of the individual services, using fault injection to find the exposure of each service to failures from the invoked services, and estimating the impact of each service in the overall architecture. The overall approach is presented with a brief demonstration of its application.
The continuous information growth in current organizations has created a need for adaptation and innovation in the field of data storage. Alternative technologies such as NoSQL have been heralded as the solution to the ever-growing data requirements of the corporate world, but these claims have not been backed by many real world studies. Current benchmarks evaluate database performance by executing specific queries over mostly synthetic data. These artificial scenarios, then, prevent us from easily drawing conclusions for the real world and appropriately characterize the performance of databases in a real system. To counter this, we used a real world enterprise system with real corporate data to evaluate the performance and space characteristics of popular NoSQL databases and compare them to SQL counterparts. We present one of the first write-heavy evaluations using enterprise software and big data. We tested Cassandra, MongoDB, Couchbase Server and MS SQL Server, comparing their performance and total used space while handling demanding and large write requests from a real company with an electrical measurement enterprise system.












Software faults are currently on of the major cause for computer-based system failures. Nowadays, no software development methodology has provided fault-free software, and fault injection is recognized as a technique to understand the effects of faults in a given software product. However, the injection of software faults is not trivial and is still an open research problem. This chapter provides an overview on the injection of software faults and focus on the injection of faults for Java-based software, highlighting the new challenges specific of this language.
This paper describes the main conclusions of the PA-PDS study . This is a European Space Agency study about the reuse of Pre-Developed Software (PDS) in space projects, using S4S (SPICE for Space) as the framework. The main objective of this study is to define the product assurance requirements to support the acquisition, evaluation, integration and maintenance of PDS to be reused in a new development of a space system. The method proposed for reuse of PDS is described, particularly the main requirements to follow. The project has considered technical and organisational viewpoints. Both the perspective of occasional (or informal) reuse and systematic reuse are addressed.

The increasing number of existing business critical applications targeted for the low end PC market and general purpose operating systems such as Microsoft windows 95/NT poses the problem of guaranteeing continuous avail-ability and data  integrity for those applications. This class of programs include online transaction processing, elec-tronic commerce, Internet/World Wide Web, data warehousing, decision support, online analytical processing (OLAP), control systems, and other business-critical solutions for the finance, retail, and healthcare markets. These consist mainly in server applications of the business world that runs 24 hours a day, i.e. run perpetually providing a service to its clients, demanding high availability, or/and applications that manage business critical data requiring high levels of data integrity. Traditionally, this software runs in highly specialized machines with special operating system support in order to provide the required levels of dependability through fault tolerance [6] [7]. Using off-the-shelf cheap PC's and operating systems without any fault tolerance support for this purpose, as is seen nowadays, can be risky, unreliable and dangerous.
This article presents a library, named WinFT, that provides fault tolerance support for Win32 applications (Windows95/NT) that need high levels of dependability. In this approach, fault tolerance is totally software imple-mented and enables the detection and recovery of software faults (bugs) that cause processes to crash or hang as well as faults in the underlying operating system and hardware that are not handled by the O.S. and hardware them-selves. Basically, WinFT performs automatic detection and restart of failed processes, diagnostic and reboot of a malfunctioning or strangled operating system, checkpointing and recovery of critical volatile data and preventive actions such as software rejuvenation. WinFT was employed successfully in real time control applications for indus-try based on personal computers running Windows 95 in a workgroup environment.


This paper presents WPVM, a PVM implementation for the 'MS Windowsâ? - Microsoft
Windows Operating System1 . WPVM stands for 'Windows Parallel Virtual Machineâ? and
intends to exploit the potential computational power of the small and medium PC LANs that
proliferate in corporations and universities. As the original PVM [1-2], WPVM creates the
abstraction of a parallel machine from a cluster of computers connected by a network. This
virtual machine appears as a single and manageable resource to the programmer. The
difference is that the target machines for WPVM are personal computers with MS Windows
instead of UNIX machines. However, this system is compatible with the original PVM,
meaning that we can use virtual machines composed simultaneously of UNIX and MS
Windows machines.
DRMonitor is a system for monitoring usage of computing resources of networked heterogeneous (Linux and Windows NT's derived) personal computers.  DRMonitor is aimed to serve resource-monitoring applications and to assist load-balancing policies by providing performance and load data about each monitored machine included in the system. Through a reduced set of primitives, applications can periodically receive updated information about system usage. The paper describes available primitives, discusses some internal aspects of the monitoring system, and presents monitoring results of a classroom with 10 personal computers.
This paper describes WMPI , the first full implementation of the Message Passing Interface standard (MPI) for clusters of Microsoft's Windows platforms (Win32). Its internal architecture and user interface are presented, along with some performance test results (for release v1.1), that evaluate how much of the total underlying system capacity for communication is delivered to the MPI based parallel applications. WMPI is based on MPICH, a portable implementation of the MPI standard for UNIXÂ® machines from the Argonne National Laboratory and, even when performance requisites cannot be satisfied, it is a useful tool for application developing, teaching and training. WMPI processes are also compatible with MPICH processes running on Unix workstations.
This paper proposes a new fault detection mechanism for the computation of eigenvalues and eigenvectors, the so called eigenproblem, for which no such scheme existed before, to the best of our knowledge. It consists of a number of assertions that can be executed on the results of the computation to determine their correctness. The proposed scheme follows the Result Checking principle, since it does not depend on the particular numerical algorithm used. It can handle both real and complex matrices, symmetric or not. Many practical issues are handled, like rounding errors and eigenvalue ordering, and a practical implementation was built on top of unmodified routines of the well-known LAPACK library. The proposed scheme is simultaneously very efficient, with less than 2% performance overhead for medium to large matrices, very effective, since it exhibited a fault coverage greater than 99.7% with a confidence level of 99%, when subjected to extensive fault-injection experiments, and very easy to adapt to other libraries of mathematical routines besides LAPACK.
Algorithm Based Fault Tolerance (ABFT) is the collective name of a set of techniques used to determine the correctness of some mathematical calculations. A less well known alternative is called Result Checking (RC) where, contrary to ABFT, results are checked without knowledge of the particular algorithm used to calculate them.
In this paper a comparison is made between the two using some practical implementations of matrix computations. The criteria are performance and memory overhead, ease of use and error coverage. For the latter extensive error injection experiments were made. To the best of our knowledge, this is the first time that RC is validated by fault injection.
We conclude that Result Checking has the important advantage of being independent of the underlying algorithm. It also has generally less performance overhead than ABFT, the two techniques being essentially equivalent in terms of error coverage.
MPI has been extremely successful. In areas like e.g. particle physics most of the available parallel programs are based on MPI. Unfortunately, they must be run in dedicated clusters or parallel machines, being unable to use for long running applications the growing pool of idle time of general-purpose desktop computers. Additionally, MPI offers a quite low level interface, which is hard to use for most scientist programmers. In the research described in this paper, we tried to see how far we could go to solve those two problems, keeping the portability of MPI programs, but drawing upon one restriction - only programs following the FARM paradigm were to be supported. The developed library - MpiFL - did provide us significant insight. It is now being successfully used at the physics department of the University of Coimbra, despite some shortcomings.
This paper addresses the problem of managing distributed mobile agent infrastructures. First,the weaknesses of current mobile agent implementations will be discussed and identi?ed from the manageabilit viewpoint.The solutions devised and experimented in order to alleviate these weaknesses in our own agent platform will then be presented.These solutions are generic and could easily be applied to the majorit of existing mobile agent implementations.The paper will ?nish with the discussion of a new approach we are following in the M&M Project,based on a rather different architecture that signi ?cantly reduces the manageabilit requirements.
Over the last couple of years we have been working on the development of mobile agents systems and its application to the areas of telecommunications and network management. This work path produced positive results: a competitive mobile agent platform was built, the run-time benefits of mobile agents were proved, and our industrial partners have developed practical applications that are being integrated into commercial products. However, despite the positive results, we feel that mobile agent technology is still not ready to enter the path of mainstream software development. In our perspective, one of the main reasons for this situation arises from the traditional approach to mobile agent technology. This approach, based on the familiar concept of the mobile-agent distributed platform as an extension of the operating system, focuses too much on the mobile agents and associated issues (mobility, agent lifecycle, security, coordination, etc.) and provides poor support for the development of applications where mobile agents are just one of several available technologies. Learning from past experience, we are now working on a new approach where the focus is brought back to the applications and mobile agents become just one the tools available to develop distributed systems. This provides a much lighter framework for application-based mobile agent systems. This paper presents the lessons learned from our previous project and discusses the new concept we are developing: application-centric mobile agent systems.
In order for mobile agents to be accepted as a basic technology for enabling electronic commerce, proper security mechanisms must be developed. Hosts must be protected from malicious agents, agents must be protected from other agents and also agents must be protected from malicious hosts. For solving the first three problems, existing technology from operating systems and distributed systems research can be used. The last problem is new and specific to the mobile agent paradigm and it is much harder to solve. Due to this problem, many say that mobile agents are not ready for the e commerce systems. In this paper we discuss the security requirements of mobile agents in the context of electronic commerce and analyze how these requirements can be met. We show that, because of the characteristics of e commerce systems, the security requirements of the agents and their users can be assured in real and open environments as the Internet.
Over the last few years, the importance of mobile computing as been steadily increasing. While it is important to provide support for accessing databases in disconnected computing environments, many of the information systems being deployed today are using a three-tier architecture. It is becoming vital to find ways of providing reliable access to the business-logic present in the middle-tier, for mobile applications.  In this paper, we present a component-based framework that enables client applications to send mobile agents to interact with application servers in disconnect computing environments. This framework allows the applications to benefit from the advantages of using mobile agents in disconnected-computing environments, and at the same time provides integration between mobile clients and corporate information systems.
Mobile agents are a promising technology for developing applications in many application domains. However, it has not yet gained a large acceptance from the developers. One important reason for this is the difficulty of using generic mobile-agent platforms in specific application domains, which have very concrete requisites. In this paper, we present a reusable component-based framework that enables the creation of domain-specific mobile-agent systems in an easy and rapid way. The framework was implemented using the JavaBeans component model, and a fully blown agent platform was built from the components. By using this framework, the development of domain-specify mobile agent platforms is simplified, allowing a more rapid development cycle, which can contribute to an easier spreading of the mobile agent technology.
Although mobile agents are a promising programming paradigm, the actual deployment of this technology in real applications has been far away from what the researchers were expecting. One important reason for this is the fact that in the current mobile agent frameworks it is quite difficult to develop applications without having to center them on the mobile agents and on the agent platforms. In this paper, we present a component-based framework that enables ordinary applications to use mobile agents in an easy and flexible way. By using this approach, applications can be developed using current object-oriented approaches and become able of sending and receiving agents by the simple drag and drop of mobility components. The framework was implemented using the JavaBeans component model and provides integration with ActiveX, which allows applications to be written in a wide variety of programming languages. By using this framework, the development of applications that can make use of mobile agents is greatly simplified, which can contribute to a wider spreading of the mobile agent technology.
Over the last few years, there has been a huge proliferation of mobile agent platforms, for the most different application domains. Although these platforms normally have a very interesting set of features, one common limitation found on most architectures is the lack of support for extensibility, i.e. the ability to add new features at runtime, after the platform has been written and deployed. This is an important question since it limits the usability of the platforms across application domains, and its future usefulness as software requirements change. In this paper, we present a general mechanism for platform extensibility based on binary software components. The mechanism was implemented using the JavaBeans component model and several services where implemented and deployed. The approach undertaken is generic and can be easily adapted into existing mobile agent platforms.
In the Mobile Agent programming model, small threads of execution migrate from machine to machine, performing their operations locally. For being able to deploy such a model into real world applications, security is a vital concern. In the M&M project we have developed a system that departures from the traditional platform-based execution model for mobile agents. In M&M there are no agent platforms. Instead there is a component framework that allows the applications to become able of sending and receiving agents by themselves in a straightforward manner. In this paper we examine the security mechanisms available in M&M, and how integration with existing applications is done. One difficult aspect of this work is that all the features must work with the security mechanisms that already exist on the applications. This is so because the components are integrated from within into the applications, which already have security mechanisms in place. Currently, M&M provides features like fine-grain security permissions, encryption of agents and data, certificate distribution using LDAP and cryptographic primitives for agents. For validating the approach and solutions found, we have integrated the framework into several off-the-shelf web servers, having the security mechanisms running, with no problems.
The mobile agent paradigm provides a new approach for developing distributed systems. During the last two years, we have been working on a project that tries to overcome some of the limitations found in terms of programmability and usability of the mobile agent paradigm in real applications. In the M&M framework there are no agent platforms. Instead applications become agent-enabled by using simple JavaBeans components. In our approach the agents arrive and departure directly from the applications, interacting with them from the inside.  In this paper we discuss our experiences on integrating these components into off-the-shelf web servers, enabling them to receive and send agents. Our approach involves wrapping the components inside a Java servlet that can be included in any web server supporting the Servlet Specification. This servlet enables the servers to receive and send agents that can query local information, and also enables the agents to behave as servlets themselves. This approach departs from the current available systems because it enables any existing web server that supports the servlet specification to send and receive agents in a straightforward way.
Mobile agents provide a new abstraction for deploying functionality over the existing internet infrastructure. During the last two years, we have been working on a project that tries to overcome some of the limitations found in terms of programmability and usability of the mobile agent paradigm in real applications. In the M&M framework there are no agent platforms. Instead applications become agent-enabled by using simple JavaBeans components. In this paper we present an architecture that allows currently available web servers to become capable of sending and receiving agents in an easy way. By using this approach, existing web infrastructure can be maintained, while gaining a whole new potential by being able to make use of agent technology. Our approach involves wrapping the components inside a Java servlet that can be included in any web server supporting the Servlet Specification. This servlet enables the servers to receive and send agents that can query local information, and also enables the agents to behave as servlets themselves.  We currently have used the framework with several existing commercial web servers, inclusively having the security mechanisms of the framework correctly running and integrated with the security architecture of the server.
Mobile Agents provide a new promising paradigm for developing distributed applications. Nevertheless, although the basic concept has been around for some years, and many agent platforms are available both from the industry and research community, there are currently few examples where the technology has been deployed in the real world. One important reason for this is that using the current available agent frameworks it is quite difficult to develop applications without having to center them completely on the agents and on the agent infrastructure. In this paper, we present the M&M project, taking place at the University of Coimbra. In this project, we are developing an extensive component-based framework that enables ordinary applications to use mobile agents in a flexible and easy way. By using this approach, applications can be developed using current object-oriented approaches and become able of sending and receiving agents by the simple drag and drop of mobility components. The framework was implemented using the JavaBeans component model and provides integration with ActiveX, which allows applications to be written in a wide variety of programming languages. By using this framework, the development of applications that can make use of mobile agents is greatly simplified, which can contribute to a wider spreading of the mobile agent technology.
The Java Remote Method Invocation (RMI) API shields the developer from the details of distributed programming, allowing him to concentrate on application specific code. But to perform some operations that are orthogonal to the application, like logging, auditing, caching, QoS, fault tolerance, and security, sometimes it is necessary to customize the default behavior of the RMI runtime. Other middleware for distributed programming, like CORBA and the Remoting framework of the .NET platform, support smart proxies and interceptors, which can be used for these purposes, allowing the separation of application-specific code from service-specific code. In RMI there is no direct way of doing so. This paper presents a framework based on the Dynamic Proxy API for using smart proxies and interceptors with RMI. This framework requires no changes in the client application and minimal changes in the server application, giving the developer greater control over the distributed application. A practical example of use is also given, by using the described framework to implement user authentication and fine-grained access control in RMI.
Mobile agents are typically referred as a very adequate paradigm for using in mobile and disconnected computing environments. They are autonomous, have a strong sense of location and are able to operate independently of a central server. Nevertheless, although this is true, for developing a mobile agent system that is capable of operating in a disconnected computing environment, much more is needed: persistence stores where the agents can be safely stored; agent reactivation mechanisms, which allow agents to migrate when a network connection is detected; policies and notification mechanisms for dealing with changing locations; among other things. In this paper, we present the mechanisms developed for the M&M mobile agent framework in order to address mobile and disconnected computing environments. These mechanisms can be readily adapted to any other platform that needs to deal with disconnected computing.
This paper describes the implementation of MPI-2 one-sided communications (OSC) for the forthcoming WMPI II product, aimed at clusters of workstations (Windows workstations, for now). This implementation is layered directly on top of the WMPI Management Layer (WML), rather than being on top of the MPI layer and as such can draw more performance from the new features of the WMPI's WML. The major features of this implementation are presented, including the synchronization operations, the remote memory operations and the datatype handling mechanism. Performance benchmarks were taken, comparing the message passing and the one-sided communication models, as well as to compare this implementation with one layered on top of MPI.
Software Implemented Fault Injection (SWIFI) is a well-established technique for fault injection, but with a significant drawback for Real-Time Systems: intrusiveness, also known as 'probe effectâ?. In fact, for most fault models, additional code has to be run on the same processor that executes the application. The danger lies in some deadlines being missed as a consequence of that overhead.
This paper identifies the sources of intrusiveness, and discusses the procedures to measure it. The question of what level of intrusiveness can be considered acceptable is also addressed.
A Pentium version of an existing SWIFI tool (Xception), developed with no real-time considerations in mind, was tested on a system composed by off the shelf (COTS) components (a standard PC with a Pentium processor and a commercial real time multitasking kernel). Data collected using this platform shows that the intrusiveness can be quite significant.
A technique called "Routine Specialization" is proposed to lower that overhead. Results obtained from a 'real time orientedâ? injector (RT-Xception) taken from the same system, show a very significant improvement. A comparison with data from other authors shows that with this change SWIFI becomes a viable technique for a wide range of real-time applications.
Stable storage is a type of memory whose contents must survive system malfunctions. It is a key element for fault tolerant systems to perform checkpointing and rollback-recovery. It is usually implemented using hard disks, because of its non-volatile characteristics and robustness against processor faults. However, for many control systems, the low speed and unpredictability of disks are not acceptable due to real-time constraints or, specially on embedded systems, disks are simply not available. In such cases, main memory should be used.
In this paper we present and test a stable storage mechanism relying exclusively on local main memory, and able to survive processor malfunctions. We show that by a careful use of some very common features of processors, like memory protection, we can obtain very high data survivability to system crashes. The study was conducted on COTS computers and a commercial real-time executive running two sample applications, which were subjected to intensive fault injection campaigns. To the best of our knowledge, this is the first time that software-implemented stable storage in RAM has been presented and tested by fault injection.
Feedback Control Systems have a peculiar behavior that allows them to compensate for disturbances in the controlled application. This paper investigates whether this resilience also extends to disturbances originating from faults in the controller itself. The question of what kind of failure model is more effective in this type of system is addressed, with three different models being studied: arbitrary failure, fail silent, and fail-bounded.
The study is conducted essentially by experimental fault-injection in the controller of one of the best known and most demanding of the benchmarks used in the control systems area: an inverted pendulum. The considered failure models are compared according to criteria based on the quality of the control action. Other insights gained from the experiments made are described, for instance on how to significantly increase dependability at a very low-cost in feedback controllers, and on the need for a different kind of real-time scheduling algorithms.
Java is a very promissing language for use in the simulation of physical models due to its object-oriented nature, portability, robustness, and support for multithreading. This paper presents JWarp, a Java library for discrete-event parallel simulations. It is based on an optimistic model for synchronization of the simulation entities: the Time Warp mechanism. We introduce the main features of the library and discuss some of the implementation details.

Feedback control algorithms are inherently designed to compensate for external disturbances that the controlled system may suffer. This resilience is also extensible to late or wrong control actions produced by a failed controller computer, providing a degree of fault tolerance without the use of any particular mechanism. However, some controller failures, due to their duration or value, may indeed collapse the system, and thus other recovery measures must be taken.
This paper proposes the inclusion of an Oracle that calculates, in a timely manner, the controlled system behavior under a failed controller, and triggers recovery when the control algorithm is predictably no more able to compensate for a particular controller failure. The systems so built follow the Fail-Bounded model. The main contribution of this paper is to show how this model can be implemented in a practical way for the very important class of applications based on feedback control, thus turning that model into a technique that can be used effectively to build production systems.
The method was validated experimentally through fault injection on the controller computer of an inverted pendulum, one of the most time-demanding control system benchmarks.
A common approach in embedded systems to achieve fault-tolerance is to reboot the computer whenever some non-permanent error is detected. All the system code and data are recreated from scratch, and a previously established checkpoint, hopefully not corrupted, is used to restart the application data. The confidence is thus restored on the activity of the computer.
The idea explored in this paper is that of unconditionally resetting the computer in each control frame (the classic read sensors -> calculate control action -> update actuators cycle). A stable-storage based in RAM is used to preserve the system's state between consecutive cleanups and a standard watchdog timer guarantees that a reset is forced whenever an error crashes the system.
We have evaluated this approach by using fault-injection in the controller of a standard temperature control system. The experimental observations show that the Reset-Driven Fault Tolerance is a very simple yet effective technique to improve reliability at an extremely low cost since it is a conceptually simple, software only solution with the advantage of being application independent.


Reuse has the potential to substantially decrease the skyrocketing costs of space missions. The European Space Agency sponsored a study on the product assurance aspects of reuse of previously developed software on space projects, called PA-PDS. Several recommendations emerged from this study, along with change proposals to the main standards of software engineering and software product assurance followed by the European space industry. This paper describes those recommendations, the scope of reuse in the existing standards, and provides a justification for the proposed changes to them. A working group has been formed to develop a standard specifically addressing product assurance aspects of reuse.
WMPI II is the only commercial implementation of MPI 2.0 that runs on both Windows and Linux clusters. It evolved from the first ever Windows version of MPI, then a port of MPICH, but is now fully built from its own code base. It supports both 32 and 64 bit versions and mixed clusters of Windows and Linux nodes. This paper describes the main design decisions and the multithreaded, non-polling architecture of WMPI II. Experimental results show that, although WMPI II has figures comparable to MPICH and LAM for latency and bandwidth, most application benchmarks perform significantly better when running
on top of WMPI II.

In this paper, we will address the list of problems that have to be solved in mobile agent systems and we will present a set of fault-tolerance techniques that can increase the robustness of agent-based applications without introducing a high performance overhead. The framework includes a set of schemes for failure detection, checkpointing and restart, software rejuvenation, a resource-aware atomic migration protocol, a reconfigurable itinerary, a protocol that avoids agents to get caught in node failures and a simple scheme to deal with network partitions. At the end, we will present some performance results that show the effectiveness of these fault-tolerance techniques.
In this paper, we present briefly the implementation of a Java interface for WMPI, a
Windows-based implementation of MPI. Then, we describe a system that is oriented for
Web-based computing and we present a solution to integrate WMPI with this tool by
making use of a Java bridge component and the Java bindings for WMPI. This solution
allows the execution of meta-applications over a mixed configuration of platforms,
execution models and programming languages. The resulting system provides a way to
solve the problem of heterogeneity and to unleash the potential of diverse computational
resources and programming tools.
This paper describes a first approach to implement MPI-2's Extended
Collective Operations. We aimed to ascertain the feasibility and effectiveness of
such a project based on existing algorithms. The focus is on the
intercommunicator operations, as these represent the main extension of the
MPI-2 standard on the MPI-1 collective operations. We expose the algorithms,
key features and some performance results. The implementation was done on
top of WMPI and honors the MPICH layering, therefore the algorithms can be
easily incorporated into other MPICH based MPI implementations
Coordinated checkpointing represents a very effective solution to assure the continuity of distributed and parallel applications in the occurrence of failures. In previous studies it has been proved that this approach achieved better results than independent checkpointing and message logging. However, we need to know more about the real overhead of coordinated checkpointing and get sustained insights about the best way to implement this technique of fault-tolerance. This paper presents an experimental evaluation of coordinated checkpointing in a parallel machine. It describes some optimization techniques and presents some performance results.




Checkpointing and rollback recovery is a very effective technique to tolerate transient faults and preventive shutdowns. In the past, most of the checkpointing schemes published in the literature were supposed to be transparent to the application programmer and implemented at the operating-system level. In the recent years, there has been some work on higher-level forms of checkpointing. In this second approach, the user is responsible for the checkpoint placement and is required to specify the checkpoint contents.
In this paper, we compare the two approaches: systemlevel and user-defined checkpointing. We discuss the pros and cons of both approaches and we present an experimental study that was conducted on a commercial parallel machine.

MPI and PVM have been widely accepted for parallel programming and several implementations have been reported for clusters of UNIX workstations, supercomputers and parallel machines. In the recent past there has been an increasing use of MPI and PVM in clusters of personal computers running the Windows operating system. Traditionally, parallel programs have been developed in Fortran and C. However, the extraordinary success of the Java language has raised an impressive hype among the community of software programmers, in such a way that it seems quite promising that parallel programmers could also use Java in their applications. This can be assured if the communication libraries (PVM and MPI) could also provide a Java interface, thereby complementing the existing C and Fortran programming interfaces.
In this paper, we describe the implementation of Java interface for WMPI, a Windows-based implementations of MPI that have been developed and supported by our group. We show some details about the implementation, a MPI programming example using Java and we present some experimental results that compare the performance of Java WMPI, Java WPVM and the C programs counterparts.

Web-based parallel computing is a promising idea provided there are a considerable number of Internet users that would volunteer their CPU cycles for a global computation. The execution time of Web-based parallel applications is thus unpredictable and quite dependent of the good willing of other users. We have developed a project, called JET, and we found this is an important problem that needs to be solved. Some features should then be provided by the system to seize and maintain the enthusiasm of users that contribute the CPU idle cycles of their machines. A possible solution is to provide a Web-page with statistics about the
contribution time and computational power offered by each individual user or institution.
In this paper, we present the design of JET-STATS, a server of statistics that was developed in our project. The server was implemented inside a HTTP daemon and several optimization techniques have been used for the sake of efficiency. It provides the statistical information in a Web page that provides a flexible and fancy interface. This server was implemented in a modular way and it can be easily adapted to any other system that uses the idea of Web-based parallel computing.












This paper presents a novel algorithm for checkpointing and rollback recovery in distributed systems. Processes belonging to the same program must take periodically a nonblocking
coordinated global checkpoint, but only a minimum overhead is imposed during normal computation. Messages can be delivered out-of-order, and the processes are not required to be deterministic. The non-blocking structure is an important characteristic to avoid laying an heavy burden on the application programs.
Our proposal also includes the damage assessment phase, unlike previous schemes that either assume that an error is detected immediately after it occurs (fail-stop) or simply ignore
the damage caused by imperfect detection mechanisms. We present a possible way to evaluate the error detection latency, which enables us to assess the damage made, and avoid the propagation of errors.




Proceedings of the Second Latin-American Symposium, LADC 2005
In this paper, we present a checkpoint sharing methodology to improve turnaround time of applications run over desktop grid environments. In fact, volatility of desktop grid nodes reduces the efficiency of such environments when a fast turnaround time is sought, since a task might get stalled if its assigned machine remains unavailable for a somewhat long period of time (long at the scale of the computation). The rationale behind our approach is to permit checkpoint reuse, so that when a computation is forced to move from one node to another, it can be restarted from an intermediary point provided by the last saved checkpoint.
We study the effects of sharing checkpoints in application turnaround time simulating three scheduling algorithms based on First Come First Served: FCFS, FCFS AT and FCFS TR. The targeted environment consists of institutional desktop grids. 

Our results show that sharing checkpoints is particularly effective in volatile environments, yielding performance improvement up to three times relatively to schemes based on private checkpoints. Furthermore, for non volatile environments, a simple timeout strategy produces good results.

















This article describes how to use Function-Based Indexes to avoid costly joins. Data warehouses are particularly suited to the usage of this technique, since joins tend to be heavy operations and dimensions are scarcely updated. This technique is only applicable in Oracle8i.
This article describes the new subqueries that are allowed in Oracle8i. These new subqueries can be used in places that haven't been allowed before, such as in a SELECT clause, in an expression, or as a function/procedure parameter.
Sometimes conditions might produce UNKNOWN instead of an expected Boolean value of TRUE or FALSE. If you have several Boolean values ANDed, ORed, and NOTed together, and if one value is UNKNOWN, it might not be obvious what the final combined value is. This article presents an example of an UNKNOWN value that propagates its insidious effects, producing a surprising result set.
Much too often you have to click, browse, and scroll to discover a particular error message in the full set of Oracle documentation. With Babylon and an Oracle Error Messages glossary, you just click once.
As of Oracle 9i, there is a new SQL statement: MERGE. It allows to, with only one instruction, either update or insert rows in a table. This is especially useful in data warehouses where we update dimensions (special reference tables) periodically.


A great deal of work on adaptive query pro- 
cessing has been done over the last few years: 
Adaptive query processing has been used to de- 
tect and correct optimizer errors due to incor- 
rect statistics or simplified cost metrics; it has 
been applied to long-running continuous queries 
over data streams whose characteristics vary over 
time; and routing-based adaptive query process- 
ing does away with the optimizer altogether. De- 
spite this large body of interrelated work, no uni- 
fying comparison of adaptive query processing 
techniques or systems has been attempted; we 
tackle this problem. We identify three families 
of systems (plan-based, CQ-based, and routing- 
based), and compare them in detail with respect 
to the most important aspects of adaptive query 
processing: plan quality, statistics monitoring and 
re-optimization, plan migration, and scalability. 
We also suggest two new approaches to adap- 
tive query processing that address some of the 
shortcomings revealed by our in-depth analysis: 
(1) Proactive re-optimization, where the optimizer 
chooses query plans with the expectation of re- 
optimization; and (2) Plan logging, where op- 
timizer decisions under different conditions are 
logged over time, enabling plan reuse as well as 
analysis of relevant statistics and benefits of adap- 
tivity.

Traditional query optimizers rely on the accuracy of estimated
statistics to choose good execution plans. This design often leads to
suboptimal plan choices for complex queries, since errors in
estimates for intermediate subexpressions grow exponentially in the
presence of skewed and correlated data distributions. Reoptimization
is a promising technique to cope with such mistakes.
Current re-optimizers first use a traditional optimizer to pick a
plan, and then react to estimation errors and resulting
suboptimalities detected in the plan during execution. The
effectiveness of this approach is limited because traditional
optimizers choose plans unaware of issues affecting reoptimization.
We address this problem using proactive reoptimization,
a new approach that incorporates three techniques:
i) the uncertainty in estimates of statistics is computed in the form of
bounding boxes around these estimates, ii) these bounding boxes are
used to pick plans that are robust to deviations of actual values from
their estimates, and iii) accurate measurements of statistics are
collected quickly and efficiently during query execution. We present
an extensive evaluation of these techniques using a prototype
proactive re-optimizer named Rio. In our experiments Rio
outperforms current re-optimizers by up to a factor of three.
Commercial applications usually rely on pre-compiled parameterized procedures to interact with a database. Unfortunately, executing a procedure with a set of parameters different from those used at compilation time may be arbitrarily sub-optimal. Parametric query optimization (PQO) attempts to solve this problem by exhaustively determining the optimal plans at each point of the parameter space at compile time. However, PQO is likely not cost-effective if the query is executed infrequently or if it is executed with values only within a subset of the parameter space. In this paper we propose instead to progressively explore the parameter space and build a parametric plan during several executions of the same query. We introduce algorithms that, as parametric plans are populated, are able to frequently bypass the optimizer but still execute optimal or near-optimal plans.
As the number of satellites in operation increases, space agencies are faced with larger 
and  more  complicated  computer  infrastructures  to  support  the  operations.  This  paper 
reviews  a  number  of  solutions  currently  applied  by  many  enterprises  to  similar  problems and shows how they can be tailored to the space operations domain.
Several new Complex Event Processing (CEP) engines have been recently released, many of which are intended to be used in performance sensitive scenarios - like fraud detection, traffic control, or health care systems. However, there is no standard means to assess the performance of a CEP engine. This omission is all the more relevant as there are currently many competing products, languages, architectures, data models, and data processing CEP techniques. A performance evaluation framework can help identify good design decisions and assist in improving engines. Here we demonstrate our work in progress: FINCoS, a framework that can be used to benchmark CEP systems. The proposed framework has five relevant characteristics: 
i.	Flexible (e.g., it allows changing the workload on the fly to measure reactions to peak loads);
ii.	Independent of particular workloads;
iii.	Neutral (not bound to any specific CEP product); 
iv.	Correctness check (validators can be plugged into the framework on demand to verify results);
v.	Scalable (many of its components, like event generators, can be centrally orchestrated and run in parallel).

Note that the framework does not include a benchmark specification. In fact, it was designed such that diverse datasets and query scenarios can be easily attached and tested on several CEP engines.

As such, this framework has three key benefits: first, it can be used by the CEP community to more quickly devise and experiment new benchmarks for event processing systems. Second, CEP vendors can employ the framework in conjunction with their own test datasets to benchmark their systems internally. Finally, customers can use it with their real data and select the CEP engine that best fits their needs.
The Internet contains thousands of Frequently Updated, Time-stamped, Structured (FUTS) data sources. This type of information represents a different class of information that is not properly handled by existing data management systems such as databases, data warehouses, search engines, pub-sub, event processing or information retrieval systems. In this position paper, we describe 9ticks, a system we are designing to collect, parse, store, query and disseminate FUTS information. 9ticks is helping us understand that all those steps raise new challenges but also bring new opportunities. In this paper, we summarize the challenges identified and present our vision of an end-to-end FUTS management system.
Event processing engines are used in diverse mission-critical scena-rios such as fraud detection, traffic monitoring, or intensive care units. Howev-er, these scenarios have very different operational requirements in terms of, e.g., types of events, queries/patterns complexity, throughput, latency and num-ber of sources and sinks. What are the performance bottlenecks? Will perfor-mance degrade gracefully with increasing loads? In this paper we make a first attempt to answer these questions by running several micro-benchmarks on three different engines, while we vary query parameters like window size, win-dow expiration type, predicate selectivity, and data values. We also perform some experiments to assess engines scalability with respect to number of que-ries and propose ways for evaluating their ability in adapting to changes in load conditions. Lastly, we show that similar queries have widely different perfor-mances on the same or different engines and that no engine dominates the other two in all scenarios.

Collaborative degree programs in software engineering are becoming more common as universities try to expand their offering globally and leverage their knowledge and expertise. Faculty training program intended to help academics learn how to teach courses from collaborating institutions is a complicated undertaking considering the need to pass along course material, the 'spirit' of how the courses are taught and the quality standards to which they must adhere. Carnegie Mellon University developed a training process for teaching faculty members in its joint software engineering programs in India, Korea and Portugal. The process, its implementation and the feedback of using it with our overseas partners will be explored and described in detail.



MapReduce has become a widely used tool for computing complex tasks that process massive amounts of data in large clusters. These tasks commonly execute on a fraction of the nodes of a larger cluster, and it is left to users to make best guesses on the number of nodes needed for a task to complete within acceptable time. Nonetheless, the time a task will take to complete is often unknown beforehand. Previous research addressed this problem by establishing time constraints for query execution and, when needed, reduce the accuracy of queries using result approximation and/or sampling. However in many situations reduced accuracy is not tolerable. In this paper we present FloodDQ, a MapReduce system that implements deadline queries?queries that must finish before a deadline, never discarding data or reducing accuracy. FloodDQ produces timely, accurate results by adaptively increasing or decreasing computing power, at runtime, towards completing execution within the specified deadline. In FloodDQ, users only specify a deadline and the input data. The system monitors the progress of the task and extrapolates whether it will complete on time. If the task is deemed to complete after the specified time, the system requests more nodes from an Infrastructure-as-a-Service provider, and adds them to the computation. On the other hand, if the task is deemed to complete before the specified time the system quiesces and releases surplus nodes, cutting costs to a minimum. This paper describes FloodDQ?s architecture for supporting deadline queries and presents experimental results where the system always meets the deadline in spite of changes to the number of nodes, size of data or existence of perturbations.
MapReduce has become a widely used tool for computing complex tasks that process massive amounts of data in large clusters. Support for MapReduce tasks in cloud environments has been provided but it is left to users to make best guesses on the number of nodes needed for a task to complete within acceptable time. Moreover, the time a task will take to complete is often unknown beforehand. Previous research addressed this problem by establishing time constraints for query execution and, when needed, reduce the accuracy of queries using result approximation and/or sampling. However in many situations reduced accuracy is not tolerable. In this paper we present FloodDQ, a MapReduce system that implements deadline queries?queries that must finish before a deadline, never discarding data or reducing accuracy. FloodDQ produces timely, accurate results by adaptively increasing or decreasing computing power, at runtime, towards completing execution within the specified deadline. In FloodDQ, users only specify a deadline and the input data. The system monitors the progress of the task and extrapolates whether it will complete on time. If the task is deemed to complete after the specified time, the system requests more nodes from an IaaS Cloud provider, and adds them to the computation. On the other hand, if the task is deemed to complete before the specified time the system quiesces and releases surplus nodes, cutting costs to a minimum. This paper describes FloodDQ?s architecture for supporting deadline queries and presents experimental results where the system always meets the deadline in spite of changes to the number of nodes, size of data or existence of perturbations.
Distributed data stream processing (DSP) is used to analyze information and raise alarms in business-critical scenarios such as financial fraud-detection, clickstream processing, network security, traffic control, or real-time KPI computations. Processing this information efficiently is very challenging because the nature of continuous streaming sources is varying in nature: often the amount of data and processing changes with time of day and day of week and frequently has unexpected spikes. Thus, the result is that most DSP computations are either over-provisioned, introducing increased cost and wasted energy, or are under- provisioned and, either incur in performance degradation or denial-of-service, or have to resort to load shedding.
We demonstrate Flood, a scalable, elastic DSP engine that solves these problems. By using a scalable computing model, MapReduce, and adequately monitoring running computations our system is able to decide, in runtime, if there is a lack or a waste of resources. Flood then acts autonomically by requesting or releasing computing nodes, immediately expanding or contracting the computation, making sure that latency and throughput requirements are guaranteed. This leads to augmented efficiency and lowered costs, all while insuring quality of service.
The last decade has witnessed the emergence of business critical applications processing streaming data for domains as diverse as credit card fraud detection, real-time recommendation systems, call-center monitoring, ad selection, network monitoring, and more. Most of those applications need to compute hundreds or thousands of metrics continuously while coping with very high event input rates. As a consequence, large amounts of state (i.e., moving windows) need to be maintained, very often exceeding the available memory resources. Nonetheless, current event processing platforms have little or no memory management capabilities, hanging or simply crashing when memory is exhausted. In this paper we report our experience in using secondary storage for solving the performance problems of memory-constrained event processing applications. For that, we propose SlideM, a novel buffer management algorithm that exploits the access pattern of sliding windows in order to efficiently handle memory shortages. The proposed algorithm was implemented in a real stream processing engine and validated through an extensive experimental performance evaluation. Results corroborate the efficacy of the approach: the system was able to sustain very high input rates (up to 300,000 events per second) for very large windows (about 30GB) while consuming small amounts of main memory (few kilobytes).
There has been an increasing interest both in academia and industry for systematic methods for evaluating the performance and scalability of event processing systems. A number of performance results have been disclosed over the last years, but there is still a lack of standardized benchmarks that allow an objective comparison of the different systems. In this paper, we present our work in progress: the BiCEP benchmark suite, a set of workloads, datasets and tools for evaluating different performance aspects of event processing platforms. In particular, we introduce Pairs, the first of the BiCEP benchmarks, aimed at assessing the ability of CEP engines in processing progressively larger volumes of events and simultaneous queries while providing quick answers.
FINCoS is a set of benchmarking tools for load generation and performance measuring of event processing systems. It leverages the development of novel benchmarks by allowing researchers to create synthetic workloads, and enables users of the technology to evaluate candidate solutions using their own real datasets. An extensible set of adapters allows the framework to communicate with different CEP engines, and its architecture permits to distribute load generation across multiple nodes. In this paper we briefly review FINCoS, introducing its main characteristics and features, and discussing how it measures the performance of event processing platforms.


A data warehouse (DW) provides information for analytical processing, decision making and data mining tools. This information is updated periodically from transactional systems. Traditional DW systems have static structures of their data schemas and relationships, and therefore are not prepared to support the dynamics of real-time live data processing. As the concept of real time enterprise evolves, the synchronism between transactional data and DW, statically implemented, has been reviewed. For these purposes, Real-Time Data Warehouses (RTDW) seem to be very promising. This paper presents methodological indications for implementing RTDW, in which transactional data sources are available through common standard database access, allowing to minimize the time needed to accomplish extraction, transformation and loading (ETL) processes of that data, as well as its loading into the DW. The main method presented consists on using structural replicas of all fact tables without primary keys or index files, adapting those replica’s data structures for housing, in real-time, all insert, edit and delete operations (data transactions) that occur within operational systems databases. This is accomplished using only append record operations towards those fact table replicas, allowing to minimize processing time, record locking and concurrency data access problems, both in transactional systems and the DW. Concurrently, this allows maintaining DW availability and keeping OLAP tools functioning properly, providing the most recent business data.



Diseases like avian influenza, severe acute respiratory syndrome and Creutzfeldt-Jacob syndrome represent a new era of biological threats. Nowadays, biological hazards are breeding, mutating and evolving at tremendous speed. Furthermore, they may spread out at the same speed as which we travel. This reveals an urgent need for an agent capable of dealing with such threats. This paper presents the architecture for an effective information system infrastructure enabling near real-time detection of disease outbreaks, fulfilling that need, using knowledge extraction algorithms to explore a symptoms/diseases data warehouse. To collect such data, we take advantage of the Internet and common personal communication devices. We also present a case-simulation proving the system can detect an outbreak within hours or even minutes of its physical occurrence, alerting health decision makers and providing quick interaction and feedback between them and medical staff. The architecture is also scalable and functional independently from its geographical dimension.
Diseases like avian influenza, severe acute respiratory syndrome and Creutzfeldt-Jacob syndrome represent a new era of biological threats. Nowadays, biological hazards are breeding, mutating and evolving at tremendous speed. Furthermore, they may spread out at the same speed as which we travel. This reveals an urgent need for an agent capable of dealing with such threats. This paper presents the architecture for an effective information system infrastructure enabling near real-time detection of disease outbreaks, fulfilling that need, using knowledge extraction algorithms to explore a symptoms/diseases data warehouse. To collect such data, we take advantage of the Internet and common personal communication devices. We also present a case-simulation proving the system can detect an outbreak within hours or even minutes of its physical occurrence, alerting health decision makers and providing quick interaction and feedback between them and medical staff. The architecture is also scalable and functional independently from its geographical dimension.
A data warehouse provides information for analytical processing, decision making and data mining tools. As the concept of real-time enterprise evolves, the synchronism between transactional data and data warehouses, statically implemented, has been reviewed. Traditional data warehouse systems have static structures of their schemas and relationships between data, and therefore are not able to support any dynamics in their structure and content. Their data is only periodically updated because they are not prepared for continuous data integration. For these purposes, real-time data warehouses seem to be very promising. In this paper we present a methodology on how to adapt data warehouse schemas and user-end OLAP (On-Line Analytical Processing) queries for efficiently supporting real-time data integration. To accomplish this, we use techniques such as table structure replication and query predicate restrictions for selecting data, managing to enable continuous data integration in the data warehouse with minimum impact in query execution time. We demonstrate the functionality of the method by analyzing its impact in query performance using benchmark TPC-H executing query workloads while simultaneously performing continuous data integration at various insertion time rates.
Optimizing the performance of OLAP queries in relational data warehouses (DW) has always been a major research issue. There are various techniques that can be used to achieve its goals, such as data partitioning, indexing, data aggregation, data sampling, redefinition of database (DB) schemas, among others. In this paper we present a simple and easy to implement method which links partitioning and indexing based on the features present in predefined major decision making queries to efficiently optimize a data warehouse’s performance. The evaluation of this method is also presented using the TPC-H benchmark, comparing it with standard partitioning and indexing techniques, demonstrating its efficiency with single and multiple simultaneous user scenarios.

A data warehouse provides information for analytical processing, decision making and data mining tools. As the concept of real-time enterprise evolves, the synchronism between transactional data and data warehouses, statically implemented, has been redefined. Traditional data warehouse systems have static structures of their schemas and relationships between data, and therefore are not able to support any dynamics in their structure and content. Their data is only periodically updated because they are not prepared for continuous data integration. For real-time enterprises with needs in decision support purposes, real-time data warehouses seem to be very promising. In this paper we present a methodology on how to adapt data warehouse schemas and user-end OLAP queries for efficiently supporting real-time data integration. To accomplish this, we use techniques such as table structure replication and query predicate restrictions for selecting data, to enable continuously loading data in the data warehouse with minimum impact in query execution time. We demonstrate the efficiency of the method by analyzing its impact in query performance using benchmark TPC-H executing query workloads while simultaneously performing continuous data integration at various insertion time rates.



The sudden fall of blood pressure (hypotension) is a common complication in medical care. In critical care patients, hypotension (HT) may cause serious heart, endocrine or neurological disorders, inducing severe or even lethal events. Moreover, recent studies report an increase of mortality in HT prone hemodialysis patients in need of critical care. If HT could be predicted in advance, medical staff could take action to minimize its effects, or even avoid its occurrence. Typically, most medical systems have focused on monitoring and detecting current patient status, rather than determining biosignal trends or predicting a patient’s future status. Therefore, predicting HT episodes in advance remains a challenge. Furthermore, since critical care actions such as hemodialysis are oftenly inconvenient and uncomfortable procedures, HT prediction or detection methods should be non-invasive, whenever possible. In this paper, we present a solution for continuous monitorization and prediction of HT episodes, using heart rate (HR) and mean blood pressure (BP) non-invasive measured biosignals. We propose an architecture for a HT Predictor (HTP) Tool, presenting a set of tools and a real-time database capable of continuously storing and real-time monitoring all patient’s historical HR and BP biosignal data, and efficiently alerting both probable and detected occurrences of HT episodes for each patient for the following 60 minutes. Additionally, the system promotes medical staff mobility, by taking advantage of using mobile personal devices such as mobile phones and PDA’s, optimizing human resources. Finally, an experimental evaluation on real-life data from the well known Physionet database shows the efficiency of the tool, outperforming the winning proposal of the Physionet 2009 Challenge.
Performance optimization of decision support queries has always been a major issue in data warehousing. A large amount of wide-ranging techniques have been used in research to overcome this problem. Bit-based techniques such as bitmap indexes and bitmap join indexes have been used and are generally accepted as standard common practice for optimizing data warehouses. These techniques are very promising due to their relatively low overhead and fast bitwise operations. In this paper, we propose a new technique which performs optimized row selection for decision support queries, introducing a bit-based attribute into the fact table. This attribute’s value for each row is set according to its relevance for processing each decision support query by using bitwise operations. Simply inserting a new column in the fact table’s structure and using bitwise operations for performing row selection makes it a simple and practical technique, which is easy to implement in any Database Management System. The experimental results, using benchmark TPC-H, demonstrates that it is an efficient optimization method which significantly improves query performance.
On-line analytical processing against data warehouse databases is a common form of getting decision making information for almost every business field. Decision support information oftenly concerns periodic values based on regular attributes, such as sales amounts, percentages, most transactioned items, etc. This means that many similar OLAP instructions are periodically repeated, and simultaneously, between the several decision makers. Our Query Cache Tool takes advantage of previously executed queries, storing their results and the current state of the data which was accessed. Future queries only need to execute against the new data, inserted since the queries were last executed, and join these results with the previous ones. This makes query execution much faster, because we only need to process the most recent data. Our tool also minimizes the execution time and resource consumption for similar queries simultaneously executed by different users, putting the most recent ones on hold until the first finish and returns the results for all of them. The stored query results are held until they are considered outdated, then automatically erased. We present an experimental evaluation of our tool using a data warehouse based on a real-world business dataset and use a set of typical decision support queries to discuss the results, showing a very high gain in query execution time
The purpose of a data warehouse is to aid decision making. As the real-time enterprise evolves, synchronism between transactional data and data warehouses is redefined. To cope with real-time requirements, the data warehouses must be able to enable continuous data integration, in order to deal with the most recent business data. Traditional data warehouses are unable to support any dynamics in structure and content while they are available for OLAP. Their data is periodically updated because they are unprepared for continuous data integration. For real-time enterprises with needs in decision support while the transactions are occurring, (near) real-time data warehousing seem very promising. In this paper we present a survey on testing today’s most used loading techniques and analyze which are the best data loading methods, presenting a methodology for efficiently supporting continuous data integration for data warehouses. To accomplish this, we use techniques such as table structure replication with minimum content and query predicate restrictions for selecting data, to enable loading data in the data warehouse continuously, with minimum impact in query execution time. We demonstrate the efficiency of the method using benchmark TPC-H and executing query workloads while simultaneously performing continuous data integration.
The sudden fall of blood pressure (hypotension - HT) is a common complication in medical care. In critical care patients, HT may cause serious neurological, heart, or endocrine disorders, inducing severe or even lethal events. Recent studies report an increase of mortality in HT prone hemodialysis patients in need of critical care. Predicting HT episodes in advance is crucial to enable medical staff to minimize its effects or even avoid its occurrence. Most medical systems have focused on monitoring and detecting current patient status, rather than determining biosignal trends or predicting the patient’s future status. Therefore, predicting HT episodes in advance remains a challenge. In this paper, we present a solution for continuous monitoring and efficient prediction of HT episodes. We propose an architecture for a HT Predictor (HTP) Tool, capable of continuously storing and real-time monitoring all patient’s heart rate and blood pressure biosignal data, alerting probable occurrences of each patient’s HT episodes for the following 60 minutes, based on non-invasive hemodynamic variables. Our system also promotes medical staff mobility, taking advantage of using mobile personal devices such as cell phones and PDA’s. An experimental evaluation on real-life data from the well-known Physionet database shows the tool’s efficiency, outperforming the winning proposal of the Physionet 2009 Challenge.
During the last few years, On-Line Analytical Processing (OLAP) has emerged as a valuable tool for the analysis,
navigation and reporting of hierarchically organized data from data warehouses. Still, it remains a challenging task to
implement and deploy OLAP tools over large databases, since no standardized architecture exists, which describes the
common components and functionality of OLAP tools. Managers all over the world face the challenge of how to analyze
their most valuable information. The fundamental nature of the right information analysis is on choosing a tool with the
features which best aids a management decision. This paper presents a prototype containing a group of features that are
recommended to be implemented in on-line analytical processing (OLAP) tools, for analyzing stored data with quality.
The prototype was evaluated on a data warehouse implemented in a relational repository using the star schema but this
prototype can be implemented on any database system.



Bipin C. Desai, Jaroslav Pokorný, Jorge Bernardino (Eds.): 16th International Database Engineering & Applications Symposium, IDEAS '12, Prague, Czech Republic, August 8-10, 2012. ACM 2012, ISBN 978-1-4503-1234-9
Decision-making is a crucial, yet challenging task in enterprise management. In many organizations, decisions are still made based on experience and intuition rather than on facts and rigorous approaches. This is often due to lack of data, unknown relationships between data and goals, conflicting goals and poorly understood risks. The success of organizations depends on fast and well-founded decisions taken by relevant people in their specific area of responsibility. Business Intelligence (BI) is a collection of decision support technologies
for enterprises aimed at enabling knowledge workers such as executives, managers, and analysts to make better and faster decisions. In this paper, the authors review the concept of BI as an open innovation strategy and address the importance of BI in revolutionizing knowledge towards economics and business sustainability.
The main objective is to discuss why the concept of BI has become increasingly important and presents some of the top key applications and technologies to implement open BI in organizations, which would like to enter into the new market and operate on a global scale.
During the last decades Genetic Algorithms (GAs) have proved to be a powerful technique for solving difficult problems. Consequently, GA courses are becoming increasingly common in universities. The laboratorial classes of such courses are crucial for students to consolidate and apply the concepts learned in theoretical classes. However, it is required a lot of programming effort and sometimes students tend to have difficulties on this part, either because the number of different GA variants they have to implement or even because the lack of programming skills. To overcome this problem we present a new educational tool for GAs called GraphEA. This tool aims to help students to learn GAs without the need of programming effort, offering novel features like the 3D visualization of the chromosome formation process and the online modification of problem data. In this paper we demonstrate three well-known optimization problems implemented on the tool, namely the Knapsack Problem, the Traveling Salesman Problem, and the Function Optimization Problem.





Testing the graphical user interface (GUI) of a software product is important to ensure the quality of the system and therefore to improve the user satisfaction of using the software. Using tools to support the testing process solves the problems of the manual testing which is tedious and time consuming. Capture and replay tools are commonly used in GUI testing. In this paper we compare five open source capture and replay tools, namely Abbot, Jacareto, JFCUnit, Marathon and Pounder, in terms of ease of use and capture and replay capabilities. In order to compare the tools, we defined comparison characteristics and after evaluating each tool, we selected the one that showed the best results in almost all criteria. The results of our study may serve as guidance for any novice tester or company that pretends to automate the GUI testing process using capture and replay tools.
Computer data has become one of the most valuable assets that individuals, organizations and enterprises own today. The majority of people agree that losing their data (programs, data sets, documentation files, email addresses, photos, customer data, etc.) would be a disaster. The reason most individuals avoid performing data backups though, is because they feel the process is complicated, tedious and expensive. This is not always true. In fact, with the right tool, it’s very easy and affordable. In this paper we compare the performance and system resources usage of five remote incremental backup open source tools for Linux: Rsync, Rdiff-backup, Duplicity, Areca and Link-Backup. These tools are tested using three distinct remote backup operations: full backup, incremental backup and data restoration. The advantages of each tool are described and we select the most efficient backup tool for simple replication operations and the overall best backup tool.
Small and Medium-Sized Enterprises (SMEs) are socially and economically important, since they represent 98% of all enterprises, providing around 90 million jobs in the European Union, and contribute to entrepreneurship and innovation. However, SMEs face particular difficulties in order to be competitive in a global world. In recent time, technology applications in different fields, especially Business Intelligence (BI) have been developed rapidly and considered to be one of the most significant uses of information technology. BI is a broad category of applications and technologies for gathering, storing, analyzing, and providing access to data to help enterprise users make better business decisions. This represents a tremendous competitive advantage that allows achieving and exploring the collective intelligence of the organization, enabling contextual, agile, and simplified information exchange and collaboration among distributed workforce and networks of partners and customers, which can be defined as Enterprise 2.0. Despite these advantages, the companies applying such systems may also encounter problems in decision-making processes because of the highly diversified interactions within the systems. Hence, the choice of a suitable BI platform for SMEs is important to take the great advantage of using information technology in all organizational fields. The authors analyze seven open source Business Intelligence tools, free of charge, given that one of the main objectives is to reduce costs and enhance Enterprise 2.0 using new open source technologies. 
Social enterprises can tackle a wide range of social and environmental issues and contribute to economic growth but they also benefit people and the planet. However, social enterprises face particular difficulties, in order to be financial sustainable and get their social mission accomplished in a global world. In recent time, technology applications in different fields, especially Business Intelligence (BI), have been developed rapidly and considered to be one of the most significant uses of information technology. BI is a broad category of applications and technologies for gathering, storing, analyzing, and providing access to data to help enterprise users make better business decisions. This represents a tremendous competitive advantage that allows achieving and exploring the collective intelligence of the organization, enabling contextual, agile, and simplified information exchange and collaboration among distributed stakeholders and networks of partners. Despite these advantages, the organizations applying such systems may also encounter problems in decision-making process because of the highly diversified interactions within the systems. Hence, the choice of a suitable BI platform for social enterprises is important to take the great advantage of using information technology in all organizational fields. This chapter aims at addressing the problems existed in the social e-enterprises, describing and evaluating the major open source BI tools that can have strong impact on social enterprise change and development.
Business intelligence platforms allows managers and business analysts monitor data and statistics, make decisions, analyze market trends, study the insertion of new products and maintain profitable and long-lasting relationships with customers in order to stay competitive in a global world. Consequently, the importance of using business intelligence tools is crucial for the success of actual organizations. Yet, many organizations are learning that this no easy task to decide which business intelligence tool to choose to expand their business. In this work we evaluate eight major business intelligence tools: four commercial (IBM Cognos, Microsoft BI, MicroStrategy and Oracle BI) and four open source solutions (JasperSoft, Pentaho, SpagoBI and Vanilla). The main objective is to present the differences in functionality between the various platforms, estimating overall usability, helping decision managers to choose the best tool to their business.

The evolution of business processes and the globalization of virtual organizations, stimulates the need to have available at any time, in any place, tools to produce information for decision support in order to increase their competitive advantage. In this context, Cloud Business Intelligence emerges as a paradigm of almost vital importance providing business analytics to organizations, reducing costs and increasing profits. In the context of virtual organizations are examined three of the most popular open source platforms for Cloud Business Intelligence market and selected the one that best suits the needs of virtual organizations. This paper evaluated SpagoBI, JaspersoftBI, and PentahoBI Cloud Business Intelligence platforms analyzing the features that best fit the needs of virtual organizations.
Globalization, increasing competition and advances in information and communication technology has forced companies to focus on managing customer relationships in order to efficiently maximize revenues. Customer relationship management (CRM) is the key competitive strategy businesses need to stay focused on the needs of the customers and to integrate a customer-facing approach throughout the organization. By using information and communication technology, businesses are trying to get closer to the customer so that they can create long-term relationships. This is crucial to maintain enterprise competitiveness especially for small and medium enterprises (SMEs). The highly competitive market forces small business to find and define new strategies, particularly technological new strategies, which enable their endurance and Open Source CRM systems represent a great opportunity for SMEs. To enhance business management we describe and compare four top open source CRM systems and study its applicability to fit SMEs requirements.
Cloud Computing concept refers to both the applications delivered as services over the Internet and the servers and system software in the datacenters that provide those services. These solutions offer pools of virtualized computing resources, paid on a pay-per-use basis, and drastically reduce the initial investment and maintenance costs. Efficient and flexible resource management is the main focus for the cloud solutions on the market as well as scalability and adaptability to new environments. Openstack exceeded the market as a scalable, performant and highly adaptive open source architecture for both public and private cloud solutions as well as leveraging from hardware resources either they be professional or entry level. This paper gives an overview of Openstack software components functionalities in order to design and implement unique cloud computing solutions to fit enterprises purposes.
Business Intelligence (BI) platforms are applications to analyze critical business data so as to gain new insights about business and markets. The new insights can be used for improving products and services, achieving better operational efficiency, improve competitiveness, and fostering customer relationships. However, we notice that BI is not yet a reality in Small and Medium Enterprises (SMEs) and it's even unknown in many of these organizations. The implementation of BI in organizations depends on their specificities. It is essential that the information available on platforms and their functionality comes clearly to organizations. Only a correct choice, in response to business needs, will allow powerful data that results in gains and business success. For this purpose, we have developed a comparative analysis of the existing capabilities in the various BI tools, which is intended to assist the selection of the BI platform best suited to each organization and/or business area. In this paper, we study and compare seven of the most used open source BI platforms: Actuate, JasperSoft, OpenI, Palo, Pentaho, SpagoBI and Vanilla.
In an increasingly competitive environment the manufacturing industry faces many challenges, like global competition and ever more demanding customers. This leads companies to be proactive in managing and to take advantage of corporate data if they want to keep up or stay ahead of the competition. That's where Business Intelligence can improve company's competitive edge. In this paper we show the suitability of an open source business intelligence tool to manufacturing industry.
The decision on the pair degree/institution, upon application to higher education will be the most important in the life of a young person. Despite recent developments, applicants to higher education still don't know many of the available indicators that could influence their decision. In this paper we propose a portal with a simple design which presents all the available and relevant indicators. It offers also necessary resources and options thus is the candidate and not the state or any other entity, to prioritize their priorities. The portal will allow each candidate to query data, by degree and institution, and make individual simulations, giving weights to more relevant criteria, in order to find a hierarchy of degrees/institutions which most closely match to his goals, thus creating a personal ranking.

Data Mining is a knowledge field that intersects domains from computer science and statistics, attempting to discover knowledge from databases in order to facilitate the decision making process. Classification is a Data Mining task that learns from a collection of cases in order to accurately predict the target class for new cases. Several machine learning techniques can be used to perform classification. Free and open source Data Mining software tools are available from the Internet that offers the capability of performing classification through different techniques. This study compares four free and open source Data Mining tools: KNIME, Orange, RapidMiner and Weka. Our objective is to reveal the most accurate tool and technique for the classification task. Analysts may use the results to rapidly achieve a good result. Our experimental results show that there is no single tool or technique that always achieves the best result but some achieve better results more often than others.
Testing the graphical user interface (GUI) of a software product is important to ensure the quality of the system and therefore to improve the user satisfaction of using the software. Using tools to support the testing process solves the problems of the manual testing which is tedious and time consuming. Capture and replay tools are commonly used in GUI testing. In this paper we compare five open source capture and replay tools, namely Abbot, Jacareto, JFCUnit, Marathon and Pounder, in terms of ease of use and capture and replay capabilities. In order to compare the tools, we defined comparison characteristics and after evaluating each tool, we selected the one that showed the best results in almost all criteria. The results of our study may serve as guidance for any novice tester or company that pretends to automate the GUI testing process using capture and replay tools.

Massively Multiplayer Online Role-Playing Game more known as MMORPG is a popular type of game played all over the globe. While being online people tend to change and so do their attitudes. But even there the concept of ethic exists and there are rules that should be followed. Even while still being a game, it is based on social activities and just like in society, code of conduct applies and misbehavior should be punished. This paper analyzes player interaction in online games and if there exist ethical behavior in MMORPGs environment.





Data is part of our everyday life and an essential object in numerous businesses and organizations. The quality of the data, i.e., the degree to which the data characteristics fulfill requirements, can have a tremendous impact on the businesses themselves, the companies, or even in human lives. In fact, research and industry reports show that huge amounts of capital are spent to improve the quality of the data being used in many systems, sometimes even only to understand the quality of the information in use. Considering the variety of dimensions, characteristics, business views, or simply the specificities of the systems being evaluated, understanding how to measure quality can be an extremely difficult task. In this paper we survey the state of the art in classification of poor data, including the definition of dimensions and specific data problems, we identify frequently used dimensions and map data quality problems to the identified dimensions. The huge variety of terms and definitions found suggests that further standardization efforts are required; also, data quality research on Big Data appears to be in its initial steps, leaving open space for further research.
Big Data has become a hot topic across several business areas requiring the storage and processing of huge volumes of data. Cloud computing leverages Big Data by providing high storage and processing capabilities and enables corporations to consume resources in a pay-as-you-go model making clouds the optimal environment for storing and processing huge quantities of data. By using virtualized resources, Cloud can scale very easily, be highly available and provide massive storage capacity and processing power. This paper surveys existing databases models to store and process Big Data within a Cloud environment. Particularly, we detail the following traditional NoSQL databases: BigTable, Cassandra, DynamoDB, HBase, Hypertable, and MongoDB. The MapReduce framework and its developments Apache Spark, HaLoop, Twister, and other alternatives such as Apache Giraph, GraphLab, Pregel and MapD - a novel platform that uses GPU processing to accelerate Big Data processing - are also analyzed. Finally, we present two case studies that demonstrate the successful use of Big Data within Cloud environments and the challenges that must be addressed in the future.

http://www.ronpub.com/publications/OJBD_2015v1i2n02_Neves.pdf

In recent time, technology applications in different fields, especially Business Intelligence (BI) have been developed rapidly and considered to be one of the most significant uses of information technology. BI is a broad category of applications and technologies for gathering, storing, analyzing, and providing access data to help enterprise users make better business decisions. Whereas in the past the Business Intelligence market was strictly dominated by closed source and commercial tools, the last few years were characterized by the birth of open source BI solutions. This represents a tremendous competitive advantage, however the choice of a suitable open source BI suite is a challenge. The present study evaluated and compared the last versions of the five main Open Source Business Intelligence suites: JasperSoft, Palo, Pentaho, SpagoBI and Vanilla using OpenBRR methodology.

The economic market constantly shifts and change, requiring increasingly complex and agile adaptations from companies, particularly in a global world where the future is becoming unpredictable. Therefore it is critical an effective accounting management tool to build a successful business, especially for SMEs Small and Medium Enterprises, where the margin for error is small. Accounting software systems are a great solution for SMEs to improve accounting management and to assist managers to overcome the complexity of the various regulatory and administrative obligations that currently attends any commercial transaction. Unfortunately, despite all the benefits, they are not yet common in SMEs, that still favor outsourcing there financial information, frequently due to the wrong idea that accounting software is an intricate tool as well too costly. In this paper, we propose to refute this preconceived idea, and present Open Source Software (OSS) as a viable alternative to proprietary software regarding accounting software systems. We describe and compare six top open source accounting software systems and study the applicability of these tools in fitting the requirements of SMEs.





The exception handling mechanism has been one of the most used reliability tools in programming languages for over four decades. Nearly all modern languages have some form of “try-catch” model for exception handling and encourage its use. Nevertheless, this model has not seen significant change, even in the face of new challenges, such as concurrent programming and the advent of reactive programming. As it stands, the current model is reactive, rather than proactive — exceptions are raised, caught, and handled. We propose an alternative exception handling model — PreX — where exceptions are no longer caught but, rather, predicted and prevented. Online Failure Prediction techniques generally work at a very high level, showing potential for pre- diction of program crashes. However, these techniques have never been at the hands of the programmers as an effective tool to improve software quality. By applying recent advances in Online Failure Prediction, PreX aims to fully prevent exceptions, bringing failure prediction techniques to a much more fine-grained level that the programmer can control. Pre- dicting exceptions enables a range of preventive measures that enhance the reliability and robustness of a system, offering new revitalization strategies to developers.
Uma das maiores dificuldades no ensino da
programação em linguagem C está relacionada com a
transmissão dos conceitos associados a ponteiros. No
universo considerado, o problema revelava-se mais
complexo do que o habitual uma vez que, dada a estrutura
curricular desta Licenciatura bietápica em Informática de
Gestão, apenas no 3.º ano os alunos tomam contacto com a
aprendizagem de uma linguagem de programação. A
escolha de um simulador que permitisse, no contexto da
cadeira de Arquitectura de Computadores, elucidar os
alunos sobre o modo de funcionamento da memória
perseguia três objectivos: 1- compreensão de conceitos
associados à memória e ciclo de instrução; 2- domínio da
programação numa linguagem muito próxima do Assembly
com um conjunto muito reduzido de instruções; 3- facilitar
a aprendizagem dos conceitos de algoritmia e de ponteiros,
permitindo um certo grau de transversalidade entre as
cadeiras de Arquitectura e de Programação.

The Boundary-Scan technology was proposed fifteen years
ago to overcome the limitations of testing digital devices due to the
increasing complexity and greater miniaturization of integrated circuits
and boards. The use of pin-level fault-injection faced similar difficulties
and became obsolete for that reason. In this paper we discuss the use
of the Boundary-Scan infrastructure for fault-injection purposes. Several
fundamental constraints of such approach are identified. Generic digital
systems and processors with Boundary-Scan based On-Chip Debugging
(OCD) are considered as target system candidates. We observe that by
combining OCD mechanisms with modified boundary-scan cells most of
the constraints reported are solved. Finally, some key properties of the
technology such as the orthogonality to the purely functional architecture
and the low abstraction level access as well as the standard interface and
description language provided make it a good candidate to provide a
standardized flexible fault injection framework.
LMC Paradigm is not a recent approach to teach Computer Architecture: it has been presented, tested and used since 1965, first by its mentors, Madnick and Donovan, and their MIT students, and since then in many other universities around the world. The main purpose of LMC Paradigm is to explain, using a very simple Model, the main components of a real computer system, and to learn how to program using a simple decimal-encoded instruction set. Using new LMC Simulators (based on LMC Paradigm) developed since then, students can nowadays take advantages from simulation processes (e.g., to simulate a program step-by-step execution). We evaluated six different LMC simulators, picked the 'best practicesâ? associated to each one and developed a new simulator, especially focused on Management and Informatics undergraduate students requirements. This new simulator - edu.LMC - has been tested in a Computer Architecture course.
Little Man Computer (LMC) Paradigm, a simple representation of a real computer system with pedagogical purposes, was firstly presented in 1965. It follows von Neumann's Architectural Model and uses a simplified instruction set. Since then, many simulators have been developed based on this Paradigm. Mainly, those simulators have been designed to be used by undergraduate students in Computer Architecture courses. We developed a new LMC simulator ' edu.LMC 'created especially for students with very basic skills on Computer Architecture, mostly not majors on Computer Science or Computer Engineering. This is a special purpose simulator with a clear pedagogical focus, tested and used during Computer Architecture classes for a Management and Informatics course. edu.LMC includes many features that are difficult to find together in other LMC simulators. Some existing simulators are more complete, but also too complex to be used in this learning context. In this paper, we present edu.LMC\'s main characteristics, some utilization examples and preliminary evaluation results.
Evolutionary Testing is an emerging methodology for automatically producing high quality test data. The focus of our on-going work is precisely on generating test data for the structural unit-testing of object-oriented Java programs. The primary objective is that of efficiently guiding the search process towards the definition of a test set that achieves full structural coverage of the test object.

However, the state problem of object-oriented programs requires specifying carefully fine-tuned methodologies that promote the traversal of problematic structures and difficult control-flow paths - which often involves the generation of complex and intricate test cases, that define elaborate state scenarios.

This paper proposes a methodology for evaluating the quality of both feasible and unfeasible test cases - i.e., those that are effectively completed and terminate with a call to the method under test, and those that abort prematurely because a runtime exception is thrown during test case execution. With our approach, unfeasible test cases are considered at certain stages of the evolutionary search, promoting diversity and enhancing the possibility of achieving full coverage.
The focus of this paper is on presenting a tool for generating test data by employing evolutionary search techniques, with basis on the information provided by the structural analysis and interpretation of the Java bytecode of third-party Java components, and on the dynamic execution of the instrumented test object. The main objective of this approach is that of evolving a set of test cases that yields full structural code coverage of the test object. Such a test set can be used for effectively performing the testing activity, providing confidence in the quality and robustness of the test object. The rationale of working at the bytecode level is that even when the source code is unavailable structural testing requirements can still be derived, and used to assess the quality of a test set and to guide the evolutionary search towards reaching specific test goals.
Mobile devices, such as Smartphones, are being used virtually by every modern individual. Such devices are expected to work continuously and flawlessly for years, despite having been designed without criticality requirements. However, the requirements of mobility, digital identification and authentication lead to an increasing dependence of societies on the correct behaviour of these 'proxies for the individual'. The Windows Mobile 5.0 release has delivered a new set of internal state monitoring services, centralized into the State and Notifications Broker. This API was designed to be used by context-aware applications, providing a comprehensive monitoring of the internal state and resources of mobile devices. In this paper we propose using this service to increase the dependability of mobile applications by showing, through a series of fault-injection campaigns, that this novel API is very effective for error propagation profiling and monitoring.
Evolutionary Testing is an emerging methodology for automatically generating high quality test data. The focus of this paper is on presenting an approach for generating test cases for the unit-testing of object-oriented programs, with basis on the information provided by the structural analysis and interpretation of Java bytecode and on the dynamic execution of the instrumented test object. The rationale for working at the bytecode level is that even when the source code is unavailable, insight can still be obtained and used to guide the search-based test case generation process. Test cases are represented using the Strongly Typed Genetic Programming paradigm, which effectively mimics the polymorphic relationships, inheritance dependences and method argument constraints of object-oriented programs.
The focus of this paper is on presenting a methodology for
generating and optimizing test data by employing evolutionary search
techniques, with basis on the information provided by the analysis and
interpretation of Java bytecode and on the dynamic execution of the
instrumented test object.
The main reason to work at the bytecode level is that even when the source
code is unavailable, structural testing requirements can still be derived and
used to assess the quality of a given test set and to guide the evolutionary
search towards reaching specific test goals.
Java bytecode retains enough high-level information about the original source
code for an underlying model for program representation to be built. The
observations required to select or generate test data are obtained by
employing dynamic analysis techniques ' i.e. by instrumenting, tracing and
analysing Java bytecode.

Search-based test case generation for object-oriented software is hindered by the size of the search space, which encompasses the arguments to the implicit and explicit parameters of the test object's public methods. The performance of this type of search problems can be enhanced by the definition of adequate Input Domain Reduction strategies.

The focus of our on-going work is on employing evolutionary algorithms for generating test data for the structural unit-testing of Java programs. Test cases are represented and evolved using the Strongly-Typed Genetic Programming paradigm; Purity Analysis is particularly useful in this situation because it provides a means to automatically identify and remove Function Set entries that do not contribute to the definition of interesting test scenarios.
This paper proposes an adaptive strategy for enhancing Genetic Programming-based approaches to automatic test case generation. The main contribution of this study is that of
proposing an adaptive Evolutionary Testing methodology for promoting the introduction of relevant instructions into the generated test cases by means of mutation; the instructions from which the algorithm can choose are ranked, with their rankings being updated every generation in accordance to the feedback obtained from the individuals evaluated in the preceding generation. The experimental studies developed show that the adaptive strategy proposed improves the algorithm's efficiency considerably, while introducing a negligible computational overhead.
In Evolutionary Testing, meta-heuristic search techniques are used for generating test data. The focus of our research is on employing evolutionary algorithms for the structural unit-testing of Object-Oriented programs. Relevant contributions include the introduction of novel methodologies for automation, search guidance and Input Domain Reduction; the strategies proposed were empirically evaluated with encouraging results.

Test cases are evolved using the Strongly-Typed Genetic Programming technique. Test data quality evaluation includes instrumenting the test object, executing it with the generated test cases, and tracing the structures traversed in order to derive coverage metrics. The methodology for efficiently guiding the search process towards achieving full structural coverage involves favouring test cases that exercise problematic structures. Purity Analysis is employed as a systematic strategy for reducing the search space.


Software quality is a crucial factor for system success. Several tools have been proposed to support the evaluation and comparison of software architecture designs. However, the diversity in focus, approaches, interfaces and results leaves the researcher and practitioner wondering what would be the most appropriate solution for their specific goals. This paper presents a comparison framework that identifies the most relevant features for categorizing different architecture evaluation tools according to six different dimensions. The results show the attributes that a comprehensive tool should support including: the ability to handle multiple modeling approaches, integration with the industry standard UML or specific ADL, support for trade-off analysis of competing quality attributes and, the reuse of knowledge through the build-up of new architectural patterns. This comparison is able to, not only guide the choice of evaluation, but also promote the development of more powerful tools for modeling and analysis of software architectures.
A sociedade moderna depende de modo vital dos seus sistemas informáticos: desde servidores utilizados em grandes instituições financeiras a microssistemas dedicados para detecção de intrusão, passando por computadores de bordo de veículos automóveis, o correcto funcionamento destes sistemas é essencial para salvaguardar vidas humanas e prevenir perdas económicas ou riscos ambientais.
A garantia do correcto funcionamento destes sistemas informáticos é uma tarefa exigente dada a sua complexidade. Assim, enquanto que em sistemas de grande porte as soluções passam essencialmente por abordagens transaccionais (certificação de que as operações efectuadas o são de forma coerente), tal não é possível em sistemas embedded. Efectivamente, nos sistemas que trabalham em tempo-real interagindo com o mundo físico, na maioria dos casos não é possível anular acções de controlo erróneas e repeti-las correctamente. Esta situação é particularmente grave quando do sistema em causa podem depender vidas humanas (p.ex. sistemas médicos de suporte à vida ou controlo de ABS – sistema anti-bloqueio de travões).
O Laboratório de Informática e Sistemas do Intituto Pedro Nunes (lis.IPN) trabalha há vários anos no desenvolvimento e validação de sistemas embedded para aplicações críticas nos domínios do controlo industrial, sistemas médicos, computadores de bordo para satélites, aeronáutica e sector automóvel. Na presente comunicação serão apresentadas algumas das abordagens utilizadas nestes domínios, com ênfase na validação da robustez de sistemas embedded a perturbações de origem física (ruído eléctrico ambiental, vibrações e partículas subatómicas).

Field Programmable Gate Arrays (FPGAs), are being increasingly used in custom systems requiring fast time-to-market delivery due to their flexibility; being reprogrammable in the field is a real value for long unattended operation and whenever on- site maintenance is costly as is the case in many remote data acquisition stations. The most powerful FPGAs are based in SRAM technology which is particularly prone to transient faults. Fault-tolerance is therefore mandatory and this can be done by simply reprogramming the FPGA, thus repairing the corrupted configuration. Recent advances in FPGA technology allow the configuration of just a portion of the FPGA, which lowers significantly the time overhead, while the remaining parts are running. This truly dynamic partial FPGA reconfiguration can be used to provide fault-tolerance for hard real-time applications, guaranteeing high reliability in long missions. This short paper addresses the technological aspects of partial dynamic reconfigurable FPGAs, presents some of most important threats to dependability of these devices, and identifies some research areas to investigate in order to increase dependability.


The software systems have been exposed to constant changes in a short period of time. The evolution of these systems demands a trade-off among several attributes to keep the software quality acceptable. It requires high maintainable systems and makes maintainability one of the most important quality attributes. This paper approaches the system evolution through the analysis of potential new architectures using the evaluation of maintainability level. The goal is to relate maintainability metrics applied in the source-code of OO systems, in particular CCC, to notations defined by COSMIC methods and proposes metrics-based models to assess CCC in software architectures.
Quantitative assessment of quality attributes (i.e., non-functional requirements, such as performance, safety or reliability) of software architectures during design supports important early decisions and validates the quality requirements established by the stakeholder. In current practice, these quality requirements are most often manually checked, which is time- consuming and error-prone due to the overwhelmingly complex designs. We propose an automated approach to assess the reliability of software architectures. It consists in extracting a Markov model from the system specification written in an Architecture Description Language (ADL). Our approach translates the specified architecture to a high-level probabilistic model- checking language, supporting system validation and quantitative reliability prediction against usage profile, component arrangement and architectural styles. We validate our approach by applying it to different architectural styles and comparing those with two different quantitative reliability assessment methods presented in the literature: the composite and the hierarchical methods.
Software engineers and practitioners regard software architecture as an important artifact, providing the means to model the structure and behavior of systems and to support early decisions on dependability and other quality attributes. Since systems are most often subject to evolution, the software architecture can be used as an early indicator on the impact of the planned evolution on quality attributes. We propose an automated approach to evaluate the impact on reliability of architecture evolution. Our approach provides relevant information for architects to predict the impact of component reliabilities, usage profile and system structure on the overall reliability. We translate a system's architectural description written in an Architecture Description Language (ADL) to a stochastic model suitable for performing a thorough analysis on the possible architectural modifications. We applied our method to a case study widely used in research in which we identified the reliability bottlenecks and performed structural modifications to obtain an improved architecture regarding its reliability.

The eCrash tool employs Strongly-Typed Genetic Programming to automate the generation of test data for the structural unit testing of Object-Oriented Java programs. This paper depicts the results attained by utilising eCrash to generate test data for the classes of the Apache Ant project.
This paper presents a preliminary evaluation of the SEU Controller Macro, a VHDL component developed by Xilinx for the detection and recovery of single event upsets, as a
building block of an FPGA fault-injector. We found that this SEU Controller Macro is extremely effective for injecting faults into the FPGA configuration memory, as single and double bitflips, with precise location, virtually no intrusiveness, and coarse timing accuracy. We present some clues on how to extend its functionalities to build a fully-fledge FPGA fault injector.
Quality attributes (e.g., performance, reliability and security) are detailed in a system’s architecture and determine the fitness of purpose and satisfaction of stakeholders regarding the final product. Although research has provided methods to assess different quality attributes, few checks are automatically performed. Manually checking a quality attribute from a large and complex architecture is a time-consuming and error-prone task. This paper addresses this issue by generating stochastic models that predict and analyze reliability from a software architecture description, in an automated way. In addition, our approach has been compiled into a tool targeting system architects, the Affidavit tool. This tool is accessible from an architecture development framework and provides information about structural issues and reliability bottlenecks of systems. As a result, Affidavit allows architects to reason about the designed architecture, helping to avoid architectural arrangements that might have a negative impact on the overall system reliability, at the same time that it indicates the most suitable arrangement for specific contexts. This paper describes the tool and its implementation details, demonstrating its capabilities on practical systems.



Self-adaptive systems continuously monitor runtime properties and analyze, plan and execute adaptation strategies to adjust their behavior in order to meet satisfactory quality levels. These systems have been applied to a wide variety of scenarios ranging from managing clusters to self-driving cars. However, such achievements require significant computational power, and there is no solid knowledge on which adaptation phases experience the higher consumption of resources. This is unfortunate, as only such insight can support a systematic optimization by improving tools, algorithms or technologies.
In this paper we assess the performance of a reference model architecture for self-adaptive systems, the MAPE-K loop, identify bottlenecks, then suggest and implement some improvements. Finally, we present a performance comparison between different approaches including the one with our improvements and show the actual financial costs of deploying such systems in a mainstream cloud computing service (AWS). We show that our approach leads to a reduction of up to 45% on annual computational costs, without degradation on the quality of service provided.
The quantitative assessment of quality attributes on software architectures allow to support early decisions in the design phase, certify quality requirements established by stakeholders and improve software quality in future architectural changes. In literature, only few of these quality requirements are verified and most often they are manually checked, which is time-consuming and error-prone due to the overwhelmingly complex designs. The goal of this thesis is to provide means for architects predict and analyze availability constraints on software architectures. We plan to generate a stochastic model from an architectural description specified by an Architecture Description Language (ADL) properly annotated to be solved by a probabilistic model-checking tool. This model will allow to quantitatively predict availability and identify bottlenecks that are negatively influencing the overall system availability. Hence, our approach will help architects to avoid undesired or infeasible architectural designs and prevent extra costs in fixing late life-cycle detected problems.
Quality attributes (e.g., performance, reliability and security) are detailed in a system’s architecture and determine the fitness of purpose and satisfaction of stakeholders regarding the final product. Although research has provided methods to assess different quality attributes, few checks are automatically performed. Manually checking a quality attribute from a large and complex architecture is a time-consuming and error-prone task. This paper addresses this issue by generating stochastic models that predict and analyze reliability from a software architecture description, in an automated way. In addition, our approach has been compiled into a tool targeting system architects, the Affidavit tool. This tool is accessible from an architecture development framework and provides information about structural issues and reliability bottlenecks of systems. As a result, Affidavit allows architects to reason about the designed architecture, helping to avoid architectural arrangements that might have a negative impact on the overall system reliability, at the same time that it indicates the most suitable arrangement for specific contexts. This paper describes the tool and its implementation details, demonstrating its capabilities on practical systems.
In Object-Oriented Evolutionary Testing, metaheuristics are employed to select or generate Test Data for Object-Oriented software. The application of search-based strategies to the Software Testing of Object-Oriented Software is fairly recent and is yet to be investigated comprehensively; this article aims to explore, review and contextualize relevant literature and research in this area, while pinpointing open problems and setting grounds for future work.
The measurement of external software attributes and the analysis of how those attributes have evolved through the software's releases are challenging activities. This is particularly evident when we discuss the maintainability of Object-Oriented (OO) systems which, due to their specific characteristics, hide information that cannot be gathered through static analysis. As maintainability can be defined as the "speed and ease with which a program can be corrected or changed", we believe that test data are reflective of the changes performed during maintenance. Moreover, empirical observations allow to speculate about the relationships between maintainability and the behaviour of the software when executed by test cases (e.g., coverage values) and the test cases' characteristics (e.g., generation time). Our aim is to complement the state-of-the art by proposing a new approach for understanding and characterizing the maintainability of OO systems, which makes use of test data and of the information gathered from the tests' execution.
Among all the reasons that leads to the success or failure of a Free/Libre Open Source Software (FLOSS) project, understanding the system’s evolution can reveal important pieces of information to open source stakeholders, helping them to identify what can be improved in the software system’s internal organization. Once software complexity is one of the most important attributes to determine software maintainability, controlling its level in the system evolution process makes the software easier to maintain, reducing the maintainability costs. Otherwise, uncontrolled complexity makes the maintenance and enhancement process lengthy, more costly and some times it can contribute to the system abandonment. This work investigates the evolution of complexity in discontinued FLOSS projects. After several analyses, the results showed that inactive FLOSS projects do not seem to be able to keep up with the extra work required to control the systems complexity, presenting a different behaviour of the successful active FLOSS projects.
A reconfigurable weblab prototype was designed according to the IEEE1451.0 Std. and based on FPGAs. A brief introduction about its architecture and underlying infrastructure is presented. After an overview about the main features of the weblab, namely the standard access and the reconfiguration capability, the proposed demonstration for the exhibition session is described.
It is already more than 10 years that weblabs are seen as important resources to provide the experimental work required in engineering education. Several weblabs have been applied in engineering courses, but there are still unsolved problems related to the development of their infrastructures. For solving some of those problems, it was implemented a weblab with a reconfigurable infrastructure compliant with the IEEE1451.0 Std. and supported by Field Programmable Gate Array (FPGA) technology. This paper presents the referred weblab, and provides and analyses a set of researchers’ opinions about the implemented infrastructure, and the adopted methodology for the conduction of real experiments.
The software systems have been exposed to constant changes in a short period of time. It requires high maintainable systems and makes maintainability one of the most important quality attributes. In this work we performed a statistical analysis of maintainability metrics in three mainstream open-source applications, Tomcat (webserver), Jedit (text editor) and Vuze (a peer to peer client). The metrics are applied to source-code and to derived similar architectural metrics using scatter plot, Pearson’s correlation coefficient and significance tests. The observations contradict the common assumption that software quality attributes (aka non-functional requirements) are mostly determined at the architectural level and raise new issues for future works in this field.
The reconfiguration capability provided by Field Programmable Gate Arrays (FPGA) and the current limitations of weblab infrastructures, opened a new research window. This paper focus on describing the way weblabs can be reconfigured with different Instruments & Modules (I&M) required to conduct remote experiments, without changing the entire infrastructure. For this purpose, the paper emphasizes the advantage of using FPGAs to create reconfigurable weblab infrastructures using the IEEE1451.0 Std. as a basis to develop, access and bind embedded I&Ms; to an IEEE1451.0-Module
Weblabs are spreading their influence in Science and Engineering (S&E) courses providing a way to remotely conduct real experiments. Typically, they are implemented by different architectures and infrastructures supported by Instruments and Modules (I&Ms;) able to be remotely controlled and observed. Besides the inexistence of a standard solution for implementing weblabs, their reconfiguration is limited to a setup procedure that enables interconnecting a set of preselected I&Ms; into an Experiment Under Test (EUT). Moreover, those I&Ms; are not able to be replicated or shared by different weblab infrastructures, since they are usually based on hardware platforms. Thus, to overcome these limitations, this paper proposes a standard solution that uses I&Ms; embedded into Field-Programmable Gate Array (FPGAs) devices. It is presented an architecture based on the IEEE1451.0 Std. supported by a FPGA-based weblab infrastructure able to be remotely reconfigured with I&Ms;, described through standard Hardware Description Language (HDL) files, using a Reconfiguration Tool (RecTool).
Reconfigurable embedded devices built on SRAM-based Field Programmable Gate Arrays (FPGA) are being increasingly used in critical embedded applications. However, the susceptibility of such memory cells to Single Event Upsets (SEU) requires the use of fault tolerant designs, for which fault injection is still the most accepted verification technique. This paper describes FIRED, a fault injector targeted at SRAM-based FPGAs for dependability evaluation of critical systems. This tool is able to perform hardware fault injection in real-time, by inserting bitflips at the SRAM cells through partial dynamic reconfiguration. These faults may produce errors in the design of the VHDL or Verilog modules deployed in the FPGA. A case study of a fault injection campaign in a PID-based cruise control system is used to evaluate the capabilities of FIRED, namely its capacity of injecting faults while a physical application is being controlled.

This paper describes the methodology, architecture and features of the eCrash framework, a Java-based tool which employs Strongly-Typed Genetic Programming to automate the generation of test data for the structural unit testing of Object-Oriented programs. The application of Evolutionary Algorithms to Test Data generation is often referred to as Evolutionary Testing. eCrash implements an Evolutionary Testing strategy developed with three major purposes: improving the level of performance and automation of the Software Testing process; minimising the interference of the tool’s users on the Test Object analysis to a minimum; and mitigating the impact of users decisions in the Test Data generation process.
The ever-growing complexity of software systems makes it increasingly challenging to foresee at design time all interactions between a system and its environment.

Most self-adaptive systems trigger adaptations through operators that are statically configured for specific environment and system conditions. However, in the occurrence of uncertain conditions, self-adaptive decisions may not be effective and might lead to a disruption of the desired non-functional attributes.

To address this, we propose an approach that improves the planning stage by predicting the outcome of each strategy. In detail, we automatically derive a stochastic model from a formal architecture description of the managed system with the changes imposed by each strategy. Such information is used to optimize the self-adaptation decisions to fulfill the desired quality goals.

To assess the effectiveness of our approach we apply it to a cloud-based news system and predicted the reliability for each possible adaptation strategy. The results obtained from our approach are compared to a representative static planning algorithm as well as to an oracle that always makes the ideal decision. Experiments show that our method improves both availability and cost when compared to the static planning algorithm, while being close to the oracle.

Our approach may therefore be used to optimize self-adaptation planning.












O VoIP (Voice over Internet Protocol) tem assumido uma importância crescente no mercado de telecomunicações. Ao usar a Internet como meio de transmissão da voz, o VoIP permite uma redução de custos significativa, ligando quaisquer dois pontos que lhe tenham acesso. No entanto, um serviço de voz requer restrições especiais de modo a ser obtida uma comunicação que satisfaça as exigências de Qualidade de Serviço dos interlocutores. Neste artigo explicam-se os conceitos básicos da VoIP e realizam-se alguns testes num ambiente de simulação simplificado, usando tráfego best-effort e o User Datagram Protocol (UDP), para determinar a performance de uma transmissão genérica de VoIP. São analisados os efeitos da utilização de uma rede com classes de prioridades distintas e o efeito de alguns parâmetros sobre a transmissão, latência, jitter, perda de pacotes, tamanho dos buffers.
Complex systems exhibiting structural changes can be better represented by models that can mimic these transformations. The Heterogeneous Flow System Specification (HFSS) is a comprehensive formalism that can represent a large variety of models using a unifying representation. The HFSS formalism represents models in a hierarchical and modular form. The explicit representation of structure makes possible to alter it dynamically. Among hybrid models, the most important include digital controllers and hybrid integrators. The HFSS ability to provide a common ground to represent all these elements permits to model complex systems in a simple framework. We present a detailed model of a PID controller and we show how this digital controller can be merged with other types of components. To illustrate formalism application we use a dynamic structure network of hybrid components to model a 2-stage rocket system, whose velocity is set by a PI digital controller. Simulation results for the rocket system are presented.
Target tracking plays a major role in traffic management and defense applications. While tracking algorithms can be tested with real data, this situation is often not possible due to the high costs evolved in real experiments and to the impracticability of some unsafe scenarios. To overcome the limitation of accessing algorithm efficiency with real data we describe a modeling environment based on the Heterogeneous Flow Systems Specification methodology (HFSS) that can be used to simulate radar detection. The HFSS formalism can describe multiple radars and multiple targets. To illustrate our approach we test a scenario with two radars and one target.




















&#61485;


Conventional modeling and simulation formalisms only give support to the representation of model behavior, providing no constructs for describing changes in model structure. However, some systems are better modeled by self-reconfigurable formalisms. We have developed the Discrete Flow System Specification (DFSS) to exploit dynamic structure, component-based and hierarchical model construction. Due to structural similarity, dynamic self-configuring DFSS models offer a good description of systems, like adaptive algorithms and reconfigurable computer architectures. In this paper, we present the modeling and simulation of a parallel adaptive divide-and-conquer integration algorithm in the CaosTalk modeling and simulation framework, a realization of the DFSS formalism.

















The support for software reuse has been a major goal in the design of programming languages. This goal, however, has proven difficult to reach, being only partially enabled by current software tools. In particular, reuse is not fully supported by object-oriented programming (OOP). Aspect-oriented programming (AOP) has introduced new operators that extend OOP, enabling a superior support for reusability. However, AOP operators exhibit limitations in supporting software reuse and more powerful constructs are still required. We consider the ability to define software in an independent manner as the key construct to enable systematic software reuse. To bridge the gap between independence and practical software tools, we have developed the concept of Independent and Pluggable Software Unit (PU), a construct that supports the definition of software topologies. In this paper, we compare PUs with AOP in their support for reusable software. To enable comparison, we employ some well described problems addressed by Software Design Patterns (SDPs). We provide PU and AOP versions of several SDPs, including, Observer, Composite, Command, Chain of Responsibility, and Proxy. In particular, we show that, whereas PUs provide a unified representation of design patterns, AOP representations do not achieve this unification. We also show that AOP solutions do not promote independent and reusable software.



Fault-tolerance is vital for dependable distributed applications that can deliver service, even in the presence of faults. Over the last few decades, above all protocols proposed to offer reliability and fault-tolerance, TCP grew to become one of the cornerstones of the Internet. However, despite emulating reliable communication in distributed environments, TCP does
not handle connection failures when the connectivity is lost for some time, even if both endpoints are still running. When this occurs, developers must rollback the peers to some coherent state, many times with error-prone, ad hoc, or custom application-level
solutions.
In this paper, we refine the Acceptor-Connector design pattern to tackle the TCP unreliability problem. The pattern decouples the failure-related processing from the connection and service
processing, efficiently handling different connections and their possible crashes concurrently, thereby yielding more reusable, extensible, and efficient distributed communication. The solution we propose incorporates proven multi-threaded solutions and a buffering scheme that discards the need for an applicationlayer acknowledgment scheme. This simplifies the development of reliable connection-oriented applications using the ubiquitous
TCP protocol.
Despite offering reliability against dropped and reordered packets, the widely adopted Transmission Control Protocol (TCP) provides nearly no recovery options for  long-term network outages.
When the network fails, developers must rollback the application to some coherent state on their own using error-prone solutions. Overcoming this limitation is, therefore, a deeply investigated and challenging problem. Existing solutions range from transport-layer to application-layer protocols, including additions to TCP, usually transparent to the application. None of these solutions is perfect for the task, because they all impact TCP's simplicity, performance or ubiquity, if not all.

To avoid these shortcomings, we contain TCP connection crashes inside a single session layer exposed as a sockets interface. Based on this interface, we create a blocking and a non-blocking fault-tolerant design pattern. We explore the blocking design in an open source File Transfer Protocol (FTP) server and perform a thorough evaluation of performance, complexity and overhead of both designs. Our results show that using one of the patterns to tolerate TCP connection crashes, in new or existing applications, involves a very limited effort and negligible penalties.






Fault-tolerance is vital for dependable distributed applications that can deliver service, even in the presence of faults. Over the last few decades, above all protocols proposed to offer reliability and fault-tolerance, TCP grew to become one of the cornerstones of the Internet. However, despite emulating reliable communication in distributed environments, TCP does not handle connection failures when the connectivity is lost for some time, even if both endpoints are still running. When this occurs, developers must rollback the peers to some coherent state, many times with error-prone, ad hoc, or custom application-level solutions.

In this report, we refine the Acceptor-Connector design pattern to tackle the TCP unreliability problem. The pattern decouples the failure-related processing from the connection and service processing, efficiently handling different connections and their possible crashes concurrently, thereby yielding more reusable, extensible, and efficient distributed communication. The solution we propose incorporates proven multi-threaded solutions and a buffering scheme that discards the need for an application-layer acknowledgment scheme. This simplifies the development of reliable connection-oriented applications using the ubiquitous TCP protocol.
A mobile agent is a simple, natural and logical extension of the remote distributed object concept. It is an object with an active thread of execution that is capable of migrating between applications. By using mobile agents, the programmer is no longer confined to have static objects and perform remote invocations, but can program the objects to move directly between applications. In itself, a mobile agent is just a programming abstraction: an active object that can move when needed. It is a structuring primitive, similar to the notion of class, remote object or thread. 

The central argument of this thesis is that although the mobile agent concept is just a structuring primitive, the current mainstream middleware implementations do not convey it simply as such. In fact, they force all the development to be centered on mobile agents, provide poor integration with existing programming environments, do not integrate well with other middleware, and force the developers to depart from current object-oriented techniques. The objective of this work was to demonstrate that it is possible to create an infrastructure such that the mobile agent concept can be leveraged into existing object-oriented languages in a simple and transparent way, without interfering in the manner in which the applications are normally structured. 

In order to achieve this goal, a component-oriented framework was devised and implemented, allowing programmers to use mobile agents as needed. Applications can still be developed using current object-oriented techniques but, by including certain components, they gain the ability to send, receive and interact with agents. The component palette was implemented using the JavaBeans technology and given the name M&M. Furthermore it was integrated with ActiveX, allowing programmers from any programming language that supports COM/ActiveX to take advantage of this paradigm. 
To validate the soundness of the approach, a large number of applications have been implemented using M&M. Two application domains were of particular interest: agent-enabling web servers and disconnected computing. The M&M component framework allows any web server that supports the Servlet Specification to send, receive and use mobile agents. M&M provides a clean integration, requiring neither a custom-made web server nor a special purpose agent platform. Finally, supporting disconnected computing has allowed us to understand the current limitations of the framework in supporting very low-level operations.



The Java Remote Method Invocation (RMI) API shields the developer from the details of distributed programming, allowing him to concentrate on application specific code. But to perform some operations that are orthogonal to the application, like logging, auditing, caching, QoS, fault tolerance, and security, sometimes it is necessary to customize the default behavior of the RMI runtime. Other middleware for distributed programming, like CORBA and the Remoting framework of the .NET platform, support smart proxies and interceptors, which can be used for these purposes, allowing the separation of application-specific code from service-specific code. In RMI there is no direct way of doing so. This paper presents a framework based on the Dynamic Proxy API for using smart proxies and interceptors with RMI. This framework requires no changes in the client application and minimal changes in the server application, giving the developer greater control over the
distributed application. A practical example of use is also given, by using the described framework to implement user authentication and fine-grained access control in RMI.

Distributed Data Collector (DDC) is a framework to ease and automate repetitive executions of console applications (probes) over a set of LAN networked Windows personal computers. The framework allows for the remote execution of probes, providing support for collecting the execution output of probes (standard output and standard error). Additionally, right after a probe execution, the output can be parsed and processed by user defined code (post collecting code) that can act accordingly to user's needs. The framework can be useful to perform repetitive large-scale monitoring and administrative tasks over machines with transient availability, that is, machines that present no guarantees of being available at a given time.
A major strength of DDC lies in the fact that it does not require installation of software in remote nodes, avoiding administrative burdens that remote daemons and alike normally provoke.
Besides presenting the data collection framework, the paper discusses the results of a 30-day monitoring experiment conducted with DDC. The experiment collected resource usage metrics over 169 Windows machines from classroom laboratories of an academic institution.
Code instrumentation is a mechanism that allows modules of programs to be completely rewritten at runtime. With the advent of virtual machines, this type of functionality is becoming more interesting because it allows the introduction of new functionality after an application has been deployed, easy implementation of aspect-oriented programming, performing security verifications, dynamic software upgrading, among others.
The Runtime Assembly Instrumentation Library (RAIL) is one of the first frameworks to implement code instrumentation in the .NET platform. It specifically addresses the limitations that exist between the reflection capabilities of .NET and its code emission functionalities. RAIL gives the programmer an object-oriented vision of the code of an application, allowing assemblies, modules, classes, references and even intermediate code to be easily manipulated. 
This paper addresses the design of an implementation of RAIL along with the difficulties and lessons learned while building a framework for code instrumentation in .NET.
Studies focusing on Unix have shown that the vast majority of workstations and desktop computers remain idle for most of the time. In this paper we quantify the usage of main resources (CPU, main memory, disk space and network bandwidth) of Windows 2000 machines from classroom laboratories. For that purpose, 169 machines of 11 classroom laboratories of an academic institution were monitored over 77 consecutive days. Samples were collected from all machines every 15 minutes for a total of 583653 samples.
Besides evaluating availability of machines (uptime and downtime) and usage habits of users, the paper assesses usage of main resources, focusing on the impact of interactive login sessions over resource consumptions. Also, recurring to Self Monitoring Analysis and Reporting Technology (SMART) parameters of hard disks, the study estimates the average uptime per hard drive power cycle for the whole life of monitored computers. The paper also analyzes the potential of non dedicated classroom Windows machines for distributed and parallel computing, evaluating the mean stability of group of machines.
Our results show that resources idleness in classroom computers is very high, with an average CPU idleness of 97.93%, unused memory averaging 42.06% and unused disk space of the order of gigabytes per machine. Moreover, this study confirms the 2:1 equivalence rule found out by similar works, with N non dedicated resources delivering an average CPU computing power roughly similar to N/2 dedicated machines. These results confirm the potentiality of these systems for resource harvesting, especially for grid desktop computing schemes. However, the efficient exploitation of the computational power of these environments requires adaptive fault tolerance schemes to overcome the high volatility of resources.
Studies focusing on Unix have shown that the vast majority of workstations and desktop computers remain idle for most of the time. In this paper we quantify the usage of main resources (CPU, main memory, disk space and network bandwidth) of
Windows 2000 machines from classroom laboratories. For that purpose, 169 machines of 11 classroom laboratories were monitored over 77 consecutive days. Samples were collected from all machines every 15 minutes for a total of 583653 samples.
Besides evaluating availability of machines (uptime and downtime) and usage habits of users, the paper assesses usage of main resources, focusing on the impact of interactive login sessions over resource consumptions. Also, resorting to Self Monitoring Analysis and Reporting Technology (SMART) parameters of hard disks, the study estimates the average uptime per hard drive power cycle for the whole life of monitored computers.
Our results show that resources idleness in classroom computers is very high, with an average CPU idleness of 97.9%, unused memory averaging 42.1% and unused disk space of the order of gigabytes per machine. Moreover, this study confirms the 2:1 equivalence rule found out by similar works, with N non-dedicated resources delivering an average CPU computing power roughly similar to N/2 dedicated machines. These results confirm the potentiality of these systems for resource harvesting, especially for grid desktop computing schemes.

O C# é a linguagem de programação criada pela Microsoft e especialmente pensada para o desenvolvimento de aplicações na plataforma .NET. Esta linguagem e a sua associada plataforma prometem modificar radicalmente a forma como as aplicações são desenvolvidas para Windows, e também para a Internet. Aliando todo o poder do C++ com a facilidade de programação do Visual Basic, o C# é uma linguagem rápida e moderna, desenhada especificamente para aumentar a produtividade dos programadores. Este livro é destinado a todos os profissionais, investigadores e estudantes universitários que adoptem a linguagem C# para o desenvolvimento de aplicações. 

Os tópicos cobertos incluem:
?	Programação orientada aos objectos em C#
?	Programação baseada em componentes
?	Tratamento de erros baseado em excepções
?	Classes base da plataforma .NET
?	Colecções de objectos
?	Ficheiros e streams
?	Serialização de objectos
?	Execução concorrente
?	Acesso à Internet
?	Desenvolvimento de web services
Most modern programming languages rely on exceptions for dealing with errors. Although exception handling was a significant improvement over other mechanisms like checking return codes, it's far from perfect. In fact, it can be argued that this mechanism is seriously flawed. In this paper we argue that exception handling should be automatically done at the runtime/operating system level. The motivation is similar to the one that lead to garbage collection: memory management was a tedious and error prone process, thus virtual machines included support for taking care of it. We believe that many exceptions can be automatically dealt with, and recovered, as long as appropriate mechanisms exist in the runtime environment. We believe that this approach may dramatically influence the way programming languages are designed and significantly contribute to having more robust code, being  actually developed with much less programming effort.
The emergence of exception handling mechanisms in modern programming languages made available a different way of communicating errors between procedures. For years, programmers trusted in the correct documentation for error codes returned by procedures to correctly handle erroneous situations. Now, they have to focus on the documentation of exceptions for the same effect. But to which extent can exception documentation be trusted? Moreover, is there enough documentation for exceptions? And in which way do these questions relate to the checked vs. unchecked exceptions discussion? For a given set of Microsoft .NET applications, code and documentation were thoroughly parsed and compared. This showed that exception documentation tends to be scarce and of poor quality when existent. In particular, it showed that 90% of exceptions are undocumented. Furthermore, programmers were demonstrated to be keener to document exceptions they explicitly throw while typically leaving exceptions resulting from method calls undocumented. These results are a contribution to the assessment of the effectiveness of the unchecked exceptions approach.

This paper describes DGSchedSim, a trace driven simulator to evaluate scheduling algorithms focused on minimising turnaround time of applications executed in heterogeneous desktop grid systems. The simulator can be used to model task farming applications comprised of a set of independent and equal sized tasks similarly to numerous @Home public computing projects like the popular SETI@Home. DGSchedSim permits to assess scheduling policies under several scenarios allowing to control parameters such as the application's requirements (number of tasks, individual requirements of tasks like needed CPU time), the properties of the environment (machines computing capabilities and availabilities) and the characteristics of the execution (frequency and storage location of checkpoints, etc.). The simulations are driven by traces collected from real desktop grid systems.

Besides DGSchedSim, the paper presents the Cluster Ideal Execution Time (CIET) algorithm that computes the ideal wall-clock time required by a fully dedicated and totally reliable cluster of M heterogeneous machines to process the T tasks of an application.
As a test to the simulator capabilities, the paper analyses the suitability of two scheduling algorithms, FCFS and MinMax, for delivering fast turnaround time in desktop grids. Both algorithms, when combined with a centrally stored checkpoint policy, achieve efficiency close to 50% of CIET for certain scenarios.
One central problem preventing widespread adoption of mobile agents as a code structuring primitive is that current mainstream middleware implementations do not convey it simply as such. In fact, they force all the development to be centered on mobile agents, which has serious consequences in terms of software structuring and, in fact, technology adoption. This chapter discusses the main limitations of the traditional platform-based approach, proposing an alternative: component-based mobile agent systems. Two case studies are discussed: the JAMES platform, a traditional mobile agent platform specially tailored for network management; and M&M, a component-based system for agent-enabling applications. Finally, a bird's eye perspective on the last 15 years of mobile agent systems research is presented along with an outlook on the future of the technology. The authors hope that this chapter brings some enlightenment on the pearls and pitfalls surrounding this interesting technology and ways for avoiding them in the future.
Most modern programming languages rely on exceptions for dealing with abnormal situations. Although exception handling was a significant improvement over other mechanisms like checking return codes, it is far from perfect. In fact, it can be argued that this mechanism is seriously limited, if not, flawed. This paper aims to contribute to the discussion by providing
quantitative measures on how programmers are currently using exception handling. We examined 32 different applications, both for Java and .NET. The major conclusion for this work is that exceptions are not being correctly used as an error recovery mechanism. Exception handlers are not specialized enough for allowing recovery and, typically, programmers just do one of the following actions: logging, user notification and application termination. To our knowledge, this is the most comprehensive study done on exception handling to date, providing a quantitative measure useful for guiding the development of new error handling mechanisms.
The emergence of exception handling (EH) mechanisms in modern programming languages made available a different way of communicating errors between procedures. For years, programmers trusted in correct documentation of error codes returned by procedures to correctly handle erroneous situations. Now, they have to focus on the documentation of exceptions for the same effect. But to what extent can exception documentation be trusted? Moreover, is there enough documentation for exceptions? And in what way do these questions relate to the discussion on checked against unchecked exceptions? For a given set of Microsoft .NET applications, code and documentation were thoroughly parsed and compared. This showed that exception documentation tends to be scarce. In particular, it showed that 90% of exceptions are undocumented. Furthermore, programmers were demonstrated to be keener to document exceptions they explicitly throw while typically leaving exceptions resulting from method calls undocumented. This conclusion lead to another question: how do programmers use the EH mechanisms available in modern programming languages? More than 16 different .NET applications were examined in order to provide an answer. The major conclusion of this work is that exceptions are not being correctly used as an error-handling mechanism. These results contribute to the assessment of the effectiveness of the unchecked exceptions approach.
Exception handling mechanisms have been around for more than 30 years. Nevertheless, modern exceptions systems are not very different from the early
models. Programming languages designers often neglect the exception mechanism and look at it more like an add-on for their language instead of central part. As a consequence, software quality suffers as programmers feel that the task of writing good error handling code is too complex, unattractive and inefficient. We propose a new model that automates the handling of exceptions by the runtime platform. This
model frees the programmer from having to write exception handling code and, at the same time, successfully increases the resilience of programs to abnormal situations.

The rise of the multicore era is catapulting concurrency into mainstream programming. Current programming paradigms build in sequentiality, and as a result, concurrency support in those languages forces programmers into low-level reasoning about execution order. In this paper, we introduce a new programming paradigm in which concurrency is the default. Our ÿMINIUM language uses access permissions to express logical dependencies in the code at a higher level of abstraction than sequential order. Therefore compiler/runtime-system can leverage that dependency information to allow concurrent execution. Because in ÿMINIUM programmers specify dependencies rather than control flow, there is no need to engage in difficult, error-prone, and low-level reasoning about execution order or thread interleavings. Developers can instead focus on the design of the program, and benefit as the runtime automatically extracts the concurrency inherent in their design.
Everyday experience tells us that some errors are transient, but also that some can be handled simply by 'retryingâ? the failed operation. For instance, a glitch on the network might turn a resource unreachable for a short period of time; or a sudden peak of work on a server can cause a momentary denial of service. In many occasions, without other kind of specialized recovery code, it is possible to keep a program running only by retrying a failed operation. Unfortunately, retry is not explicitly available on many platforms or programming languages and, even if it were, it could not be blindly used for dealing with every abnormal situation. On languages like C# or Java, or even on languages that offer the retry construct such as Smalltalk and Eiffel, where errors are represented and communicated through exceptions, there is no simple way to clear the effects of a failed operation and, thus, re-attempt its execution. Programmers have to explicitly write sometimes complex and error-prone code to repair the state of a program and the execution context. In this paper, we propose an AOP technique for implementing 'retryâ? on systems lacking such a feature without using any language extensions for AOP or imposing modifications to the development language. Our approach eliminates the need for programmers to write \"state-cleaning\" code for normal objects by means of a transparent transactional mechanism and provides the means to identify non-idempotent operations on the code. In our evaluation we show that a relevant number of application failures can be masked using this approach.

Writing concurrent applications is extremely challenging, not only in terms of producing bug-free and maintainable software, but also for enabling developer productivity. In this paper we present the ÆMINIUM concurrent-by-default programming language. Using ÆMINIUM programmers express data dependencies rather than control flow between instructions. Dependencies are expressed using permissions, which are used by the type system to automatically parallelize the application. The ÆMINIUM approach provides a modular and composable mechanism for writing concurrent applications, preventing data races in a provable way. This allows programmers to shift their attention from low-level, error-prone reasoning about thread interleaving and synchronization to focus on the core functionality of their applications. We study the semantics of ÆMINIUM through ?ÆMINIUM, a sound core calculus that leverages permission flow to enable concurrent-by-default execution. After discussing our prototype implementation we present several case studies of our system. Our case studies show up to 6.5X speedup on an eight-core machine when leveraging data group permissions to manage access to shared state, and more than 70% higher throughput in a web server application.
The advent of multi-core systems set off a race to get concurrent programming to the masses. One of the challenging aspects of this type of system is how to deal with exceptional situations, since it is very difficult to assert the precise state of a concurrent program when an exception arises. In this paper we propose an exception-handling model for concurrent systems. Its main quality attributes are simplicity and expressiveness, allowing programmers to deal with exceptional situations in a concurrent setting in a familiar way. The proposal is centered on a new kind of exception type that defines new paths for exception propagation among concurrent threads of execution. In our model, beyond being able to control where exceptions are raised, the developer can define in which thread, and when during its execution, a particular exception will be handled. The proposed model has been implemented in Scala, and we show its application to the construction of concurrent software.
Assertions based on the knowledge of the application can be very effective at detecting corruption of critical data or design faults that transparent techniques are unable to deal with. Particularly, high coverage assertions are very promising because of their 
low-overhead and high error coverage.
This thesis starts by characterizing the behavior of assertions-based error detection mechanisms under faults injected according to a quite general fault model. The main problems are identified as being the lack of protection of data outside the section covered by assertions, namely during input and output, and the possible incorrect execution of the assertions. 
To handle those weak-points the Robust Assertions technique is proposed, whose effectiveness is shown by extensive fault injection experiments on a realistic control application. With this technique a system follows a new failure model, that is called 
Fail-Bounded, where with high probability all results produced are either correct or, if wrong, they are within a certain bound of the correct value, whose exact distance depends on the output assertions used. 
We claim that this failure model is very useful to describe the behavior of many systems with low hardware and software redundancy.

Any kind of assertions can be considered, from simple likelihood tests to high coverage assertions such as those used in the two paradigms investigated in this thesis, Algorithm Based Fault Tolerance (ABFT) and Result-Checking (RC). 
ABFT is the collective name of a set of techniques used to determine the correctness of some mathematical calculations. The basic idea is to apply some encoding to the input data of the calculation, execute the algorithm on the encoded data, and check that the encoding is preserved at the end. A less well known alternative is called Result Checking (RC) where, contrary to ABFT, results are checked without knowledge of the particular algorithm used to calculate them. These two error detection methods based on high coverage assertions assure a nearly complete error coverage for most of the matrix operations that are the basis of all scientific computation. 
A comparative study between ABFT and RC shows that RC is a good alternative to ABFT for detecting errors due to physical or design faults. Being the first time that RC is evaluated by fault injection, it is shown that although both methods exhibit equivalent error coverage, RC has lower overhead and has the important advantage of being independent of the underlying algorithm.
To substantiate the practical value of RC, we have developed a result checker for a problem for which no such checker existed: the computation of eigenvalues and eigenvectors, the so-called eigenproblem. The proposed fault detection mechanism based on the new RC, implemented on top of routines from the LAPACK library using the Robust Assertions structure, is simultaneously very efficient and very effective. It has less than 2% performance overhead for medium to large matrices and it exhibited an error coverage greater than 99.7% with a confidence level of 99%, when subjected to extensive 
fault-injection experiments.
We conclude that high coverage assertions can be used to build highly reliable 
low-overhead systems.
Traditionally related to industrial control, continuous real-time control systems are spread throughout every area of our technology driven society. One of the most popular ways to achieve the correct functioning of those systems is through redundancy, by replicating some parts of the controller hardware and comparing their results. However, the high cost demanded by this approach is normally only acceptable in mission-critic or human-critic systems. A major percentage of control systems is limited to ad hoc solutions for fault tolerance based on empirical observations.
An important characteristic shared by the generality of continuous real-time control systems is the ability to tolerate some external disturbances affecting the controlled process, such as the air temperature, wind or dust. However, when the controller itself is subject to faults and produces some wrong or late control actions, it has been observed that, some times, there is no need to use any redundant system to recover.
The purpose of replication is to guaranty the correct and timely behaviour of the controller. However, continuous real"-time control systems are able to intrinsically tolerate transient controller malfunctions, the same way they can tolerate external disturbances affecting the controlled process. It is thus acceptable that fault tolerance be considered to avoid the collapse of the whole system (i.e., when the system stops delivering the expected 
service or its quality becomes unacceptable), instead of avoiding the controller failure (i.e., when the controller produces wrong or late results).
By allowing a controller to fail, although during a limited amount of time, fault tolerance can be addressed by a new standpoint. It is then possible to trade off spatial by time redundancy, significantly reducing the costs for fault tolerance and thus permitting its use in low-cost control systems.
The studies presented in this thesis, along with some practical solutions and experimental results, prove that it is feasible to provide real-time control systems with high dependability by means of generic software solutions.
Code coverage tools are becoming increasingly popular as valuable aids in assessing and improving the quality of software structural tests. For some industries, such as aeronautics or space, they are mandatory in order to comply with standards and to help reduce the validation time of the applications. These tools usually rely on code instrumentation, thus introducing important time and memory overheads that may jeopardize its applicability to embedded and real-time systems. This paper explores the use of IEEE 1149.1 (boundary scan) infrastructure and on-chip debugging facilities from embedded processors for collecting the program execution trace during tests, without the introduction of any extra code, and then extracting detailed code coverage analysis and profiling information. We are currently developing an extension to the csXception tool to include such capabilities, in order to study the advantages, difficulties and impediments of using boundary scan for code coverage.
The European industry competitiveness in the embedded devices market is threatened by challenges such as cost-effectiveness, interoperability, reliability, and re-usability. This is particularly important now, when the value of embedded electronics components share in the final products is increasing, especially in ICT and health/medical equipment domains.
To address these challenges, the pSHIELD project, co-funded by ARTEMIS JU, was aimed at developing an architecture framework supporting security, privacy and dependability (SPD) as built-in features in a network of embedded nodes. That approach will provide industry with the key improvements such as a faster design, standardized development of SPD solutions and a flexible way to reuse already verified embedded systems.
This paper reports the architecture of an FPGA-based intrusion detection embedded device for a freight train, built to validate the pSHIELD concept at a node level. The use case demonstrates the legacy components integration, dependability, security, self-reconfiguration and the node-level composability.
Cost reduction of embedded systems design and development is on top of ARTEMIS targets for the consolidation of European industry world leadership in embedded computing technologies. The pSHIELD consortium proposed a framework addressing Security, Privacy and Dependability (SPD) in the context of Embedded Systems (ES) as “built in” rather than as “add-on” functionalities, allowing a seamless composition of SPD technologies.
In this context, the pSHIELD project, co-funded by ARTEMIS-JU initiative, aims at demonstrating SPD composability by applying both existing and new SPD technologies on a SHIELD integrated system. SPD composability is supported by a layered architecture composed of four levels: node, network, middleware and overlay. For each level, the state of the art in SPD of single technologies and solutions will be improved and integrated, and enhanced with composable functionality, in order to fit in the pSHIELD architectural framework.
This paper presents the study underway regarding the definition of the node level architectural framework, and the design and construction of a SPD Node ES prototype. The demonstrator scenario consists of a FM demodulation of an encrypted signal, exhibiting node-level composability, dependability, security, self reconfiguration and legacy component integration. The next step will be the integration of all the four layers into a full pSHIELD demonstrator.
As embedded systems are silently spreading into every corner of our technological society and being interconnected in a world-wide network, they become invaluable to our daily lives, therefore our concern in their correct and secure behavior increases. However, improving dependability and security of embedded devices must be done with special care, since they are usually developed under severe resource- and price-constraints.
In the frame of Critical Step project some FPGA-based embedded systems have been developed, with special concern for dependability and security improvement. Due to resource constraints of these devices, some particular challenges were faced, that are described in this paper: when to employ security during system lifecycle; which security threats to deal with; how to cope with computational-demanding cryptography; how to deal with security in safety-critical systems; how to increase dependability; how to assure timeliness; and how to evaluate dependability.


In this paper we present Chkpt2Chkpt, a desktop grid
system that aims to reduce turnaround times of applications
by replicating checkpoints. We target desktop computing 
projects with applications that are comprised of long-running 
independent tasks, executed in hundreds or thousands of 
computers spread over the Internet. While these
applications typically do local checkpointing to deal with
failures, we propose to replicate those checkpoints in 
remote places to make them available to other worker nodes.

The main idea is to organize the worker nodes of a desktop
grid into a peer-to-peer Distributed Hash Table. Worker
nodes can take advantage of this P2P network to keep track,
share,manage and reclaim the space of the checkpoint files.
We used simulation to validate our system and we show that
remotely storing replicas of checkpoints can considerably
reduce the turnaround times of the tasks, when compared to
the traditional approaches where nodes manage their own
checkpoints locally. These results make us conclude that the
application of P2P techniques seems to be quite helpful in
wide-scaledesktop grid environments.
This paper describes a study of predicting machine availabilities and user presence in a pool of desktop computers. The study is based on historical traces collected from 32 machines, and shows that robust prediction accuracy can be achieved even in this highly volatile environment. The employed methods include a multitude of classification methods known from data mining, such as Bayesian methods and Support Vector Machines. Further contribution is a time series framework used in the study which automates correlations search and attribute selection, and allows for easy reconfiguration and efficient prediction. The results illustrate the utility of prediction techniques in highly dynamic computing environments. Potential applications for proactive management of desktop pools are discussed.
In this paper, we present a checkpoint-based scheme to improve the turnaround time of bag-of-tasks applications executed on institutional desktop grids. We propose to share checkpoints among desktop machines in order to reduce the negative impact of resource volatility. Several scheduling policies are evaluated in our study: FCFS, adaptive timeouts, simple replication, replication with checkpoint on demand, and prediction-based checkpointing combined with replication.

We used a set of real traces collected from an academic desktop grid environment to perform trace-driven simulations of the proposed scheduling algorithms. The results show that using a shared checkpoint approach may considerably reduce the turnaround time of the applications when compared to the private checkpoints methodology.




Desktop grids use the free resources in Intranet and Inter- 
net environments for large-scale computation and storage. While desktop 
grids offer a high return on investment, one critical issue is the validation 
of results returned by participating hosts. Several mechanisms for result 
validation have been previously proposed. However, the characterization 
of errors is poorly understood. To study error rates, we implemented 
and deployed a desktop grid application across several thousand hosts 
distributed over the Internet. We then analyzed the results to give quan- 
titative and empirical characterization of errors stemming from input or 
output (I/O) failures. We find that in practice, error rates are widespread 
across hosts but occur relatively infrequently. Moreover, we find that er- 
ror rates tend to not be stationary over time nor correlated between 
hosts. In light of these characterization results, we evaluated state-of- 
the-art error detection mechanisms and describe the trade-offs for using 
each mechanism.
The Radio Network Design (RND) constitutes an important class of problems, particularly in the planning of wireless communication networks. RND problems are challenging to tackle since they fall in the NP-hard class of optimisation problems. In this paper, we assess the viability of adapting the Differential Evolution (DE) algorithm to a wide-scale real world RND problem. To fulfil the high computational demands of the DE approach, we resort to a pool of more than 150 non-dedicated machines, whose CPU cycles are scavenged through a high throughput system. Our results show that DE is a viable approach for RND problems if proper computing power is available.
Desktop grids use the free resources in Intranet and Internet environments 
for large-scale computation and storage. While desktop grids offer tremendous 
computational power and a high return on investment, one critical issue is the 
validation of results returned by participating hosts that are volatile, anonymous, 
and potentially malicious. We conduct a bene&#64257;t analysis of a mechanism for 
result validation that we proposed recently for the detection of errors in long- 
running applications. This mechanism is based on using the digest of intermediate 
checkpoints, and we show in theory and simulation that the relative bene&#64257;t of 
this method compared to the state-of-the-art is as high as 45%.
Checkpoints that store intermediate results of computation have a fundamental 
impact on the computing throughput of Desktop Grid systems, like BOINC. 
Currently, BOINC workers store their checkpoints locally. A major limitation 
of this approach is that whenever a worker leaves un&#64257;nished computation, no 
other worker can proceed from the last stable checkpoint. This forces tasks to be 
restarted from scratch when the original machine is no longer available. 

To overcome this limitation, we propose to share checkpoints between nodes. 
To organize this mechanism, we arrange nodes to form complete graphs (cliques), 
where nodes share all the checkpoints they compute. Cliques function as sur- 
vivable units, where checkpoints and tasks are not lost as long as one of the 
nodes of the clique remains alive. To simplify construction and maintenance of 
the cliques, we take advantage of the central supervisor of BOINC. To evaluate 
our solution, we combine simulation with some real data to answer the most 
fundamental question: what do we need to pay for increased throughput?
Desktop Grid systems reached a preeminent place 
among the most powerful computing platforms in the 
planet. Unfortunately, they are extremely vulnerable to mis- 
chief, because volunteers can output bad results, for reasons 
ranging from faulty hardware (like over-clocked CPUs) to 
intentional sabotage. To mitigate this problem, Desktop 
Grid projects replicate work units and apply majority vot- 
ing, typically on 2 or 3 results. 

In this paper, we observe that this form of replication is 
powerless against malicious volunteers that have the inten- 
tion and the (simple) means to ruin the project using some 
form of collusion. We argue that each work unit needs at 
least 3 voters and that voting pools with con&#64258;icts enable the 
master to spot colluding malicious nodes. Hence, we post- 
process the voting pools in two steps: i) we use a statistical 
approach to identify nodes that were not colluding, but sub- 
mitted bad results; ii) we use a rather simple principle to 
go after malicious nodes which acted together: they might 
have won con&#64258;icting voting pools against nodes that were 
not identi&#64257;ed in step i. We use simulation to show that our 
heuristic can be quite effective against colluding nodes, in 
scenarios where honest nodes form a majority.


We experimentally evaluate the performance overhead of the virtual environments VMware Player, QEMU, VirtualPC and VirtualBox on a dual-core machine. Firstly, we assess the performance of a Linux guest OS running on a virtual machine by separately benchmarking the CPU, file I/O and the network bandwidth. These values are compared to the performance achieved when applications are run on a Linux OS directly over the physical machine. Secondly, we measure the impact that a virtual machine running a volunteer @home project worker causes on a host OS. Results show that performance attainable on virtual machines depends simultaneously on the virtual machine software and on the application type, with CPU-bound applications much less impacted than IO-bound ones. Additionally, the performance impact on the host OS caused by a virtual machine using all the virtual CPU, ranges from 10\% to 35\%, depending on the virtual environment.
EDGeS is an European funded Framework Program 7 project that aims to connect desktop and service grids together. While in a desktop grid, personal computers pull jobs when they are idle, in service grids there is a scheduler that pushes jobs to available resources. The work in EDGeS goes well beyond conceptual solutions to bridge these grids together: it reaches as far as actual implementation, standardization, deployment, application porting and training.

One of the work packages of this project concerns monitoring the overall EDGeS infrastructure. Currently, this infrastructure includes two types of desktop grids, BOINC and XtremWeb, the EGEE service grid, and a couple of bridges to connect them. In this paper, we describe the monitoring effort in EDGeS: our technical approaches, the goals we achieved, and the plans for future work.
It is a well-known fact that most of the computing power spread over the Internet simply goes unused, with CPU and other resources sitting idle most of the time: on average less than 5% of the CPU time is effectively used. Desktop grids are software infrastructures that aim to exploit the otherwise idle processing power, making it available to users that require computational resources to solve long-running applications. The outcome of some efforts to harness idle machines can be seen in public projects such as SETI@home and Folding@home that boost impressive performance figures, in the order of several hundreds of TFLOPS each. At the same time, many institutions, both academic and corporate, run their own desktop grid platforms. However, while desktop grids provide free computing power, they need to face important issues like fault tolerance and security, two of the main problems that harden the widespread use of desktop grid computing. In this thesis, we aim to exploit a set of fault tolerance techniques, such as checkpointing and redundant executions, to promote faster turnaround times. We start with an experimental study, where we analyze the availability of the computing resources of an academic institution. We then focus on the benefits of sharing checkpoints in both institutional and wide-scale environments. We also explore hybrid schemes, where the traditional centralized desktop grid organization is complemented with peer-to-peer resources. Another major issue regarding desktop grids is related with the level of trust that can be achieved relatively to the volunteered hosts that carry out the executions. We propose and explore several mechanisms aimed at reducing the waste of computational resources needed to detect incorrect computations. For this purpose, we detail a checkpoint-based scheme for early detection of errors. We also propose and analyze an invitation-based strategy coupled to a credit rewarding scheme, to allow the enrollment and filtering of more trustworthy and more motivated resource donors. To summarize, we propose and study several fault tolerance methodologies oriented toward a more efficient usage of resources, resorting to techniques such as checkpointing, replication and sabotage tolerance to fasten and to make more reliable executions that are carried over desktop grid resources. The usage of techniques like these ones will be of ultimate importance for the wider deployment of applications over desktop grids.
Desktop Grid systems reached a preeminent place among the most powerful computing platforms in the planet. Unfortunately, they are extremely vulnerable to mischief, because computing projects exert no administrative or technical control on volunteers. These can very easily output bad results, due to software or hardware glitches (resulting from over-clocking for instance), to get unfair computational credit, or simply to ruin the project. To mitigate this problem, Desktop Grid servers replicate work units and apply majority voting, typically on 2 or 3 results. 
In this paper, we observe that simple majority voting is powerless against malicious volunteers that collude to attack the project. We argue that to identify this type of attack and to spot colluding nodes, each work unit needs \emph{at least} 3 voters. In addition, we propose to post-process the voting pools in two steps. $i$) In the first step, we use a statistical approach to identify nodes that were not colluding, but submitted bad results; $ii$) then, we use a rather simple principle to go after malicious nodes which acted together: they might have won conflicting voting pools against nodes that were not identified in step $i$. We use simulation to show that our heuristic can be quite effective against colluding nodes, in scenarios where honest nodes form a majority.

BOINC is a client-server desktop grid middleware that has grown to power very large computational projects. BOINC clients request computing jobs to a central server and run them alongside other regular applications. Unfortunately, this kind of execution causes two kinds of problems. Firstly, developers must port their application to every single operating system target, which usually means maintaining several different versions of the same application. Secondly, any application running natively on desktop grid hardware is a potential security threat to the volunteer client.\\
During the course of this research we sought an efficient and generic method for alternative execution of jobs in BOINC clients. Our approach is strongly guided by the principles of non-intrusiveness and contains two main components. The first is a library, libboincexec, which is able to control several virtual machines monitors. The second is a modified BOINC wrapper that provides the glue between libboincexec and the middleware.\\
Through the use of this solution we are able to effectively use virtual machines to perform computation on desktop grids. This computation is inherently safe because virtual machines provide sandboxing. Additionally, by targeting the same virtual operating system, the problem of maintaining different versions of an application does not exist, thereby solving the heterogeneity problem of desktop grid nodes.

As in many other areas of computing, security and dependability are of paramount importance in the desktop grid ecosystem. In such environments, threats exist for both the donated resources and the submitters. On one hand volunteers need protection against code that inadvertently or maliciously might harm their resources. Indeed, a volunteer project disrupting the regular working of the volunteer machines would have serious public relation consequences seriously hindering the public perception of volunteer computing. On the other hand, the behavior and results produced by volunteers need to be properly scrutinized to detect inappropriate actions and/or erroneous results, and thus ensure the dependability of the performed computations.
Therefore, security and dependability are two major goals that need to be carefully considered for a successful desktop grid experience. However, as we shall see in this chapter, security and dependability are complex issues, with many desktop grid frameworks addressing both issues in a less than perfect way. Thus, the volunteering of resources and submission of tasks still require a fairly amount of implicit trust between donors and harvesters.
This chapter aims to present the state of the art regarding both security and dependability for desktop grids. The first part presents an overview of the existing issues regarding security and dependability of desktop grid computing. The second part provides a formal  approach on several techniques oriented toward the efficient detection of malicious behavior, namely collusion.
Cloud infrastructures play an increasingly important role for telecom operators, because they enable internal consolidation of resources with the corresponding savings in hardware and management costs. However, this same consolidation exposes core services of the infrastructure to very disruptive attacks. This is indeed the case with monitoring, which needs to be dependable and secure to ensure proper operation of large datacenters and cloud infrastructures. We argue that currently existing centralized monitoring approaches (e.g., relying on a single solution provider, using single point of failure components) represent a huge risk, because a single vulnerability may compromise the entire monitoring infrastructure.

In this paper, we describe the TRONE approach to trustworthy monitoring, which relies on multiple components to achieve increased levels of reliance on the monitoring data and hence increased trustworthiness. In particular, we focus on the TRONE framework for event dissemination, on security-oriented diagnosis based on collected events and on fast network adaptation in critical situations based on multi-homing application support.

To validate our work, we will deploy and demonstrate our solutions in a live environment provided by Portugal Telecom.

Although the need for the exactly-once request-response interaction
pattern is ubiquitous in distributed systems, making it work in practice
is anything but simple. Ensuring the at-most-once part of the invocation
is relatively easy. Unfortunately, the same is not true for the at-least-once
guarantee, which depends on the recovery from crashes of the client, the server
and the network. This is what
makes the exactly-once interaction so difficult in practice: client
and server must log their actions into stable storage, and they must
be able to restart the network connections. 

In this paper, we present a middleware that implements the exactly-once request-response pattern, in presence of network and endpoints
crashes. The main contribution of our work is to release the programmer from the complex tasks of recovering from message losses and network crashes.

Execution of critical services traditionally requires multiple distinct replicas, supported by independent network and hardware. To operate properly, these services often depend on the correctness of a fraction of replicas, usually over $2/3$ or $1/2$. Defying the ideal situation, economical reasons may tempt users to replicate critical services onto a single multi-tenant cloud infrastructure. Since this may expose users to correlated failures, we assess the risks for two kinds of majorities: a conventional one, related to the number of replicas, regardless of the machines where they run; and a second one, related to the physical machines where the replicas run. This latter case may exist in multi-tenant virtualized environments only. We evaluate crash-stop and Byzantine faults that may affect virtual machines or physical machines. Contrary to what one might expect, we conclude that replicas do not need to be evenly distributed by a fixed number of physical machines. On the contrary, we found cases where they should be as unbalanced as possible. We try to systematically identify the best defense for each kind of fault and majority to conserve.


Cloud computing is increasingly important, with the industry moving towards outsourcing computational resources as a means to reduce investment and management costs, while improving security, dependability and performance. Cloud operators use multi-tenancy, by grouping virtual machines (VMs) into a few physical machines (PMs), to pool computing resources, thus offering elasticity to clients. Although cloud-based fault tolerance schemes impose communication and synchronization overheads, the cloud offers excellent facilities for critical applications, as it can host varying numbers of replicas in independent resources. Given these contradictory forces, determining whether the cloud can host elastic critical services is a major research question. We address this challenge from the perspective of a standard three-tiered system with relational data.  We propose to tolerate Byzantine faults using groups of replicas placed on distinct physical machines, as a means to avoid exposing applications to correlated failures. To improve the scalability of our system, we divide data to enable parallel accesses. Using a realistic setup, this setting can reach speedups largely exceeding the number of partitions. Even for a wide variation of the load, the system preserves latency and throughput within reasonable bounds. We believe that the elasticity we observe demonstrates the feasibility of tolerating Byzantine faults in a cloud-based server using a relational database.

Execution of critical services traditionally requires multiple distinct replicas, supported by independent networks and hardware. To operate properly, these services often depend on the correctness of a fraction of replicas, usually over $2/3$ or $1/2$. Defying the ideal situation, economical reasons may tempt users to replicate critical services onto a single multi-tenant cloud infrastructure. Since this may expose users to correlated failures, we assess the risks for two kinds of majorities: a conventional one, related to the number of replicas, regardless of the machines where they run; and a second one, related to the physical machines where the replicas run. This latter case may exist in multi-tenant virtualized environments only.

 To assess these risks, under crash and Byzantine failures of virtual and physical machines, we resort to theoretical and experimental evaluation. Contrary to what one might expect, we conclude that it is not always favorable to distribute replicas evenly over a fixed number of physical machines. On the contrary, we found cases where they should be as unbalanced as possible. We systematically identify the best defense for each kind of failure and majority to preserve. We then review the most common real-life attacks on clouds and discuss the \emph{a priori} placement of service replicas that minimizes the effects of these attacks.




Os actuais sistemas de informaÃ§Ã£o necessitam de ferramentas capazes de registar os diversos problemas ocorridos no sistema, coordenar os vÃ¡rios esforÃ§os de resoluÃ§Ã£o desses problemas, e armazenar a experiÃªncia adquirida com a sua resoluÃ§Ã£o. Essas aplicaÃ§Ãµes sÃ£o tradicionalmente designadas por Sistemas de Acompanhamento de Problemas ou Trouble Ticket Systems (TTS). Basicamente, pede-se aos TTS que
funcionem como mapas hospitalares, 'coordenando o trabalho de vÃ¡rias entidades
(pessoas/grupos/organizaÃ§Ãµes) envolvidas em cada problema e funcionando como bases de conhecimento de problemas passadosâ?.
Existe neste momento um grande nÃºmero de ferramentas enquadrÃ¡veis nesta definiÃ§Ã£o de TTS. Estas sÃ£o habitualmente desenvolvidas para ambientes especÃ­ficos (desenvolvimento de projectos, administraÃ§Ã£o de hosts, manutenÃ§Ã£o de queixas, etc.) tendo acoplados, por essa razÃ£o, um modelo de informaÃ§Ã£o e um modelo administrativo que, porque demasiado especÃ­ficos, se mostram pouco flexÃ­veis noutros ambientes. Talvez por isso elas ainda nÃ£o consigam resultados completamente satisfatÃ³rios na Ã¡rea de gestÃ£o de sistemas e redes,
apesar dos bons nÃ­veis de desempenho obtidos em outros ambientes. Entre as razÃµes para esta situaÃ§Ã£o, destacam-se a grande heterogeneidade das redes de dados, a sua disseminaÃ§Ã£o geogrÃ¡fica (caracterÃ­stica predominante em organizaÃ§Ãµes de mÃ©dia e grande dimensÃ£o), o carÃ¡cter multi-organizaÃ§Ã£o de certas redes de Ã¡rea alargada (com diversos fornecedores de serviÃ§os de comunicaÃ§Ãµes entre os dois extremos da rede), e a decomposiÃ§Ã£o hierÃ¡rquica ou funcional das tarefas e responsabilidades de gestÃ£o.
Este artigo apresenta os principais resultados de um projecto desenvolvido na Universidade de Coimbra com o objectivo de inverter tal situaÃ§Ã£o. O referido projecto envolveu a concepÃ§Ã£o e implementaÃ§Ã£o do NetTrouble: um TTS que integra conceitos e caracterÃ­sticas inovadoras, designadamente o conceito de domÃ­nio administrativo (como resposta ao carÃ¡cter multi-organizaÃ§Ã£o do sistema), a autonomia na definiÃ§Ã£o de polÃ­ticas de cooperaÃ§Ã£o entre quaisquer pares de domÃ­nios, um modelo administrativo flexÃ­vel (com
granularidade atÃ© ao nÃ­vel do problema, com a possibilidade de envolver recursos humanos locais e remotos),
e a virtualizaÃ§Ã£o no TTS local de problemas remotos em que o domÃ­nio Ã© parte interessada. Esta virtualizaÃ§Ã£o permite o acesso descentralizado Ã  informaÃ§Ã£o e a capacidade de aceder a um TTS especÃ­fico a partir da rede, aumentando a acessibilidade da informaÃ§Ã£o e possibilitando a interacÃ§Ã£o com eventuais aplicaÃ§Ãµes remotas.
Merece ainda destaque o esquema de notificaÃ§Ãµes desenvolvido, em que ao registo de cada problema (ticket) surgem associados a noÃ§Ã£o de timeout e um mecanismo de comunicaÃ§Ã£o por e-mail mais sofisticado que o habitual.
No Ã¢mbito do Projecto JAMES foi desenvolvida na Universidade de Coimbra uma infra-estrutura para Agentes MÃ³veis explicitamente orientada para a GestÃ£o de Sistemas e
Redes.
Neste artigo serÃ¡ apresentada a forma como a arquitectura SNMP foi integrada e implementada na plataforma do Projecto JAMES, de modo a que os Agentes MÃ³veis possam  aceder a 'recursos SNMPâ? e constituir, eles prÃ³prios, serviÃ§os de gestÃ£o com interface SNMP.

This paper presents a preliminary overview of a research project called JAMES where we exploit the use of mobile agents technology in the management of telecommunication
networks. The project is partially funded by Agência de Inovação, and was accepted as en Eureka Project (S!1921). The project partners are Siemens SA, University of Coimbra
and Siemens AG.
This paper presents an overview of JAMES, a Java-based platform of mobile agents that is mainly oriented for the management of data and telecommunication networks. This platform has been developed on behalf of a Eureka Project (Eureka!1921) and the project partners are Siemens SA, University of Coimbra and Siemens AG. We describe the main architecture of the platform giving more emphasis to the most important features. To show the efectiveness of some of the techniques that have been implemented we will present some performance results that compare the JAMES platform with the Aglets Workbench.
The main target of our platform is network management and telecommunication applications. In this line, we have done a Java-based implementation of SNMP that has been integrated within the platform. The industrial partners of our project (i.e. Siemens S.A.) have developed a prototype application for TMN performance management. Although it
is still a prototype it is being used to validate the technological advantages of using mobile agents in the management of telecommunication networks.
Mobile Code is an emerging paradigm that is gaining mo mentum in several felds of application. Network Management is a potential area for the use of this technology, provided it will be able to interoperate with well established solutions for Network Management.
This paper presents the integration a classic NM protocol, like SNMP, into a platform of Mobile Agents. Our platform, called JAMES, has been developed in the context of an Eureka Project (Eureka!1921) where the project partners are University of Coimbra, Siemens SA and Siemens AG. Since the main target of the platform is network management, it includes a set of SNMP services allowing mobile agents to easily interface with SNMP agents, as well as with legacy SNMP-based management applications.
In the paper we present a brief overview of the general architecture of the platform and we describe in some detail the framework we used to provide for integration between mobile agent applications and SNMP.
In this paper we present results from an experimental study addressing the use of
mobile agents in the retrieval of management information. We compare several
agent-based models for distributed collection and processing of management data,
focusing on performance but also considering network traffic and setup costs. This
study reveals that in many situations the performance of mobile agent systems is
mainly determined by distribution, rather than locality. Furthermore, it shows that
despite its importance in the flexibility and adaptability of the management system,
agent mobility does not increase the system performance.
Neste artigo é apresentado um estudo experimental onde se compara o comportamento
de diversos modelos de distribuição, baseados em agentes móveis, em operações de
recolha e processamento de informação de gestão dispersa para rede. O desempenho
constitui a principal métrica deste estudo, mas são também considerados outros factores,
tais como o tráfego de rede e os custos de arranque associados a cada modelo. Este
estudo mostra que em muitas situações o desempenho dos sistemas de agentes móveis é
determinado essencialmente pela distribuição do processamento, e não pela localização
dos agentes. O estudo mostra também que em geral os modelos migratórios penalizam
significativamente o desempenho do sistema, ainda que possam ser relevantes noutras
vertentes, tais como a flexibilidade e adaptabilidade das aplicação de gestão.
Mobile Agent Technology (MAT) is a fresh paradigm for distributed programming, with potential for application in a broad range of fields. A Mobile Agent (MA) corresponds to a small program that is able to migrate to some remote machine, where it is able to execute some function or collect some relevant data and then migrate to other machines in order to accomplish another task. The basic idea of this
paradigm is to distribute the processing throughout the network: that is, send the code to the data instead of bringing the data to the code. MA systems differ from other mobile code and agent-based technologies because increased code and state mobility allow for even more flexible and dynamic solutions.
In the last couple of years we have developed the JAMES platform for mobile agents [5], which was later used by our industrial partners to produce and deploy several MA-based applications for telecommunications and network management. These circumstances provided us with a good perspective on infrastructure manageability and lead to the development of several services for better administration of the JAMES platform. This paper presents those services and discusses their impact on the reduction of the associated management costs.
Network management is often considered as one of the application areas with greatest potential for Mobile Agent (MA) technology. However, legacy applications and architectures impose considerable inertia to the deployment of new solutions for network management. For this reason, successful MA infrastructures will need flexible and effortless integration with legacy management frameworks, like the widespread SNMP architecture.
This paper presents a set of solutions for the integration of SNMP into MA platforms, to support access to SNMP management resources and to introduce MA-based management services that can be reachable from legacy SNMP-capable applications. Such solutions were successfully applied to the JAMES platform, a joint project from University of Coimbra, Siemens Portugal and Siemens AG.

Neste artigo será apresentada a ferramenta J.AgentX, a primeira
implementação em Java da norma AgentX para extensão dinâmica de
agentes SNMP.
Para além das características mais interessantes desta ferramenta para
desenvolvimento rápido de novos serviços de gestão serão descritas duas
experiências concretas da sua aplicação: a instalação de serviços de gestão
baseados em código móvel e o desenvolvimento de proxies.
This paper presents J.AgentX, a Java-based toolkit for dynamic extension of SNMP
agents. This toolkit provides an easy way to dynamically supply SNMP-based interfaces to new management services, reducing the costs associated with the deployment of these new services.
J.AgentX is fully compliant with the recent AgentX standard (thus assuring interoperability) but goes one step further by including several features for faster service development, like a powerful high-level interface, simultaneous support for different communication mechanisms, a small footprint and portability.
These features were successfully employed to provide SNMP-compliant network management services based on flexible and innovative delegation topologies. Two of those experiences will also be presented in this paper. In the first one J.AgentX provides an SNMP interface to management services based on mobile code. In the second experience J.AgentX is used to develop transparent proxies for legacy monolithic services.





























In this paper, we discuss how a Content Distribution Network (CDN) can be applied to Desktop Grid computing environments, particularly to the BOINC platform. When using multiple data servers, BOINC's current data distribution mechanisms are rudimentary and do not take into account server load and availability. We propose the use of a customCDNthat applies client redirection, health monitoring and content distribution techniques. A more efficient distribution of
requests would improve data transfers, making it possible to explore applications with larger data sets.
Performance management and dependability are two of the fundamental issues in business-critical applications. The ability to detect the occurrence of performance failures
and anomalies has raised the attention of researchers in the last years. It is in fact a dif?cult problem, since a visible change in the performance can result from some natural cause (e.g., workload variations, upgrades) or by some internal anomaly or fault that may end up in a performance failure or application crash.

Distinguish between the two scenarios is the goal of  the framework presented in this paper. Our framework is targeted for web-based and component-based  applications.  It  makes use of AOP-based monitoring, data correlation techniques and time-series alignment  algorithms to spot the occurrence of performance anomalies avoiding false alarms due to workload variations. The paper includes some experimental results that show the effectiveness of our techniques under the occurrence of dynamic workloads and some fault-load situations.
The  topic  of  this  paper  is  about  prediction  of performance  anomalies  caused  by  software  aging.  We have developed a framework for detection of performance anomalies that is targeted to web and component-based applications. In  this  study, we  selected  some  amount  of  historical  data previously collected and we conducted a correlation analysis with  this  data.  The  resulting  dataset  was  then  submitted to  some  Machine-Learning  (ML)  classification  algorithms. The best algorithms were selected according to the accuracy and precision.  In  a  second  step,  we  induced  some  synthetic aging  scenarios  (memory  leaks  and  CPU  contention)  in  the application  and  we  tried  to  do  estimation  of  the  system parameters by using time-series analysis. With the estimated values we conducted a classification with the three previous ML algorithms. From the initial results we observed that combining the estimation of parameters supported by time-series models with ML classification techniques provides some good results on the prediction of performance anomalies. We also observed that there is no single ML algorithm that can be applied effectively to predict the response time for all the web-transactions.

The complexity behind current business-critical applications leads many times to performance problems difficult to anticipate and analyze. In our previous work we described a framework for detection of performance anomalies in web-based and component-based applications. It provides low overhead monitoring, correctly distinguishes performance anomalies from common workload variations and also presents initial information for system or application server changes related with an application performance anomaly.

In this paper we present a framework extension devised to offer root-cause failure analysis for a given performance anomaly. The monitoring module enables application profiling and ANOVA analysis is used to verify if a performance anomaly is due to internal changes within the application (e.g., application updates) or to external changes (e.g., remote services changes, system/application server change). The paper includes some experimental results that show the effectiveness of our approach to pinpoint the root-cause for different types of performance anomalies and remarks its potential to avoid a considerable number of service failures.

http://dl.acm.org/authorize?17962
The most important factor in the assessment of the availability of a system is the mean-time to repair (MTTR). The lower the MTTR the higher the availability. A significant portion of the MTTR is spent in the detection and localization of the cause of the failure. One possible method that may provide good results in the root-cause analysis of application failures is run-time profiling. The major drawback of run-time profiling is the performance impact.

In this paper we describe two algorithms for selective and adaptive profiling of web-based applications. The algorithms make use of a dynamic profiling interval and are mainly triggered when some of the transactions start presenting some symptoms of performance anomaly. The algorithms were tested under different types of degradation scenarios and compared to static sampling strategies. We observed through experimentation that the pinpoint of performance anomalies, supported by the data collected using the adaptive profiling algorithms, stills timely as with full-profiling while the response time overhead is reduced in almost 60%. When compared to a non-profiled version the response time overhead is less than 1.5%. These results show the viability of using run-time profiling to support quickly detection and pinpointing of performance anomalies and enable timely recovery.
Video-streaming services are nowadays one of the biggest contributors to Internet traffic. Due to the real-time characteristic of video-streaming, very low failure detection and recovery delays are required. Proactive recovery in face of performance degradation is a prominent area to explore. Current performance analysis work in video-streaming focuses mostly on capacity planning and session admission through complex workload and resource modeling. These models have limited application when degradation is not explained by client workload (e.g., dynamic resource reallocation, software faults and misconfiguration). We explore server-side monitoring of performance degradations in video-streaming servers, based on statistical analysis of server metrics and client-server interaction messages. Statistical analysis of event logs show that analyzed metrics can be used as symptoms of failures to anticipate them and thus enabling proactive recovery. Exception is server overloading caused by streaming of unpopular videos, which are exposed by metrics when QoS degradation is close to accepted quality thresholds.
Video-streaming services are dominating the In- ternet, delivering content for video-on-demand, TV, education and collaborative work. Service parameters addressing quality and continuity of video content have a special importance due to the human sensitiveness to variations on video quality and decades of quality patterns absorbed by traditional TV users. Thus, the performance analysis and repair lifecycle at server and network levels is mandatory to avoid degradation of user experience. At the network level, there are several effective techniques based on temporal and spatial data redundancy, though they deeply depend on healthy servers with enough resources to afford both the client and recovery workloads. Excess of streaming workloads and performance anomalies (i.e., server resources exhaustion not explained by client requests) are typical causes of server performance failures. The former is often caused by memory caching of popular videos, which impacts the number of requests accepted by the server and consequently blurs load admittance mechanisms when the workload changes. The latter is caused by server internal factors independent of client workloads (e.g., memory leaks and maintenance activities). Separating client workload related failures from performance anomalies is mandatory for selection of immediate repair actions, capacity planning and to support fault repair. We evaluated the performance of Naive Bayes and C4.5 Trees algorithms for classification of these failure states using client and server performance metrics. Results shown that it is possible to predict the type of failure with levels of recall and accuracy higher than 90% for workload types with different popularity levels.
Streaming media is now one of the killer applications on the Internet. Availability in streaming services is a critical concern, as consumer expectations are drawn around decades of traditional TV experience. Server performance has particular importance in streaming, as its sensitiveness to delays makes it vulnerable to performance anomalies. Current work on server- level performance analysis fails to cope with performance failures not explained by the workload. We propose a self-healing architecture for streaming servers sustained by a biological metaphor of heart that explores proactive server recovery by anticipating performance failures through detection of arrhythmias (transmission delays of streaming content) and session probing. We evaluated the approach in RTSP streaming through experimental work in several resource exhaustion scenarios. Results have shown that our approach is able to predict and localize service failures several seconds before their occurrence for most failure scenarios.
Failure detection in Web Applications is a topic of paramount importance. There are several papers in the literature about end-to-end monitoring of web-sites. Most of those systems use a distributed set of probes, located in different places in the Internet, and conduct some remote checking of the main HTML pages of the web-sites. This approach seems viable and many web-sites are already using some commercial services that provide this type of monitoring. In this paper we present new insights about the nature of user-visible failures in web-sites. We conducted an experimental study in the Internet by monitoring a set of 20 well-known web-sites during a one-month period. The results achieved are interesting for the whole community that is studying the reliability of the Internet and Internet Sites.
The web-based applications are exposed to a large spectrum of factors that may affect their availability and per- formability. The mean-time-to-detect (MTTD) and the mean- time-to-repair (MTTR) are considered of utmost importance to reduce the failure impacts. In this context, the combination of multiple monitoring techniques is commonly adopted to provide IT staff with information useful for timely detection and recovery from the failures.

In this paper we provide an experimental study about the de- tection abilities provided by the monitoring tools that are being used nowadays in web-based applications. Besides the system- level, end-to-end and container-level monitoring techniques we incorporate an application-level monitoring technique. This technique provides the detection of performance anomalies by performing a correlation analysis among application param- eters collected by an aspect-oriented program. The detection latency, the number of end-users affected, the coverage analysis and the overhead achieved by each monitoring technique, was evaluated considering different anomaly scenarios. Despite the importance of the monitoring techniques complementarity, the results achieved by the application-level monitoring are very interesting: it has detected 100% of the anomaly scenarios tested; for 73% of the anomalies it was the fastest detection technique; and due to the low detection latency it contributes to reduce the number of end-users experiencing the anomalies.
In this paper we study the performance impact induced by different application-level monitoring tools, targeted for the detection of performance anomalies in Web-based applications. Adaptive and selective algorithms, able to self- adapt the monitoring behavior, are proposed to minimize the performance impact induced by application-level profiling. From the experimental results, becomes clear the usefulness of adaptive and selective monitoring: depending on the system load, the response time latency induced has varied between 0.5 and 14 milliseconds per request; the throughput penalty was inferior to 1%; and the ability to detect and pinpoint the anomalies was not compromised. These outcomes are very favorable to the adoption of application-level profiling and runtime analysis, as a way to detect, pinpoint and repair from anomalies in production systems.
In this paper, we describe the SHõWA framework and evaluate its ability to recover from performance anomalies in Web-based applications. SHõWA is meant to automatically detect and recover from performance anomalies, without calling for human intervention. It does not require manual changes to the application source code or previous knowledge about its implementation details. The application is monitored at runtime and the anomalies are detected and pinpointed by means of correlation analysis. A recovery procedure is performed every time an anomaly is detected. An experimental study was conducted to evaluate the recovery process included in the SHõWA framework. The experimental environment considers a benchmarking application, installed in a high-availability system. The results show that SHõWA is able to detect and recover from different anomaly scenarios, before any visible error, higher-latency or work-in-progress loss is observed. It proved to be efficient in terms of time of repair. The performance impact induced on the managed system was low: the response time penalty per request varied between 0 and 2.21 milliseconds, the throughput was affected in less than 1%.
The adaptation of a cloud infrastructure is an ongoing process. Cloud adaptation aims to provide the cloud infrastructure with the necessary computational resources to meet the agreed SLAs and, simultaneously, optimize the resources usage. In a cloud, the consumers are typically limited to the SLAs defined in advance with the cloud service provider. This creates a strong dependence in the cloud provider, and gives little room for maneuver when the cloud customers need to adapt the infrastructure very quickly to avoid service degradations. 

In this paper we present a framework that aims to reduce this gap. The SHõWA framework is targeted for self-healing Web-based applications. It detects workload and performance anomalies from the consumer perspective and interacts with the cloud service provider to dynamically adjust the infrastructure. From the experimental study conducted, is noteworthy the role of SHõWA to avoid the degradation of service upon the occurrence of load and resource contention scenarios.
The complexity of systems is considered an obstacle to the progress of the IT industry. Autonomic computing is presented as the alternative to cope with the growing complexity. It is a holistic approach, in which the systems are able to configure, heal, optimize, and protect by themselves. Web-based applications are an example of systems where the complexity is high. The number of components, their interoperability, and workload variations are factors that may lead to performance failures or unavailability scenarios. The occurrence of these scenarios affects the revenue and reputation of businesses that rely on these types of applications.

In this article, we present a self-healing framework for Web-based applications (SHõWA). SHõWA is composed by several modules, which monitor the application, analyze the data to detect and pinpoint anomalies, and execute recovery actions autonomously. The monitoring is done by a small aspect-oriented programming agent. This agent does not require changes to the application source code and includes adaptive and selective algorithms to regulate the level of monitoring. The anomalies are detected and pinpointed by means of statistical correlation. The data analysis detects changes in the server response time and analyzes if those changes are correlated with the workload or are due to a performance anomaly. In the presence of performance anomalies, the data analysis pinpoints the anomaly. Upon the pinpointing of anomalies, SHõWA executes a recovery procedure. We also present a study about the detection and localization of anomalies, the accuracy of the data analysis, and the performance impact induced by SHõWA. Two benchmarking applications, exercised through dynamic workloads, and different types of anomaly were considered in the study. The results reveal that (1) the capacity of SHõWA to detect and pinpoint anomalies while the number of end users affected is low; (2) SHõWA was able to detect anomalies without raising any false alarm; and (3) SHõWA does not induce a significant performance overhead (throughput was affected in less than 1%, and the response time delay was no more than 2 milliseconds).
This dissertation focuses on Text Databases, which has grown in interest and relevance in the last years. In fact, text databases are now particularly relevant in several contexts and application areas such as administration of organization documents, digital libraries, creation of automated dictionaries and encyclopedias, and in any problem dealing with the storage and retrieval of textual information. These databases manage efficiently the textual information through the use of the Relational Databases technologies in combination with the indexing and searching techniques of the Information Retrieval. However, the integration of the information retrieval techniques in the database systems has not been easy and several approaches have emerged for this problem. 
This work presents a survey on text databases and discusses the fundamental theoretical concepts of databases and information retrieval and the main approaches for the integration of databases systems and information retrieval systems.
The ExpSRI system, specifically developed for the study done in this dissertation, allows the manipulation of textual information stored in a text database of large capacity. This database is managed by a DataBase Management System &#40;DBMS&#41; that also integrates information (textual) retrieval techniques. The experimental setup done around the ExpSRI system holds a large test collection (˜473000 documents) and has been used as a test
bench. As the aim of a retrieval information system is to find the textual information required by the system users, it is important to assess the effectiveness of these systems.
The effectiveness evaluation of the ExpSRI has been based on the experimental evaluation of measures such as precision, recall, precision versus recall and R-Precision. The analysis performed to the results obtained during ExpSRI evaluation permitted the identification of the main factors that can influence the effectiveness of information retrieval systems.
Due to wiring questions, computer integration, or simply as a way to increase the features of audio systems, the
need for an audio networking solution increases every day. However we do not have to reinvent the wheel, it is
possible to use the know-how of decades of computer networking, and use winning technologies such as
Ethernet. With a few adjustments, we can have an audio and midi networking solution, which will achieve our
goals and scale for the future, without hurting our pockets.




As benchmarks de confiabilidade devem incluir injectores de falhas com elevada portabilidade, muito fáceis de instalar e de utilizar, devendo estes, simultaneamente e em conjunto com os outros componentes da benchmark, ser facilmente distribuíveis via web. Tais facilidades não deverão ser, contudo, restritivas no que concerne às capacidades de injecção, devendo esses injectores ser capazes de, em particular, injectar falhas quer no espaço do sistema quer no espaço do utilizador, independentemente da complexidade da aplicação alvo. 
Uma vez que os actuais injectores de falhas não preenchem esses requisitos, neste trabalho é apresentado um novo injector de falhas, denominado DBench-FI, implementado por software, capaz de colmatar essa lacuna. Através de uma metodologia inovadora, baseada num algoritmo de alteração automática do kernel do sistema operativo durante a execução, o DBench-FI é capaz de injectar falhas quer no espaço utilizador quer no espaço do sistema, sendo simultaneamente muito fácil de instalar, não exigindo qualquer conhecimento do código fonte, quer da aplicação alvo, quer do sistema operativo. Complementarmente, é capaz de injectar falhas em aplicações que, independentemente da sua complexidade, já se encontrem em execução na altura da sua instalação e não é baseado em qualquer ferramenta de debug, não apresentando, por isso, os inconvenientes associados a esse tipo de injectores.
Presentemente, o DBench-FI encontra-se disponível para o sistema operativo Linux e para os processadores Intel baseados na arquitectura IA-32, podendo, contudo, devido à elevada portabilidade do método em que assenta, ser facilmente adaptado a outros sistemas e processadores. Apesar de, actualmente, injectar somente falhas de memória, está em curso a inclusão de outros modelos de falhas.
De modo a demonstrar as capacidades de injecção de falhas do DBench-FI em aplicações complexas, são apresentadas, neste trabalho, um conjunto de experiências conduzidas utilizando o SGBD Oracle 9i como aplicação alvo.

Desenvolvimento de uma biblioteca de instrumentação de código para a plataforma .NET.
Criação e validação de padrões de instrumentação de código de alto nível.
Multi-core processors are present in everyone’s daily life. Consequently, concurrent programming has re- emerged as a pressing concern for everyone interested in exploring all the potential computational power in these ma- chines. But, the emergence of new concurrency models and programming languages also brings new challenges in terms of how one can deal with abnormal occurrences, much due to the heterogenous parallel control flow. Unexpectedly, sequential Exception Handling models remain as the most used tool for robustness, even in the most recent concurrent programming languages. Though, the appearance of more complex models, such as programming languages with implicit concurrency, might pose a challenge too big for these sequential mechanisms. In this article we will provide evidences why such models are not generally suited to deal with faults in programs with implicit concurrency and, in the light of more recent advances in concurrent Exception Handling, we will discuss the attributes of a model for addressing this problem.
As a consequence of the immense computational power available in GPUs, the usage of these platforms for running data-intensive general purpose programs has been increasing. 
Since memory and processor architectures of CPUs and GPUs are substantially different, programs designed for each platform are also very different and often resort to a very distinct set of algorithms and data structures. But, using the right “gear” is not always a guarantee that programs will execute faster on their targeted platform. Variations in the hardware of the GPU and in the size of data, among others, can have a major influence in performance. 
ÆminiumGPU is a new high-level programming framework for developing and running parallel programs in CPUs and GPUs. ÆminiumGPU programs are written in a high-level general purpose programming language and compiled into hybrid executables which can run in both platforms. Thus, the decision of which platform is going to be used for executing a program is delayed until run-time and automatically performed by the system using Machine-Learning techniques.
Our tests show that ÆminiumGPU is able to achieve speedups up to 65x and that the average accuracy of the platform selection algorithm, in choosing the best platform for executing a program, is above 92\%.


We propose a model for event-oriented programming under shared memory based on access permissions with explicit parallelism. In order to obtain safe parallelism, programmers need to specify the variable permissions of functions. Blocking operations are non existent, and callback-based APIs are used instead, which can be called in parallel for different events as long as the access permissions are guaranteed. This model scales for both IO and CPU-bounded programs.
We have implemented this model in the Eve language, which includes a compiler that generates parallel tasks with synchronization on top of variables, and a work-stealing runtime that uses the epoll interface to manage the event loop.
We have also evaluated that model in micro-benchmarks in programs that are either CPU-intensive or IO-intensive with and without shared data. In CPU-intensive programs, it achieved results very close to multithreaded approaches. In the share-nothing IO-intensive benchmark it outperformed all other solutions. In shared-memory IO-intensive benchmark it outperformed other solutions with a more or equal value of writes than read operations.
There are billions of lines of sequential code inside nowadays software which do not benefit from the parallelism available in modern multicore architectures. Transforming legacy sequential code into a parallel version of the same programs is a complex and cumbersome task. Trying to perform such transformation automatically and without the intervention of a developer has been a striking research objective for a long time. This work proposes an elegant way of achieving such a goal. By targeting a task-based runtime which manages execution using a task dependency graph, we developed a translator for sequential JAVA code which generates a highly parallel version of the same program. The translation process interprets the AST nodes for signatures such as read-write access, execution-flow modifications, among others and generates a set of dependencies between executable tasks. This process has been applied to well known problems, such as the recursive Fibonacci and FFT algorithms, resulting in versions capable of maximizing resource usage. For the case of two CPU bounded applications we were able to obtain 10.97x and 9.0x speedup on a 12 core machine.

Rich Web Clients allow developers to manage data locally and update it from the server by means of asynchronous requests, thus providing more interactive interfaces and an improved user experience. On the other hand, they face concerning challenges regarding error management. When there is a need to update the local data through multiple asynchronous requests and it is required that all them succeed, an error on a single call can lead to having incorrect information shown to the user. Consequently, developers need to explicitly implement proper recovery mechanisms, a task that most of times is complex and highly error prone, leading to tangled code and harder maintenance, especially in an asynchronous environment. These problems could be lessened through automatic error recovery techniques, but the existing state of the art for Rich Web Client development does not support recovery from asynchronous scenarios. To cope with this problem we extended the existing error recovery technique of Reconstructors, adding to it the capability of recovering the state in the presence of several asynchronous requests. We applied this technology in a widely used open source project for rendering interactive charts, ChartJs, thus allowing the developer to effortlessly guarantee that the data shown to the user, even when it results from multiple asynchronous requests, is never inconsistent. We compare our proposal to other solutions using state of the art approaches and verify that by using Reconstructors the overall implementation requires 39.16% less lines of code, and 5.66 times less lines of code are dedicated specifically to error management, while avoiding code tangling completely. Execution time showed by reconstructors is between 5.2% and 9.3% slower than other solutions, a cost we believe is worth its benefits, and feasible for using these techniques in real world client applications.

In this paper we present a general purpose game AI library implementing a modern agent based model. We discuss the conceptual model underlying the library and the technical issues related to its use. We present the principles gathered from the science of complexity and bio-inspired computation that guide the research of what this library is a part of, and several scienti&#64257;c research applications are also addressed. Implications for game design and practical use scenarios are given.
The science of complexity has been gaining momentum. It spans multiple areas of research and, most of them, use computer generated models to try to reproduce and thus study the phenomena. We describe a framework — BitBang — that aims to provide a base system in which to implement these models. This framework is agent-based, with roots in Alife systems. The framework provides a generic model upon which one can implement the desired system. It started as an effort to apply complexity science to modern computer games, and thus it provides an integration with modern 3D and physical engines. This can offer an added realism to the visualisation, but also adds details to the model being studied. As an example of the possible uses of this framework, we present a research scenario that tries to study the emergence of complexity underlying circadian clocks.
The synchronisation phenomena in biological systems is a current and recurring subject of scientific study. This topic, namely that of circadian clocks, served as inspiration to develop an agent-based simulation that serves the main purpose of being a proof-of-concept of the model used in the BitBang framework, that implements a modern autonomous agent model. The experiment described in this paper gets inspiration from a current subject in biology research. Despite having been extensively studied, circadian clocks still have much to be investigated. Rather than wanting to learn more about the internals of this biological process, we look to study the emergence of this kind of adaptation to a daily cycle. To that end we implemented a world with a day / night cycle, and analyse the ways the agents adapt to that cycle. The results show the evolution of the agents' ability to gather food. If we look at the total number of agents over the course of an experiment, we can pinpoint the time when reproductive technology emerges. We also show that the agents adapt to the daily cycle. This circadian rhythm is best observed in a  real-time 3D visualisation of the experiments, but can also be shown by analysing the variation on the agents metabolic rate, which is affected by the variation of their movement patterns. In the experiments conducted we can observe that the metabolic rate of the agents varies according to the daily cycle.
The synchronisation phenomena in biological systems is a current and recurring subject of scientific study. This topic, namely that of circadian clocks, served as inspiration to develop an agent-based simulation that serves the main purpose of being a proof-of-concept of the model used in the BitBang framework, that implements a modern autonomous agent model. Despite having been extensively studied, circadian clocks still have much to be investigated. Rather than wanting to learn more about the internals of this biological process, we look to study the emergence of this kind of adaptation to a daily cycle. To that end we implemented a world with a day/night cycle, and analyse the ways the agents adapt to that cycle. The results show the evolution of the agents' ability to gather food. If we look at the total number of agents over the course of an experiment, we can pinpoint the time when reproductive technology emerges. We also show that the agents adapt to the daily cycle. This circadian rhythm can be shown by analysing the variation on the agents metabolic rate, which is affected by the variation of their movement patterns. In the experiments conducted we can observe that the metabolic rate of the agents varies according to the daily cycle.



Throughout the last decades, Darwin?s theory of natural se- lection has fueled a vast amount of research in the field of computer science, and more specifically in artificial intelligence. The majority of this work has focussed on artificial selection, rather than on natural se- lection. In this paper we study the evolution of agents? controllers in an open-ended scenario. To that end, we set up a multi-agent simulation inspired by the ant foraging task, and evolve the agents? brain (a rule list) without any explicit fitness function. We show that the agents do evolve sustainable foraging behaviors in this environment, and discuss some evolutionary conditions that seem to be important to achieve these results.

Most biological systems have some sort of adaptation to our planet’s cycle of day and night. This adaptation is a current subject of scientific research, and serves as inspiration to de- velop a multi-agent simulation to investigate the evolution of complexity in an open-ended evolutionary framework. In a previous work, we created a simulated world where artificial organisms evolve to synchronize with a daily cycle of light and darkness. A multi-agent, artificial life framework was used to implement these simulations. In this paper, we fur- ther develop that world, by adding caves to the environment. When in these caves, the agents will perceive a low level of light, as if it were night. This adds an extra layer of complex- ity to the desired behavior of the agents, as now they need to distinguish "night" from "cave". Using the same agent structure, and the same open-ended evolution framework, we show that the agents evolve to adapt to this new environment. We also show how the agents adapt to the environment with caves, by analyzing their brains.
A common issue in Artificial Life research, and mainly in open-ended evolution simulations, is that of defining the bootstrap conditions of the simulations. One usual technique employed is the random initialization of individuals at the start of each simulation. However, by using this initialization method, we force the evolutionary process to always start from scratch, and thus require more time to accomplish the objective. Artificial Life simulations, being typically, very time consuming, suffer particularly when applying this method. In a previous paper we described a technique we call step evolution, analogous to incremental evolution techniques, that can be used to shorten the time needed to evolve complex behaviors in open-ended evolutionary simulations. In this paper we further extend this technique by automating the process of stepping the simulation. We provide results from experiments done on an open-ended evolution of foraging scenario, where agents evolve, adapting to a world with a day and night cycle. The results show that we can indeed automate this process and achieve a performance at least as good as on the best performant non-automated version.




A localized Delaunay triangulation owns the following interesting properties for sensor and wireless ad hoc networks: it can be built with localized information, the communication cost imposed by control information is limited, and it supports geographical routing algorithms that offer guaranteed convergence. This paper presents two localized algorithms, fast localized Delaunay triangulation 1 (FLDT1) and fast localized Delaunay triangulation 2 (FLDT2), that build a graph called planar localized Delaunay triangulation, PLDel, known to be a good spanner of the Unit Disk Graph, UDG. Our algorithms improve previous algorithms with similar theoretical bounds in the following aspects: unlike previous work, FLDT1 and FLDT2 build PLDel in a single communication step, maintaining a communication cost of O(n log n), which is within a constant of the optimal. Additionally, we show that FLDT1 is more robust than previous triangulation algorithms, because it does not require the strict UDG connectivity model to work. The small signaling cost of our algorithms allows us to improve routing performance, by efficiently using the PLDel graph instead of sparser graphs, like the Gabriel or the Relative Neighborhood graphs.
In this paper we describe the first steps we gave to construct a prototype of RMrun , a peer-to-peer system where users share programs they have with each other. We enumerate some of the problems we are facing and some directions we are considering to take. RMrun uses standard Unix tools, like the X protocol and Secure Shell (SSH) to run remote programs pretty much as in any Unix system. The difference is that it does not require a particular user account in server hosts. One single account enables the user to run any program available in anonymous computers inside its community.

We think that RMrun can fit two main trends of uses: utilization of programs by peers that do not have access to such programs or do not want to configure them; and utilization of desktop CPU time in public volunteer computer.
Service grids and desktop grids are both promoted by their supportive communities as great solutions for solving the available compute power problem and helping to balance loads across network systems. Little work, however, has been undertaken to blend these two technologies together in an effort to create one vast and seamless pool of resources. In this paper we will introduce a new FP7 infrastructures project, entitled Enabling Desktop Grids for e-Science (EDGeS), that is building technological bridges to facilitate service and desktop grid interoperability. We pro- vide a taxonomy for existing state of the art grid systems and background into service grids, such as EGEE and volunteer computing platforms, such as BOINC and XtremWeb. We then describe our approach within three themes for identifying translation technologies for porting applications between service grids and desktop grids and vice versa. The individual themes discuss the actual bridging technologies employed, the distributed data issues surrounding deployment and application development and user access issues.
Service grids and desktop grids are both promoted by their supportive communities
as great solutions for solving the available compute power problem and
helping to balance loads across network systems. Little work, however, has been
undertaken to blend these two technologies together. In this paper we introduce
a new EU project, that is building technological bridges to facilitate service and
desktop grid interoperability. We provide a taxonomy and background into service
grids, such as EGEE and desktop grids or volunteer computing platforms,
such as BOINC and XtremWeb. We then describe our approach for identifying
translation technologies between service and desktop grids. The individual
themes discuss the actual bridging technologies employed and the distributed
data issues surrounding deployment.
A localized Delaunay triangulation owns the following interesting properties for sensor and wireless ad hoc networks: it can be built with localized information, the communication cost imposed by control information is limited, and it supports geographical routing algorithms that offer guaranteed convergence. This paper presents two localized algorithms, FLDT1 and FLDT2, that build a graph called planar localized Delaunay triangulation, PLDel, known to be a good spanner of the Unit Disk Graph, UDG. Our algorithms improve previous algorithms with similar theoretical bounds in the following aspects: unlike previous work, FLDT1 and FLDT2 build PLDel in a single communication step, maintaining a communication cost of O(n log n), which is within a constant of the optimal. Additionally, we show that FLDT1 is more robust than previous triangulation algorithms, because it does not require the strict UDG connectivity model to work. The small signaling cost of our algorithms allows us to improve routing performance, by effciently using the PLDel graph instead of sparser graphs, like the Gabriel or the Relative Neighborhood graphs.


Graphics processing units (GPUs) have become increasingly popular over the last years as a cost-effective means of accelerating various computationally intensive tasks. We study the particular case of modular exponentiation, the crucial operation behind most modern public-key cryptography algorithms. We focus our attention on the NVIDIA GT200 architecture, currently one of the most popular for general purpose GPU computation.

We report our efforts to run modular exponentiation faster than any other method we were aware of for GPUs. Part of our performance advantage results from a different interleaving of the Montgomery multiplication, which was neglected in previous literature. The other part comes from carefully exploring general techniques, like loop unrolling and inline PTX assembly. Our throughput results, at over 20000 RSA-1024 decryptions per second or 41426 512-bit modular exponentiations per second, present a significant speedup over previous GPU implementations, without any significant latency penalty.

Lastly, we evaluate our results in light of several popular metrics, namely performance/price and performance/watt ratios. We find that, while current GPUs generally perform better than CPUs, they show worse performance/watt ratios.
In this paper we present Tyche, a nonlinear pseudorandom number generator designed for computer simulation. Tyche has a small $128$-bit state and an expected period length of $2^{127}$. Unlike most nonlinear generators, Tyche is consistently fast across architectures, due to its very simple iteration function derived from ChaCha, one of today's fastest stream ciphers.

Tyche is especially amenable for the highly parallel environments we find today, in particular for Graphics Processing Units (GPUs), where it enables a very large number of uncorrelated parallel streams running independently. For example, $2^{16}$ parallel independent streams are expected to generate about $2^{96}$ pseudorandom numbers each, without overlaps.

Additionally, we determine bounds for the period length and parallelism of our generators, and evaluate their statistical quality and performance. We compare Tyche and the variant Tyche-i to the XORWOW and TEA$_8$ generators in CPUs and GPUs. Our comparisons show that Tyche and Tyche-i simultaneously achieve high performance and excellent statistical properties, particularly when compared to other nonlinear generators.


Sparse matrix-vector multiplication dominates the performance of many scientific and industrial problems.  For example, iterative methods for solving linear systems rely on the performance of this critical operation. The particular case of binary matrices shows up in many important areas of computing, such as graph theory and cryptography. Unfortunately, irregular memory access patterns cause poor memory throughput, slowing down this operation.

We transform the matrix into a straight-line program that takes full advantage of the instruction cache. The regular loop-less pattern of the program minimizes cache misses, thus decreasing the latency for most instructions. We focus on the widely used {\tt x86\_64} architecture and on binary matrices, to explore several possible tradeoffs regarding memory access policies and code size. When compared to a Compressed Row Storage (CRS) implementation, we obtain a 20\% performance improvement in a binary sparse matrix with $5426753$ rows and weight $370909586$.
Defending programs against illegitimate use and tampering has become both a field of study and a large industry. Code obfuscation is one of several strategies to stop, or slow down, malicious attackers from gaining knowledge about the internal workings of a program.

Binary code obfuscation tools often come in two (sometimes overlapping) flavors. On the one hand there are ``binary protectors'', tools outside of the development chain that translate a compiled binary into another, less intelligible one. On the other hand there are software development kits that require a significant effort from the developer to ensure the program is adequately obfuscated.

In this paper, we present obfuscation methods that are easily integrated into the development chain of C++ programs, by using the compiler itself to perform the obfuscated code generation. This is accomplished by using advanced C++ techniques, such as operator overloading, template metaprogramming, expression templates, and more. We achieve obfuscated code featuring randomization, opaque predicates and data masking. We evaluate our obfuscating transformations in terms of potency, resilience, stealth, and cost.
The European Desktop Grid Initiative (EDGI) is an European project that aims to close the gap between service push-based grids, such as gLite, ARC and UNICORE, and desktop, pull-based, grids, such as BOINC and XtremWeb. Given the inevitably large size of the overall infrastructure, the EDGI team needs to benchmark its components, to identify configuration problems, performance bottlenecks, and to ensure the appropriate QoS levels. 

In this paper, we describe our  benchmarking effort. To take our measurements, we submitted batches of jobs to demonstration facilities, and to components that we disconnected from the infrastructure. We focused our measurements on the two most important metrics for any grid resource: latency and throughput of jobs. Additionally, by increasing job submission load to the limits of the EDGI components, we identified several bottlenecks in the job flow processes. The results of our work provide important performance guidelines for grid developers and administrators.


Despite being extremely successful, TCP has a number of shortcomings when network disruptions occur, or when peers do not follow a request-reply interaction: it does not handle connection crashes, event-driven communication or application migration. In many cases, programmers must engineer their own solutions to write reliable distributed applications.
To overcome these limitations, we propose FTSL, a Fault- Tolerant Session Layer that works on top of TCP. Besides offering a full-duplex connection, FTSL owns a number of distinctive features: it tolerates TCP connection crashes, it provides highly decoupled reliable patterns for one-way communication, and it enables server-side migration. While the first two greatly simplify distributed systems programming for a wide range of applications, the latter enables cloud systems managers to move a server application for load balance or maintenance, without moving the entire virtual machine. We present the FTSL protocol, its implementation, and resort to performance to show that FTSL imposes a reasonable overhead for the guarantees it provides.

Mobile robots can be an invaluable aid to human first responders (FRs) in catastrophic incidents, as they are expendable and can be used to reduce human exposure to risk in search and rescue (SaR) missions, as well as attaining a more effective response. Moreover, parallelism and robustness yielded by multi-robot systems (MRS) may be very useful in this kind of spatially distributed tasks, which are distributed in space, providing augmented situation awareness (SA). However, this requires adequate cooperative behaviors, both within MRS teams and between human and robotic teams. Collaborative context awareness between both teams is crucial to assess information utility, efficiently share information and build a common and consistent SA. This paper presents the foreseen research within the CHOPIN research project, which aims to address these scientific challenges and provide a proof of concept for the cooperation between human and robotic teams in SaR scenarios.

Reliable request-response interactions, in which the server never executes a given request more than once, are being used to support business and safety-critical operations in diverse sectors, such as banking, E-commerce, or healthcare. This form of interactions can be quite difficult to implement, because the client, server, or communication channel may fail, potentially requiring diverse and complex recovery procedures, which may result in duplicate messages being processed at the server. In this paper we address the fol- lowing question: could we provide a meaningful taxonomy of reliable request-response protocols? We generate valid sequences of client and server actions, organize the generated sequences into a prefix tree, and classify them according to their reliability semantics and memory requirements. The tree reveals three families of protocols matching common real-world implementations that try to deliver exactly-once or at-most-once. The strict organization of the protocols provides a solid foundation for creating correct services, and we show that it also serves to easily identify fallacies and pitfalls of existing implementations.
Sparse matrix-vector multiplication dominates the performance of many scientific and industrial problems.  For example, iterative methods for solving linear systems often rely on the performance of this critical operation. The particular case of binary matrices shows up in several important areas of computing, such as graph theory and cryptography. Unfortunately, irregular memory access patterns cause poor memory throughput, slowing down this operation.

To maximize memory throughput, we translate the matrix into a straight-line program that takes advantage of the CPU's instruction cache and hardware prefetchers. The regular loopless pattern of the program reduces cache misses, thus decreasing the latency for most instructions. We focus on the widely used \texttt{x86\_64} architecture and on binary matrices, to explore several possible tradeoffs regarding memory access policies and code size. We also consider matrices with elements over various mathematical structures, such as floating-point reals and integers modulo $m$. When compared to a Compressed Row Storage (CRS) implementation, we obtain significant speedups.
HTTP is currently being used as the communication protocol for many applications on the web, supporting business and safety-critical services throughout the world. Despite the growing importance, HTTP-based applications are quite exposed to network failures, which can bring in huge losses to the service users and providers, including financial and reputation losses. Several approaches try to achieve reliable communication by using logging and retransmission of whole HTTP messages, which is especially ill-adapted to large messages. Stream-based approaches are more efficient as, upon failure, they transparently resume data transmission from where it stopped. Despite this, designing a stream-based mechanism involves significant challenges, as it is very difficult to know how much data is lost due to failure. In this paper we propose a stream-based mechanism for reliable HTTP communication that is entirely based on HTTP messages and is compatible with existing software. The mechanism is presented as a design pattern and relieves developers from explicitly handling connection failures, providing a standard way for building more reliable applications. Results show that the mechanism is functional, compatible with legacy applications, and that the coding and runtime costs of this design pattern are quite low.

Standing on virtualization techniques, low maintenance costs and economies of scale, cloud computing emerged in the last few years as a major trend in the industry. Since cloud resources grow and shrink as needed, providers and users of the cloud must carefully determine the exact amount of such resources they need. For this reason, getting accurate and timely information from the system is of paramount importance to properly adjust the means serving a given application. However, previous attempts to detect bottlenecks have resulted in complex, heavy and customized frameworks that lack any sort of standardization and may change widely from provider to provider. Improved monitoring mechanisms should be independent from the server technology, should require little to no configuration and should provide information of the real quality of service offered to clients.

To reach these goals, we intend to observe the server infrastructure from the outside and gather the smallest possible number of metrics from the inside. We undertook several experiments in a controlled server, to identify the patterns that correspond to bottlenecks. These experiments clearly show that one can actually diagnose different bottlenecks, by analyzing response times on browsers. These results pave the way to future monitoring mechanisms, mostly based on quality of service evidence, supported by user data.

Ensuring short response times is a major concern for all web site administrators. To keep these times under control, they usually resort to monitoring tools that collect a large spectrum of system metrics, such as CPU and memory occupation, network traffic, number of processes, etc. Despite providing a reasonably accurate picture of the server, the times that really matter are those experienced by the user. However, not surprisingly, system administrators will usually not have access to these end-to-end figures, due to their lack of control over web browsers.
To overcome this problem, we follow the opposite approach of monitoring a site based on times collected from browsers. We use two browser-side metrics for this: $i$) the time it takes for the first byte of the response to reach the user (request time) and $ii$) the time it takes for the entire response to arrive (response time). We conjecture that an appropriate choice of the resources to control, more precisely, one or two URLs, suffices to detect CPU, network and I/O bottlenecks. In support of this conjecture, we run periodical evaluations of request and response times on some very popular web sites to detect bottlenecks.
Our experiments suggest that collecting data from the browsers can indeed contribute for better monitoring tools that provide a deeper understanding of the system, thus helping to maintain faster, more interactive web sites.

We propose replacing the traditional tree depth limit in Genetic Programming by a single limit on the amount of resources available to the whole population, where resources are the tree nodes. The resource-limited technique removes the disadvantages of using depth limits at the individual level, while introducing automatic population resizing, a natural side-effect of using an approach at the population level. The results show that the replacement of individual depth limits by a population resource limit can be done without impairing performance, thus validating this first and important step towards a new approach to improving the efficiency of GP.
Location is an important topic on Ambient Intelligence. Different
techniques are used, alone or together, to determine the position of
people and objects. One aspect of this problem concerns to indoor
location. Various authors propose the analysis of Radio Frequency
(RF) signatures as a solution for this challenge. An approach for
indoor location is the use of RF signals acquired from a Global
System for Mobile Communications (GSM) by Mobile Units(MU).

In this paper we make a study based on around 485.000 signatures
gathered from four buildings. We present our conclusions on the
suitability and limitations of this approach for indoor location.


The objective of this paper is the definition of a new methodology for carrying out security risk assessment in the Air Traffic Management (ATM) domain. This process is carried out by modelling the system, identifying the assets, threats and vulnerabilities, prioritizing the threats and proposing countermeasures for the weaknesses found. 
ATM security is concerned with securing the ATM assets, to prevent threats and limit their effects on the overall aviation network. This effect limitation could be achieved by removing the vulnerability from the system and/or increasing the tolerance in case of component failures due to attacks. 
The security risk assessment methodology proposed is based on what is currently being done by the industry and international organisations (International Civil Aviation Organization (ICAO), Common Criteria (CC), International Standard Organisation (ISO), EUROCONTROL Guidance Material, etc.) and comprises five main stages. 
For demonstrative purposes, the methodology is applied to a case study on the Flight Data Processing Subsystem &#40;FDPS&#41;, which is a component of many ATM systems


After 9/11 terrorist attacks, critical assets protection has become a priority all over the world. The focus moved from “safety” to “security”: from the prevention and mitigation of casual and unexpected events to the mitigation of deliberate acts. Regarding the protection of particular critical assets as vessels and ports or aircrafts and airports, respectively International Maritime Organisation (IMO) and International Civil Aviation Organization (ICAO) developed two different methodologies for security management, both taking into account that “total security” would be attainable only with an infinite cost. IMO, through the International Ship and Port Facility Security (ISPS) Code (ref. to IMO, 2002), has stated that countermeasures have to be identified and implemented in a scalable way, according to the “security level”. Nevertheless, “security level” is the result of intelligence information, whose trustworthiness is in inverse relation to malicious people’s capability to act by surprise, which undoubtedly increases the success of their actions. Therefore, security risk assessment and consequent countermeasures should set aside intelligence information and base their cost-effectiveness on other considerations. This paper aims at proposing an innovative methodology for security risk management that allows the identification of cost-effective countermeasures, based on the evaluation of the impact of each potential incident, independently from the “security level”. To meet this objective we will benefit of past experiences in airport security, where different strategies are suggested by ICAO
Mobile devices are becoming more and more omnipresent due to their lightweight, small size and increasing performance. Almost every mobile device has Bluetooth (BT) capabilities and this powerful combination widely used in our daily life is coming to new environments like the car and the military industries. As any technology, BT has security issues that hackers have extensively exploited over the years, while users seem not to care too much. To raise the security awareness we present an analysis of BT attack methods and tools over time. We paid particular attention to the severity, possible targets and the ability to persist over new versions of BT. Results show that adversaries can take complete control of the victims’ mobile device features if they forget to use simple safety measures like turning off the BT when not in use. To increase security we also propose the development of a novel BT Firewall.
This chapter presents a methodology to evaluate and benchmark web application vulnerability scanners using software fault injection techniques. The most common software faults are injected in the web application source code, which is then checked by the scanners. Using this procedure, we evaluated three leading commercial scanners, which are often regarded as an easy way to test the security of web applications, including critical vulnerabilities such as XSS and SQL Injection. Our idea consists of providing the scanners with the input they are supposed to handle, which is a web application with software faults and possible vulnerabilities originated by such faults. The results of the scanners are compared evaluating the efficiency in identifying the potential vulnerabilities created by the injected fault, their coverage of vulnerability detection and false positives. However, the results show that the coverage of these tools is low and the percentage of false positives is very high.

This work aims at providing mechanisms to increase users' satisfaction when using database systems. We express users' satisfaction in terms of Quality of Experience (QoE). Therefore, our proposals aim to increase the degree of QoE a database system provides.

Traditional database systems execute operations immediately upon submission
and, since they do not allow users to express execution-related constraints, they do not evaluate whereas those constraints are covered, and they do not take corrective action when necessary.

Our proposal for QoE makes the database system take into consideration users'
expectations on deciding how or when to execute operations. This is based on a set of Data Access Requirements (DAR) that users can associate to database operations and the QoE-prepared system considers those when processing the operations.

Since the objective of the QoE-oriented database system is to provide user
satisfaction, the analysis of its performance must consider success rate measures on achieving the user specified constraints. We have defined Key Performance Indicators based on such measures and used those as part of our comparison of approaches.

Our proposed Data Access Requirements (DARs) include execution deadlines,
execution start and end times, data availability, data freshness, execution priority, disconnected execution and job repetition. Some of those, such as execution deadlines, are useful in any data processing architecture - centralized, parallel or distributed – whereas DARs such as availability are especially designed for parallel and distributed contexts.

For the proposed approach to work on any of those architectures we needed to
develop a set of features that includes runtime estimations, requirements-based task scheduling and future jobs monitor and scheduling.
Besides that, we also developed some other features required for parallel and
distributed QoE-oriented database systems. Those include an election-based global scheduler, capacity to evaluate data availability degree and capacity to decide on data replication. Another important aspect in parallel and distributed settings was the development of reputation strategies. These allow the system to constantly have updated quantitative information on the degree of QoE expectations fulfillment by nodes or sites and, based on these, to adapt in order to maintain high QoE capabilities. Requirements fulfillment rates and runtime estimations are the bases of proposed reputation algorithms, which support better scheduling decisions.

We show experimentally, using benchmark scenarios, that the proposed QoEoriented database system is able to satisfy the user-defined execution-related constraints in both centralized, parallel and distributed cases. In order to do this we have created a prototype and tested the most important concepts proposed in this thesis. The approaches were compared with best effort (no QoE) counterparts and, when relevant, with scheduling approaches such as round-robin or on-demand.




Os web services são actualmente um componente essencial em muitas organizações. São crescentemente um factor crítico de sucesso para as organizações, dada a sua importância na disponibilização de informação e distribuição de dados. Muitos sistemas são construídos tendo como suporte uma arquitectura baseada em web services compostos. A composição de web services refere-se à utilização orquestrada de diversos web services que funcionam conjuntamente para o fornecimento de um serviço mais abrangente. O termo ‘processo de negócio’ é utilizado correntemente como sinónimo deste conceito. Dado existir uma dependência entre os elementos de uma composição, facilmente se percebe que uma falha num dos elementos pode afectar a disponibilidade de toda a composição. De um modo análogo, o desempenho individual de cada componente tem reflexo no desempenho global da composição.
A tolerância a falhas é um tópico de investigação bastante actual. Uma das soluções usuais aplicada para aumentar a tolerância a falhas de hardware é a de replicação de componentes. Apesar disto, o problema subsiste a nível do software utilizado, dado que uma mesma versão continua a existir nas múltiplas instâncias de um sistema. Uma das abordagens a este problema passa pela utilização de versões diferentes de um mesmo elemento para a detecção e tolerância a falhas.
Apesar disto, o suporte à composição de serviços é pobre no fornecimento de soluções para o reconhecimento e selecção de web services alternativos. De facto, não há formas padronizadas ou prácticas para escolha que permitam uma melhoria posterior no desempenho e confiabilidade de um processo de negócio.
O objectivo do trabalho apresentado nesta dissertação passa pela avaliação e comparação do desempenho e confiabilidade de web services em várias vertentes (robustez, recuperação, etc.). Adicionalmente, é também um dos focos deste trabalho a investigação de novos modos de uso da diversidade em termos da melhoria das propriedades de autonomia de serviços compostos.

-

In evolutionary testing, meta-heuristic search techniques are used to generate high-quality test data. The focus of our on-going work is on employing evolutionary algorithms for the structural unit-testing of object-oriented Java programs. 

Test cases are evolved using the Strongly-Typed Genetic Programming technique. Test data quality evaluation includes instrumenting the test object, executing it with the generated test cases, and tracing the structures traversed in order to derive coverage metrics. The strategy for efficiently guiding the search process towards achieving full structural coverage involves favouring test cases that exercise problematic structures and control-flow paths. Static analysis and instrumentation is performed solely with basis on the information extracted from the test objects' Java Bytecode.

Relevant contributions include the introduction of novel methodologies for automation, search guidance and input domain reduction, and the presentation of the eCrash automated test case generation tool.
Mobile devices, such as Smartphones, are being used virtually by every modern individual. Such devices are expected to work continuously and flawlessly for years, despite having been designed without criticality requirements.
However, the requirements of mobility, digital identification and authentication lead to an increasing dependence of societies on the correct behaviour of these "proxies for the individual".
The Windows Mobile 5.0 release has delivered a new set of internal state monitoring services, centralized into the State and Notifications Broker. This API was designed to be used by context-aware applications, providing a comprehensive monitoring of the internal state and resources of mobile devices.
We propose using this service to increase the dependability of mobile applications by showing, through a series of fault-injection campaigns, that this novel API is very effective for error propagation proling and monitoring.
Evolutionary Testing is an emerging methodology for automatically generating
high quality test data. The focus of this work is on presenting a searchbased
approach for the the unit-testing of third-party object-oriented Java
software.
Test cases are represented and evolved using the Strongly Typed Genetic
Programming paradigm, which effectively mimics the inheritance and polymorphic
properties of object-oriented programs and enables the maintenance
of call dependences when applying tree construction, mutation or crossover.
Our strategy for evaluating the quality of test cases includes instrumenting
the test object for basic block analysis and structural event dispatch,
and executing the instrumented test object using the generated test cases as
'inputsâ? ' in order to collect trace information and derive coverage metrics.
Static analysis, instrumentation and execution tracing is performed solely
with basis on the high-level information extracted from the Java Bytecode of
the test object. Given that the test object's source code is often unavailable,
working at the Bytecode level allows broadening the scope of applicability of
our approach; it can be used, for instance, to perform structural testing on
third-party Java components.
Test objects are represented internally by weighted control-flow graphs;
strategies are introduced for favouring test cases that exercise problematic
structures and difficult control-flow paths, which involve dynamic weight
reevaluation. The aim is that of efficiently guiding the search process towards
achieving full structural coverage ' which often involves promoting
the definition of complex and intricate test cases that define elaborate state
scenarios.
The work performed so far allowed us to develop the prototype of a test
case generation tool, called eCrash. Experiments have been carried and quality
solutions have been found, proving the pertinence of our methodology and
encouraging further studies.
Software Testing is the process of exercising an application to detect errors and to verify that it satisfies the specified requirements. It is an expensive process, typically consuming roughly half of the total costs involved in software development; automating Test Data generation is thus vital to advance the state-of-the-art in Software Testing. The application of Evolutionary Algorithms to Test Data generation is often referred to as Evolutionary Testing. The goal of Evolutionary Testing is to find a set of Test Programs which satisfies a particular test criterion. The focus of this work was put on developing a Genetic Programming-based solution for evolving Test Data for the structural Unit Testing of Object-Oriented programs.

The technical approach to Object-Oriented Evolutionary Testing proposed involves representing Test Programs using the Strongly-Typed Genetic Programming paradigm. Test Program quality evaluation includes instrumenting the Test Object, and executing it using the generated Test Programs with the intention of collecting trace information with which to derive coverage metrics. The aim is that of efficiently guiding the search process towards achieving full structural coverage of the program under test.

The foremost objectives of this work were those of defining strategies for addressing the challenges posed by the Object-Oriented paradigm, and of proposing methodologies for enhancing the efficiency and the effectiveness of search-based approaches to Software Testing. 

Relevant contributions include: the introduction of a novel strategy for Test Program evaluation and search guidance; 
the presentation of an Input Domain Reduction methodology based on the concept of Purity Analysis; suggesting an adaptive methodology for promoting the introduction of relevant instructions into the generated Test Programs by means of Mutation; and the proposal of an Object Reuse methodology for Genetic Programming-based approaches to Evolutionary Testing, which allows a single object instance to be used as a method parameter multiple times.

The advances attained resulted in the development and implementation of the eCrash Test Data generation tool, which embodies the approach to Object-Oriented Evolutionary Testing proposed; special attention was paid to improving the level of automation of both the static Test Object analysis and the iterative Test Data generation processes.
Portugal has recently become one the few European countries to fully acknowledge Traditional Chinese Medicine (TCM); this substantial paradigm shift calls for novel tools for TCM practitioners, students and patients alike. This paper describes an Expert System for supporting the TCM consultation process – both in terms of gathering and managing the patients’ personal and symptomatic data, and of obtaining accurate diagnoses and treatments under regulated and reviewed protocols. The proposed tool was designed and is being developed with the support of two TCM therapists, which act as experts and provide aid to the processes of building the knowledge base and the automatic diagnosis system. In terms of architecture, the current version of the framework includes a mobile client application for the Android platform, integrated with an online spreadsheet. A survey was conducted in order to assess some of the needs of the community of TCM practitioners, and allowed gathering information on their needs.



to be added



















Authorization infrastructures are an integral part of any network where resources need to be protected. As networks expand and organizations start to federate access to their resources, authorization infrastructures become increasingly difficult to manage. In this paper, we explore the automatic adaptation of authorization assets (policies and subject access rights) in order to manage federated authorization infrastructures. We demonstrate adaptation through a Self-Adaptive Authorization Framework (SAAF) controller that is capable of managing policy based federated role/attribute access control authorization infrastructures. The SAAF controller implements a feedback loop to monitor the authorization infrastructure in terms of authorization assets and subject behavior, analyze potential adaptations for handling malicious behavior, and act upon authorization assets to control future authorization decisions. We evaluate a prototype of the SAAF controller by simulating malicious behavior within a deployed federated authorization infrastructure (federation), demonstrating the escalation of adaptation, along with a comparison of SAAF to current technology.
Self-adaptive access control, in which self-* properties are applied to protecting systems, is a promising solution for the handling of malicious user behaviour in complex infrastructures. A major challenge in self-adaptive access control is ensuring that chosen adaptations are valid, and produce a satisfiable model of access. The contribution of this paper is the generation, transformation and verification of Role Based Access Control (RBAC) models at run-time, as a means for providing assurances that the adaptations to be deployed are valid. The goal is to protect the system against insider threats by adapting at run-time the access control policies associated with system resources, and access rights assigned to users. Depending on the type of attack, and based on the models from the target system and its environment, the adapted access control models need to be evaluated against the RBAC metamodel, and the adaptation constraints related to the application. The feasibility of the proposed approach has been demonstrated in the context of a fully working prototype using malicious scenarios inspired by a well documented case of insider attack.
Although web services are becoming business-critical components, they are often deployed with critical software bugs, causing security vulnerabilities that can be maliciously exploited. Develop time constraints and developers not specialized in security often lead to security cautions being disregarded, giving utmost importance to t the use of automated security testing tools to detect existing security vulnerabilities. However, automated security testing tools often do not deserve the confidence that developing teams put on them. In fact, previous research shows that many vulnerabilities remain undetected even when using well-known and widely used vulnerability detection tools.
The present work has two main contributions: the evaluation of existing tools and the proposal of new approaches for the detection of vulnerabilities. First we evaluate existing tools to assess their effectiveness in the detection of vulnerabilities in web services environments. Results show that many web services are deployed with security vulnerabilities (being SQL Injection the most common type of vulnerability in this context) and that security test tools present an unsatisfactory effectiveness in web services environment (low coverage and high number of false positives). This way, we propose two new techniques for detection of security vulnerabilities in web services. The first is based on penetration testing and target SQL Injection vulnerabilities. The second is a gray-box approach for the detection of SQL Injection and XPath Injection vulnerabilities.
The experimental evaluation shown that the penetration testing tool achieved higher effectiveness than the web security scanners on detecting SQL Injection vulnerabilities, showing that is possible to develop a vulnerability scanner for web services that performs much better than the commercial ones currently available. In relation to the proposed gray-box approach, experimental evaluation has shown that it performs much better than known tools (including commercial ones), achieving extremely high detection coverage while maintaining the false positives rate very low.



Cryptography, the science of writing secrets, has been used for centuries to conceal information from eavesdroppers and spies. Today, in the information age, data security and authenticity are paramount, as more services and applications start to rely on the Internet, an unsecured channel. Despite the existence of security protocols and implementations, many online services refrain to use cryptographic algorithms due to their poor performance, even when using cryptography would be a clear advantage.

Graphics processing units (GPU) have been increasingly used in the last few years for general purpose computing. We present and describe serial and parallel efficient algorithms for modular arithmetic in the GPU. Based on these, we developed GPU implementations of symmetric-key ciphers, namely AES and Salsa20, and public-key algorithms, such as RSA, Diffie-Hellman and DSA. We bundled this software into a library that contains the main achievements of this thesis.

We show that our symmetric-key cipher and modular exponentiation implementations included in this library outperform recent Intel CPUs and all previous GPU implementations. We achieve 11686 512-bit modular exponentiations per second, 1215 1024-bit modular exponentiations per second and peak AES-CTR throughputs of 1032 MB/s.
Intel recently documented its AVX2 instruction set extension that introduces
support for 256-bit wide single-instruction multiple-data (SIMD) integer arithmetic over
double (32-bit) and quad (64-bit) words. This will enable Intel's future processors|starting
with the Haswell architecture, to be released in 2013|to fully support 4-way SIMD com-
putation of 64-bit ARX algorithms (32-bit is already supported since SSE2). AVX2 also
introduces instructions with potential to speed-up cryptographic functions, like any-to-any
permute and vectorized table lookup. In this paper we show how the AVX2 instructions will
benet the SHA-3 nalist hash function BLAKE, an algorithm that naturally lends itself
to 4-way 32- or 64-bit SIMD implementations thanks to its inherent parallelism. We also
wrote BLAKE-256 assembly code for AVX and AVX2, and measured for the former a speed
of 7.62 cycles per byte, setting a new speed record.
 In 2013 Intel will release the AVX2 instructions, which introduce 256-bit single-instruction multiple-data (SIMD) integer arithmetic. This will enable desktop and server processors from this vendor to support 4-way SIMD computation of 64-bit add-rotate-xor algorithms, as well as 8-way 32-bit SIMD computations. AVX2 also includes interesting instructions for cryptographic functions, like any-to-any permute and vectorized table-lookup. In this paper, we explore the potential of AVX2 to speed-up the SHA-3 finalist BLAKE, and present the first working assembly implementations of BLAKE-256 and BLAKE-512 with AVX2. We then investigate the potential of the recent AVX and XOP instructions to accelerate BLAKE, and report new speed records on Sandy Bridge and Bulldozer microarchitectures (7.47 and 11.64 cycles per byte for BLAKE-256, 5.71 and 6.95 for BLAKE-512). 
We present the hash function BLAKE2, an improved version of the SHA-3 finalist BLAKE optimized for speed in software. Target applications include cloud storage, intrusion detection, or version control systems. BLAKE2 comes in two main flavors: BLAKE2b is optimized for 64-bit platforms, and BLAKE2s for smaller architectures. On 64-bit platforms, BLAKE2 is often faster than MD5, yet provides security similar to that of SHA-3: up to 256-bit collision resistance, immunity to length extension, indifferentiability from a random oracle, etc. We specify parallel versions BLAKE2bp and BLAKE2sp that are up to 4 and 8 times faster, by taking advantage of SIMD and/or multiple cores. BLAKE2 reduces the RAM requirements of BLAKE down to 168 bytes, making it smaller than any of the five SHA-3 finalists, and 32% smaller than BLAKE. Finally, BLAKE2 provides a comprehensive support for tree-hashing as well as keyed hashing (be it in sequential or tree mode). 
This paper introduces NORX, a novel authenticated encryption scheme supporting
arbitrary parallelism degree and based on ARX primitives, yet not using modular
additions. NORX has a unique parallel architecture based on the monkeyDuplex
construction, with an original domain separation scheme for a simple processing
of header, payload and trailer data. Furthermore, NORX specifies a dedicated
datagram to facilitate interoperability and avoid users the trouble of defining
custom encoding and signalling. NORX was optimized for efficiency in both
software and hardware, with a SIMD-friendly core, almost byte-aligned
rotations, no secret-dependent memory lookups, and only bitwise operations. On
a Haswell processor, a serial version of NORX runs at 2.51 cycles per byte.
Simulations of a hardware architecture for 180 nm UMC ASIC give a throughput
of approximately 10 Gbps at 125 MHz.
This paper presents a thorough analysis of the AEAD scheme NORX, focussing on
differential and rotational properties. We first introduce mathematical models
that describe differential propagation with respect to the non-linear operation
of NORX. Afterwards, we adapt a framework previously proposed for ARX designs
allowing us to automatise the search for differentials and characteristics. We
give upper bounds on the differential probability for a small number of steps of
the NORX core permutation. For example, in a scenario where an attacker can only
modify the nonce during initialisation, we show that characteristics have
probabilities of less than $2^{-60}$ ($32$-bit) and $2^{-53}$ ($64$-bit) after
only one round. Furthermore, we describe how we found the best characteristics
for four rounds, which have probabilities of $2^{-584}$ ($32$-bit) and
$2^{-836}$ ($64$-bit), respectively.  Finally, we discuss some rotational
properties of the core permutation which yield some first, rough bounds and can
be used as a basis for future studies.

This paper analyses the cryptography used in the Open Smart Grid Protocol (OSGP). The authenticated encryption (AE) scheme deployed by OSGP is a non-standard composition of RC4 and a home-brewed MAC, the ``OMA digest''.

We present several practical key-recovery attacks against the OMA digest. The first and basic variant can achieve this with a mere 13 queries to an OMA digest oracle and negligible time complexity. A more sophisticated version breaks the OMA digest with only 4 queries and a time complexity of about 225 simple operations. A different approach only requires one arbitrary valid plaintext-tag pair, and recovers the key in an average of 144 \emph{message verification} queries, or one ciphertext-tag pair and 168 \emph{ciphertext verification} queries.

Since the encryption key is derived from the key used by the OMA digest, our attacks break both confidentiality and authenticity of OSGP.
Invalid curve attacks are a well-known class of attacks against implementations of elliptic curve cryptosystems, in which an adversary tricks the cryptographic device into carrying out scalar multiplication not on the expected secure curve, but on some other, weaker elliptic curve of his choosing. In their original form, however, these attacks only affect elliptic curve implementations using addition and doubling formulas that are independent of at least one of the curve parameters. This property is typically satisfied for elliptic curves in Weierstrass form but not for newer models that have gained increasing popularity in recent years, like Edwards and twisted Edwards curves. It has therefore been suggested (e.g. in the original paper on invalid curve attacks) that such alternate models could protect against those attacks.

In this paper, we dispel that belief and present the first attack of this nature against (twisted) Edwards curves, Jacobi quartics, Jacobi intersections and more. Our attack differs from invalid curve attacks proper in that the cryptographic device is tricked into carrying out a computation not on another elliptic curve, but on a group isomorphic to the multiplicative group of the underlying base field. This often makes it easy to recover the secret scalar with a single invalid computation.

We also show how our result can be used constructively, especially on curves over random base fields, as a fault attack countermeasure similar to Shamir's trick.
This paper presents NORX8 and NORX16, the 8-bit and 16-bit versions of the authenticated cipher NORX, one of the CAESAR candidates. These new versions are better suited for low-end systems---such as ``internet of things'' devices---than the original 32-bit and 64-bit versions: whereas 32-bit NORX requires 64 bytes of RAM or cache memory, NORX8 and NORX16 require just 16 and 32 bytes, respectively. Both of the low-end variants were designed to retain the security properties of the initial NORX and be fast on small CPUs.
A popular approach to tweakable blockcipher design is via masking, where a certain primitive (a blockcipher or a permutation) is preceded and followed by an easy-to-compute tweak-dependent mask. In this work, we revisit the principle of masking. We do so alongside the introduction of the tweakable Even-Mansour construction MEM. Its masking function combines the advantages of word-oriented LFSR- and powering-up-based methods. We show in particular how recent advancements in computing discrete logarithms over finite fields of characteristic 2 can be exploited in a constructive way to realize highly efficient, constant-time masking functions. If the masking satisfies a set of simple conditions, then MEM is a secure tweakable blockcipher up to the birthday bound. The strengths of MEM are exhibited by the design of fully parallelizable authenticated encryption schemes OPP (nonce-respecting) and MRO (misuse-resistant). If instantiated with a reduced-round BLAKE2b permutation, OPP and MRO achieve speeds up to 0.55 and 1.06 cycles per byte on the Intel Haswell microarchitecture, and are able to significantly outperform their closest competitors.
Haptic devices are getting very cheap which makes it possible to use them more broadly. However the lack of software that supports these devices is still quite high. We present a prototype of an authoring system, based on Dragon pathways, which allows users to take advantage of such devices, and build their own simulations and activities using an iconic language and mainly drag and drop techniques.
Safety is a fundamental property for a wide class of systems, which can be assessed through safety analysis. Recent standards, as the ISO26262 for the automotive domain, recom-mend safety analysis processes to be performed at system, hard-ware, and software levels. While Failure Modes and Effects Analysis (FMEA) is a well-known technique for safety assessment at system level, its application at software level is still an open problem, especially concerning its integration into certification processes. Fault injection has been envisioned as a viable approach for performing Software-FMEA (SW-FMEA), but it typically requires an advanced development stage where code is available. The approach we propose in this paper, aims to perform software fault injection at model-level, namely on fUML-ALF models obtained from a component-based UML description through transformations proposed in a previous work. Model-level fault injection allows SW-FMEA to assess the effectiveness of safety mechanisms from the early stages of system design. The work in this paper focuses on how the software fault injection is be implemented, and on the study of fault propagation through appropriate points of observation to highlight possible violations of requirements, with the identification critical paths.

This paper presents a self-powered telemetric torque meter. The idealized instrument uses strain gauge, telemetry, and LABVIEW graphic programming. The electronic transduction signal is transmitted by digital modulation from a remote transduction unit fixed to a rotation shaft to a base station sending signals to a personal computer (PC) by means of a virtual instrument developed in LABVIEW. The signal can also be delivered to other units besides the PC. The ZigBee/IEEE 802.15.4 protocol is the standard protocol for wireless communications and is highly used in industrial monitoring and control applications. A low-noise method for supplying the remote transduction unit components in the rotation shaft—using its rotating movement to generate the demanded energy—has also been developed. After extensive experimentation, the theoretical model seems to confirm the idea proposed. The system presented in this study is robust, precise, cost-effective, and has high-noise immunity even in abrasive and strong vibration environments.
This paper presents the development of a dynamic torque meter to be applied to rotating shafts using electronic transduction, strain gage, telemetry and LabView Graphic programming. A mathematical model was developed. The electronic transduction signal is transmitted by digital modulation from a remote transduction unit fixed to a shaft to a base station, sending the signal to a PC, by means of a VI (Virtual Instrument) developed in LabView. It can also be delivered to other units besides the PC. The use of digital modulation to transmit the radiofrequency signal, replacing conventional couplings, allows communication with a high signal/noise ratio. A clamp acts as sealant, protecting it from the environment and making it easier to install. The prototype can be used at situation that it is impossible to use flanges or sockets (the most of the industrial applications) and it is installed directly on the surface of the shaft. The use of superbatteries allows the remote unit to remain independent of a feed for several days, with long periods between maintenance. After thousands of experimental essays, the theoretical model seems to confirm the proposed idea. The system presented has a potential for high precision, low cost, long work life and easy maintenance.
This paper aims at evaluating the communication performance of the IEEE 802.15.4 standard in an industrial environment. We investigated the correlation between Packet Error Rate (PER) and Spectral Occupancy (SO) in the network environment. For this goal, we studied the impact on SO and PER resulting from the insertion of interference from other sources within the same range of the channels defined by the standard. The results show that an IEEE 802.15.4-based sensor network can suffer large drop in performance with the addition of new sources of interference. The information provided by the experiments can be used to guide the development of new spectrum-aware techniques and protocols to mitigate the interference in IEEE 802.15.4-based industrial wireless sensor networks.
Society is each time more dependent of information systems. In this way, system development has to be agile and achieve the desired level of quality for success. Therefore, it is essential ensure that these systems are highly maintainable and have a controlled evolution process. As software evolves the complexity of the software is increasingly growing to the extent that we cannot hand it easily. In this paper, we correlate how the internal complexity of software systems can affect systems survivability. The analysis of the systems evolution, regarding to complexity, can be used to support the developers to understand the behaviour of the system and avoid inactiveness.































A honeypot is a set of computational resources, designed to be swept, attacked and compromised. With a constant monitoring, detailedly record the attacker activities creating means to further understanding of the used approaches and tools. The value obtained from this computational resource is a measure calculated between the captured information and the future use of this data. Neofelis is a framework for high-interaction honeypots on Mac OS X operating system, that allows the system administrator to create a high-interaction honeypot feasible to several different scenarios.
This paper discusses Neofelis design, implementation and pointing out how the framework helps in different areas of the information security,  e.g. detecting zero-day exploits and capturing informations about the attacks





This study applies passive mobile positioning data such as Call Volume, Handover, and Erlang to detect the spatiotemporal distributions of urban activities. We obtained hourly aggregated cellphone data from a dataset of communications in Lisbon, Portugal. Fuzzy c-mean clustering algorithm was applied to the cellphone data to create clusters of locations with similar features in two aspects of activities: the pattern and intensity of urban activities along the hours of a day. In order to validate those clusters as actual predictors of human activity, we compared them with clusters formed using ground truth variables namely presence of people, buildings, points of interest, and bus and taxi movement. To identify the patterns of urban activities, the Erlang data provided a better match, with the ground truth giving 69% of overall accurate predictions. In the case of the intensity of activities, Handover data provided the highest match, with the ground truth yielding 80% of overall accuracy of predictions. Hence, the results demonstrate the potential of passive mobile positioning data in detecting intensity of activities that are superimposed on the different activity patterns, which is a fundamental piece of information for transportation and urban planning.
A rise in population, along with urbanization, has been causing an increase in demand for urban transportation services in the Sub-Saharan Africa countries. In these countries, mobility of people is mainly ensured by bus services and a large- scale informal public transport service, known as paratransit (e.g., car rapides in Senegal, Tro Tros in Ghana, taxis in Uganda and Ethiopia, and Matatus in Keneya, etc.). Transport demand estimation is a challenging task, particularly in developing countries, mainly due to its expensive and time consuming data collection requirements. Without accurate demand estimation, it is difficult for transport operators to provide their services and make other important decisions. In this article, we present a methodology to estimate passenger demand for public transport services using cell phone data. Significant origins and destinations of inhabitants are extracted and used to build origin-destination matrices that resemble travel demand. Based on the inferred travel demand, we are able to reasonably suggest strategic locations for public transport services such as paratransit and taxi stands, and new transit routes. The outcome of this study can be useful for the development of policies that can potentially help fulfill the mobility needs of city inhabitants.




Social networks such as Facebook have grown exponentially over the past decade. This growth led to the exploration of new services that could enhance users’ experiences and constitute a driver for even more followers. With the proliferation of smartphones and the increasing search for applications that enable the sharing of experiences, social networks became eager to integrate into mobile devices, taking advantage of their impressive omnipresence and panoply of sensors. Amongst the sensors, the most notable are the localization sensors (GPS) that allow for the development of location-based services that use the geographical position to enrich user experiences in a variety of contexts, including location-based searching and location-based mobile interaction. ChronoFindMe enhances location-based services by adding a temporal component not present in current approaches. The authors allow information about past and future locations to be considered by defining an architecture that provides location-based services to users of social networks. This information includes data about time and space, which can be accessed through the social network or a specific mobile application, using privacy policies to assure users’ privacy.